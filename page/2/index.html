<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Hive" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Hive/" class="article-date">
  <time datetime="2019-08-08T03:41:48.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Hive/">Hive</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><p>hive中所有查询除了select * from table 其他都要通过MapReduce方式执行 即使只有一行一列 如不是select * from table 也要查询8,9秒.</p>
<h2 id="1-数据仓库"><a href="#1-数据仓库" class="headerlink" title="1. 数据仓库"></a>1. 数据仓库</h2><h3 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1. 基本概念"></a>1.1. 基本概念</h3><p>英文名称为Data Warehouse，可简写为DW或DWH。数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。</p>
<p>数据仓库是存数据的，企业的各种数据往里面存，主要目的是为了分析有效数据，后续会基于它产出供分析挖掘的数据，或者数据应用需要的数据，如企业的分析性报告和各类报表等。</p>
<p>可以理解为：<code>面向分析的存储系统</code>。</p>
<h3 id="1-2-主要特征"><a href="#1-2-主要特征" class="headerlink" title="1.2. 主要特征"></a>1.2. 主要特征</h3><p>数据仓库是面向主题的（Subject-Oriented ）、集成的（Integrated）、非易失的（Non-Volatile）和时变的（Time-Variant ）数据集合，用以支持管理决策。</p>
<h4 id="1-2-1-面向主题"><a href="#1-2-1-面向主题" class="headerlink" title="1.2.1. 面向主题"></a>1.2.1. 面向主题</h4><p>数据仓库是面向主题的,数据仓库通过一个个主题域将多个业务系统的数据加载到一起，为了各个主题（如：用户、订单、商品等）进行分析而建，操作型数据库是为了支撑各种业务而建立。</p>
<h4 id="1-2-2-集成性"><a href="#1-2-2-集成性" class="headerlink" title="1.2.2. 集成性"></a>1.2.2. 集成性</h4><p>数据仓库会将不同源数据库中的数据汇总到一起,数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过统一与整合，这一步是数据仓库建设中最关键、最复杂的一步(ETL)，要统一源数据中所有矛盾之处，如字段的同名异义、异名同义、单位不统一、字长不一致，等等。</p>
<h4 id="1-2-3-非易失性"><a href="#1-2-3-非易失性" class="headerlink" title="1.2.3. 非易失性"></a>1.2.3. 非易失性</h4><p>操作型数据库主要服务于日常的业务操作，使得数据库需要不断地对数据实时更新，以便迅速获得当前最新数据，不至于影响正常的业务运作。</p>
<p>在数据仓库中只要保存过去的业务数据，不需要每一笔业务都实时更新数据仓库，而是根据商业需要每隔一段时间把一批较新的数据导入数据仓库。<br>数据仓库的数据反映的是一段相当长的时间内历史数据的内容，是不同时点的数据库的集合，以及基于这些快照进行统计、综合和重组的导出数据。数据仓库中的数据一般仅执行查询操作，很少会有删除和更新。但是需定期加载和刷新数据。</p>
<h4 id="1-2-4-时变性"><a href="#1-2-4-时变性" class="headerlink" title="1.2.4. 时变性"></a>1.2.4. 时变性</h4><p>数据仓库包含各种粒度的历史数据。数据仓库中的数据可能与某个特定日期、星期、月份、季度或者年份有关。数据仓库的目的是通过分析企业过去一段时间业务的经营状况，挖掘其中隐藏的模式。虽然数据仓库的用户不能修改数据，但并不是说数据仓库的数据是永远不变的。分析的结果只能反映过去的情况，当业务变化后，挖掘出的模式会失去时效性。因此数据仓库的数据需要定时更新，以适应决策的需要。</p>
<h3 id="1-3-数据库与数据仓库的区别"><a href="#1-3-数据库与数据仓库的区别" class="headerlink" title="1.3. 数据库与数据仓库的区别"></a>1.3. 数据库与数据仓库的区别</h3><p>数据库与数据仓库的区别实际讲的是 <code>OLTP</code> 与 <code>OLAP</code> 的区别。</p>
<p>操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。</p>
<p>分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持 管理决策。</p>
<p>首先要明白，数据仓库的出现，并不是要取代数据库。</p>
<ul>
<li>数据库是面向事务的设计，数据仓库是面向主题设计的。</li>
<li>数据库一般存储业务数据，数据仓库存储的一般是历史数据。</li>
<li>数据库设计是尽量避免冗余，一般针对某一业务应用进行设计，比如一张简单的User表，记录用户名、密码等简单数据即可，符合业务应用，但是不符合分析。数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。</li>
<li>数据库是为捕获数据而设计，数据仓库是为分析数据而设计。</li>
</ul>
<p><strong>数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。</strong></p>
<h3 id="1-4-数仓的分层架构"><a href="#1-4-数仓的分层架构" class="headerlink" title="1.4. 数仓的分层架构"></a>1.4. 数仓的分层架构</h3><p>按照数据流入流出的过程，数据仓库架构可分为三层——源数据、数据仓库、数据应用。</p>
<p><a href="https://manzhong.github.io/images/hive/1561705874041.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hive/1561705874041.png" alt="img"></a></p>
<p>数据仓库的数据来源于不同的源数据，并提供多样的数据应用，数据自下而上流入数据仓库后向上层开放应用，而数据仓库只是中间集成化数据管理的一个平台。</p>
<ul>
<li><code>源数据层（ODS）</code>：此层数据无任何更改，直接沿用外围系统数据结构和数据，不对外开放；为临时存储层，是接口数据的临时存储区域，为后一步的数据处理做准备。</li>
<li><code>数据仓库层（DW）</code>：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。</li>
<li><code>数据应用层（DA或APP）</code>：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。</li>
</ul>
<p>数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。</p>
<h5 id="为什么要对数据仓库分层？"><a href="#为什么要对数据仓库分层？" class="headerlink" title="为什么要对数据仓库分层？"></a>为什么要对数据仓库分层？</h5><p>用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</p>
<p>通过数据分层管理可以简化数据清洗的过程，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</p>
<h3 id="1-5-数仓的元数据管理"><a href="#1-5-数仓的元数据管理" class="headerlink" title="1.5. 数仓的元数据管理"></a>1.5. 数仓的元数据管理</h3><p>元数据（Meta Date），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。</p>
<p>元数据是数据仓库管理系统的重要组成部分，元数据管理是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p>
<ul>
<li><p>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</p>
</li>
<li><p>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</p>
</li>
<li><p>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</p>
<p><a href="https://manzhong.github.io/images/hive/1561705895292.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hive/1561705895292.png" alt="img"></a></p>
</li>
</ul>
<p>元数据可分为技术元数据和业务元数据。技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。</p>
<p>由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体。</p>
<h2 id="2-Hive-的基本概念"><a href="#2-Hive-的基本概念" class="headerlink" title="2. Hive 的基本概念"></a>2. Hive 的基本概念</h2><h3 id="2-1-Hive-简介"><a href="#2-1-Hive-简介" class="headerlink" title="2.1. Hive 简介"></a>2.1. Hive 简介</h3><h5 id="什么是-Hive"><a href="#什么是-Hive" class="headerlink" title="什么是 Hive"></a>什么是 Hive</h5><p>Hive是基于Hadoop的一个数据仓库工具，可以将<strong>结构化的数据</strong>文件映射为一张数据库表，并提供类SQL查询功能。</p>
<p>其本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更进一步可以说hive就是一个MapReduce的客户端</p>
<h5 id="为什么使用-Hive"><a href="#为什么使用-Hive" class="headerlink" title="为什么使用 Hive"></a>为什么使用 Hive</h5><ul>
<li>采用类SQL语法去操作数据，提供快速开发的能力。</li>
<li>避免了去写MapReduce，减少开发人员的学习成本。</li>
<li>功能扩展很方便。</li>
</ul>
<h3 id="2-2-Hive-架构"><a href="#2-2-Hive-架构" class="headerlink" title="2.2. Hive 架构"></a>2.2. Hive 架构</h3><p><a href="https://manzhong.github.io/images/hive/1561705918286.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hive/1561705918286.png" alt="img"></a></p>
<ul>
<li><strong>用户接口：</strong> 包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。</li>
<li><strong>元数据存储：</strong> 通常是存储在关系数据库如mysql/derby中。Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</li>
<li><strong>解释器、编译器、优化器、执行器:</strong> 完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有MapReduce 调用执行。</li>
</ul>
<h3 id="2-3-Hive-与-Hadoop-的关系"><a href="#2-3-Hive-与-Hadoop-的关系" class="headerlink" title="2.3. Hive 与 Hadoop 的关系"></a>2.3. Hive 与 Hadoop 的关系</h3><p>Hive利用HDFS存储数据，利用MapReduce查询分析数据</p>
<p><a href="https://manzhong.github.io/images/hive/1561705940248.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hive/1561705940248.png" alt="img"></a></p>
<h3 id="2-4-Hive与传统数据库对比"><a href="#2-4-Hive与传统数据库对比" class="headerlink" title="2.4. Hive与传统数据库对比"></a>2.4. Hive与传统数据库对比</h3><p>hive用于海量数据的离线数据分析</p>
<p><a href="https://manzhong.github.io/images/hive/1561705959607.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hive/1561705959607.png" alt="img"></a></p>
<p>总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析</p>
<h3 id="2-5-Hive-的安装"><a href="#2-5-Hive-的安装" class="headerlink" title="2.5. Hive 的安装"></a>2.5. Hive 的安装</h3><p>这里我们选用hive的版本是2.1.1<br>下载地址为：<br><a href="http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz" target="_blank" rel="noopener">http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz</a></p>
<p>下载之后，将我们的安装包上传到第三台机器的/export/softwares目录下面去</p>
<h5 id="第一步：上传并解压安装包"><a href="#第一步：上传并解压安装包" class="headerlink" title="第一步：上传并解压安装包"></a>第一步：上传并解压安装包</h5><p>将我们的hive的安装包上传到第三台服务器的/export/softwares路径下，然后进行解压</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares/</span><br><span class="line">tar -zxvf apache-hive-2.1.1-bin.tar.gz -C ../servers/</span><br></pre></td></tr></table></figure>

<h5 id="第二步：安装mysql"><a href="#第二步：安装mysql" class="headerlink" title="第二步：安装mysql"></a>第二步：安装mysql</h5><p>第一步：在线安装mysql相关的软件包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mysql mysql-server mysql-devel</span><br></pre></td></tr></table></figure>

<p>第二步：启动mysql的服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/mysqld start</span><br></pre></td></tr></table></figure>

<p>第三步：通过mysql安装自带脚本进行设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/mysql_secure_installation</span><br></pre></td></tr></table></figure>

<p>第四步：进入mysql的客户端然后进行授权</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>

<h5 id="第三步：修改hive的配置文件"><a href="#第三步：修改hive的配置文件" class="headerlink" title="第三步：修改hive的配置文件"></a>第三步：修改hive的配置文件</h5><p>修改hive-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/conf</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br><span class="line">HADOOP_HOME=/export/servers/hadoop-2.7.5</span><br><span class="line">export HIVE_CONF_DIR=/export/servers/apache-hive-2.1.1-bin/conf</span><br></pre></td></tr></table></figure>

<p>修改hive-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/conf</span><br><span class="line">vim hive-site.xml</span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;123456&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;node03&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<h5 id="第四步：添加mysql的连接驱动包到hive的lib目录下"><a href="#第四步：添加mysql的连接驱动包到hive的lib目录下" class="headerlink" title="第四步：添加mysql的连接驱动包到hive的lib目录下"></a>第四步：添加mysql的连接驱动包到hive的lib目录下</h5><p>hive使用mysql作为元数据存储，必然需要连接mysql数据库，所以我们添加一个mysql的连接驱动包到hive的安装目录下，然后就可以准备启动hive了</p>
<p>将我们准备好的mysql-connector-java-5.1.38.jar 这个jar包直接上传到<br><code>/export/servers/apache-hive-2.1.1-bin/lib</code> 这个目录下即可</p>
<p>至此，hive的安装部署已经完成，接下来我们来看下hive的三种交互方式</p>
<h5 id="第五步：配置hive的环境变量"><a href="#第五步：配置hive的环境变量" class="headerlink" title="第五步：配置hive的环境变量"></a>第五步：配置hive的环境变量</h5><p>node03服务器执行以下命令配置hive的环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br><span class="line">export HIVE_HOME=/export/servers/apache-hive-2.1.1-bin</span><br><span class="line">export PATH=:$HIVE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>

<h3 id="2-6-Hive-的交互方式"><a href="#2-6-Hive-的交互方式" class="headerlink" title="2.6. Hive 的交互方式"></a>2.6. Hive 的交互方式</h3><h5 id="第一种交互方式-bin-hive"><a href="#第一种交互方式-bin-hive" class="headerlink" title="第一种交互方式 bin/hive"></a>第一种交互方式 <code>bin/hive</code></h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin/</span><br><span class="line">bin/hive</span><br></pre></td></tr></table></figure>

<p>创建一个数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists mytest;</span><br></pre></td></tr></table></figure>

<h5 id="第二种交互方式：使用sql语句或者sql脚本进行交互"><a href="#第二种交互方式：使用sql语句或者sql脚本进行交互" class="headerlink" title="第二种交互方式：使用sql语句或者sql脚本进行交互"></a>第二种交互方式：<code>使用sql语句或者sql脚本进行交互</code></h5><p>不进入hive的客户端直接执行hive的hql语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.1.1-bin</span><br><span class="line">bin/hive -e &quot;create database if not exists mytest;&quot;</span><br></pre></td></tr></table></figure>

<p>或者我们可以将我们的hql语句写成一个sql脚本然后执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">vim  hive.sql</span><br><span class="line">create database if not exists mytest;</span><br><span class="line">use mytest;</span><br><span class="line">create table stu(id int,name string);</span><br></pre></td></tr></table></figure>

<p>通过hive -f 来执行我们的sql脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -f /export/servers/hive.sql</span><br></pre></td></tr></table></figure>

<h2 id="3-Hive-的基本操作"><a href="#3-Hive-的基本操作" class="headerlink" title="3. Hive 的基本操作"></a>3. Hive 的基本操作</h2><h3 id="3-1-数据库操作"><a href="#3-1-数据库操作" class="headerlink" title="3.1 数据库操作"></a>3.1 数据库操作</h3><h4 id="3-1-1-创建数据库"><a href="#3-1-1-创建数据库" class="headerlink" title="3.1.1 创建数据库"></a>3.1.1 创建数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists myhive;</span><br><span class="line">use  myhive;</span><br></pre></td></tr></table></figure>

<p>说明：hive的表存放位置模式是由hive-site.xml当中的一个属性指定的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br></pre></td></tr></table></figure>

<h4 id="3-1-2-创建数据库并指定位置"><a href="#3-1-2-创建数据库并指定位置" class="headerlink" title="3.1.2 创建数据库并指定位置"></a>3.1.2 创建数据库并指定位置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database myhive2 location &apos;/myhive2&apos;;</span><br></pre></td></tr></table></figure>

<h4 id="3-1-3-设置数据库键值对信息"><a href="#3-1-3-设置数据库键值对信息" class="headerlink" title="3.1.3 设置数据库键值对信息"></a>3.1.3 设置数据库键值对信息</h4><p>数据库可以有一些描述性的键值对信息，在创建时添加：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database foo with dbproperties (&apos;owner&apos;=&apos;itcast&apos;, &apos;date&apos;=&apos;20190120&apos;);</span><br></pre></td></tr></table></figure>

<p>查看数据库的键值对信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe database extended foo;</span><br></pre></td></tr></table></figure>

<p>修改数据库的键值对信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter database foo set dbproperties (&apos;owner&apos;=&apos;itheima&apos;);</span><br></pre></td></tr></table></figure>

<h4 id="3-1-4-查看数据库更多详细信息"><a href="#3-1-4-查看数据库更多详细信息" class="headerlink" title="3.1.4 查看数据库更多详细信息"></a>3.1.4 查看数据库更多详细信息</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc database extended  myhive2;</span><br></pre></td></tr></table></figure>

<h4 id="3-1-5-删除数据库"><a href="#3-1-5-删除数据库" class="headerlink" title="3.1.5 删除数据库"></a>3.1.5 删除数据库</h4><p>删除一个空数据库，如果数据库下面有数据表，那么就会报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop  database  myhive2;</span><br></pre></td></tr></table></figure>

<p>强制删除数据库，包含数据库下面的表一起删除</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop  database  myhive  cascade;</span><br></pre></td></tr></table></figure>

<h3 id="3-2-数据库表操作"><a href="#3-2-数据库表操作" class="headerlink" title="3.2 数据库表操作"></a>3.2 数据库表操作</h3><h4 id="3-2-1-创建表的语法"><a href="#3-2-1-创建表的语法" class="headerlink" title="3.2.1 创建表的语法:"></a>3.2.1 创建表的语法:</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create [external] table [if not exists] table_name (</span><br><span class="line">col_name data_type [comment &apos;字段描述信息&apos;]</span><br><span class="line">col_name data_type [comment &apos;字段描述信息&apos;])</span><br><span class="line">[comment &apos;表的描述信息&apos;]</span><br><span class="line">[partitioned by (col_name data_type,...)]</span><br><span class="line">[clustered by (col_name,col_name,...)]</span><br><span class="line">[sorted by (col_name [asc|desc],...) into num_buckets buckets]</span><br><span class="line">[row format row_format]</span><br><span class="line">[storted as ....]</span><br><span class="line">[location &apos;指定表的路径&apos;]</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ol>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">create table</a></p>
<p>创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">external</a></p>
<p>可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">comment</a></p>
<p>表示注释,默认不能使用中文</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">partitioned by</a></p>
<p>表示使用表分区,一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下 .</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">clustered by</a><br>对于每一个表分文件， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">sorted by</a></p>
<p>指定排序字段和排序规则</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">row format</a></p>
<p> 指定表文件字段分隔符</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">storted as</a>指定表文件的存储格式, 常用格式:SEQUENCEFILE, TEXTFILE, RCFILE,如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 storted as SEQUENCEFILE。</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener"><strong>location</strong></a></p>
<p>指定表文件的存储路径</p>
</li>
</ol>
<h4 id="3-2-2-内部表的操作"><a href="#3-2-2-内部表的操作" class="headerlink" title="3.2.2 内部表的操作"></a>3.2.2 内部表的操作</h4><p>创建表时,如果没有使用external关键字,则该表是内部表（managed table）</p>
<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">Hive建表字段类型</a></p>
<table>
<thead>
<tr>
<th align="left">分类</th>
<th align="left">类型</th>
<th align="left">描述</th>
<th align="left">字面量示例</th>
</tr>
</thead>
<tbody><tr>
<td align="left">原始类型</td>
<td align="left">BOOLEAN</td>
<td align="left">true/false</td>
<td align="left">TRUE</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">TINYINT</td>
<td align="left">1字节的有符号整数, -128~127</td>
<td align="left">1Y</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">SMALLINT</td>
<td align="left">2个字节的有符号整数，-32768~32767</td>
<td align="left">1S</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">INT</td>
<td align="left">4个字节的带符号整数</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">BIGINT</td>
<td align="left">8字节带符号整数</td>
<td align="left">1L</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">FLOAT</td>
<td align="left">4字节单精度浮点数</td>
<td align="left">1.0</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">DOUBLE</td>
<td align="left">8字节双精度浮点数</td>
<td align="left">1.0</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">DEICIMAL</td>
<td align="left">任意精度的带符号小数</td>
<td align="left">1.0</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">STRING</td>
<td align="left">字符串，变长</td>
<td align="left">“a”,’b’</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">VARCHAR</td>
<td align="left">变长字符串</td>
<td align="left">“a”,’b’</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">CHAR</td>
<td align="left">固定长度字符串</td>
<td align="left">“a”,’b’</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">BINARY</td>
<td align="left">字节数组</td>
<td align="left">无法表示</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">TIMESTAMP</td>
<td align="left">时间戳，毫秒值精度</td>
<td align="left">122327493795</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">DATE</td>
<td align="left">日期</td>
<td align="left">‘2016-03-29’</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">INTERVAL</td>
<td align="left">时间频率间隔</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">复杂类型</td>
<td align="left">ARRAY</td>
<td align="left">有序的的同类型的集合</td>
<td align="left">array(1,2)</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">MAP</td>
<td align="left">key-value,key必须为原始类型，value可以任意类型</td>
<td align="left">map(‘a’,1,’b’,2)</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">STRUCT</td>
<td align="left">字段集合,类型可以不同</td>
<td align="left">struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0)</td>
</tr>
<tr>
<td align="left"></td>
<td align="left">UNION</td>
<td align="left">在有限取值范围内的一个值</td>
<td align="left">create_union(1,’a’,63)</td>
</tr>
</tbody></table>
<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">建表入门:</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">use myhive;</span><br><span class="line">create table stu(id int,name string);</span><br><span class="line">insert into stu values (1,&quot;zhangsan&quot;);  #插入数据</span><br><span class="line">select * from stu;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">创建表并指定字段之间的分隔符</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create  table if not exists stu2(id int ,name string) row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">创建表并指定表文件的存放路径</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create  table if not exists stu2(id int ,name string) row format delimited fields terminated by &apos;\t&apos; location &apos;/user/stu2&apos;;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">根据查询结果创建表</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table stu3 as select * from stu2; # 通过复制表结构和表内容创建新表</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">根据已经存在的表结构创建表</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table stu4 like stu;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">查询表的详细信息</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formatted  stu2;</span><br></pre></td></tr></table></figure>

<p>. <a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">删除表</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table stu4;</span><br></pre></td></tr></table></figure>

<h4 id="3-2-3-外部表的操作"><a href="#3-2-3-外部表的操作" class="headerlink" title="3.2.3 外部表的操作"></a>3.2.3 外部表的操作</h4><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">外部表说明</a></p>
<p>外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉.</p>
<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">内部表和外部表的使用场景</a></p>
<p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">操作案例</a></p>
<p>分别创建老师与学生表外部表，并向表中加载数据</p>
<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">创建老师表</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create external table teacher (t_id string,t_name string) row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">创建学生表</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create external table student (s_id string,s_name string,s_birth string , s_sex string ) row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">加载数据</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/export/servers/hivedatas/student.csv&apos; into table student;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">加载数据并覆盖已有数据</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/export/servers/hivedatas/student.csv&apos; overwrite  into table student;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">从hdfs文件系统向表中加载数据（需要提前将数据上传到hdfs文件系统）</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hivedatas</span><br><span class="line">hdfs dfs -mkdir -p /hivedatas</span><br><span class="line">hdfs dfs -put techer.csv /hivedatas/</span><br><span class="line">load data inpath &apos;/hivedatas/techer.csv&apos; into table teacher;</span><br></pre></td></tr></table></figure>

<h4 id="3-2-4-分区表的操作"><a href="#3-2-4-分区表的操作" class="headerlink" title="3.2.4 分区表的操作"></a>3.2.4 分区表的操作</h4><p>在大数据中，最常用的一种思想就是分治，我们可以把大的文件切割划分成一个个的小的文件，这样每次操作一个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每月，或者天进行切分成一个个的小的文件,存放在不同的文件夹中.</p>
<p><a href="https://manzhong.github.io/data/20180101/1.log" target="_blank" rel="noopener">创建分区表语法</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table score(s_id string,c_id string, s_score int) partitioned by (month string) row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/data/20180101/1.log" target="_blank" rel="noopener">创建一个表带多个分区</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table score2 (s_id string,c_id string, s_score int) partitioned by (year string,month string,day string) row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/data/20180101/1.log" target="_blank" rel="noopener">加载数据到分区表中</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/export/servers/hivedatas/score.csv&apos; into table score partition (month=&apos;201806&apos;);</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/data/20180101/1.log" target="_blank" rel="noopener">加载数据到多分区表中</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/export/servers/hivedatas/score.csv&apos; into table score2 partition(year=&apos;2018&apos;,month=&apos;06&apos;,day=&apos;01&apos;);</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/data/20180101/1.log" target="_blank" rel="noopener">多分区表联合查询(使用 <code>union all</code>)</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where month = &apos;201806&apos; union all select * from score where month = &apos;201806&apos;;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/data/20180101/1.log" target="_blank" rel="noopener">查看分区</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show  partitions  score;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/data/20180101/1.log" target="_blank" rel="noopener">添加一个分区</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table score add partition(month=&apos;201805&apos;);</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/data/20180101/1.log" target="_blank" rel="noopener">删除分区</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table score drop partition(month = &apos;201806&apos;);</span><br></pre></td></tr></table></figure>

<h4 id="3-2-5-分区表综合练习"><a href="#3-2-5-分区表综合练习" class="headerlink" title="3.2.5 分区表综合练习"></a>3.2.5 分区表综合练习</h4><p><code>需求描述</code>：</p>
<p>现在有一个文件score.csv文件，存放在集群的这个目录下/scoredatas/month=201806，这个文件每天都会生成，存放到对应的日期文件夹下面去，文件别人也需要公用，不能移动。需求，创建hive对应的表，并将数据加载到表中，进行数据统计分析，且删除表之后，数据不能删除</p>
<p><code>数据准备</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /scoredatas/month=201806</span><br><span class="line">hdfs dfs -put score.csv /scoredatas/month=201806/</span><br><span class="line">创建外部分区表，并指定文件数据存放目录</span><br><span class="line">create external table score4(s_id string, c_id string,s_score int) partitioned by (month string) row format delimited fields terminated by &apos;\t&apos; location &apos;/scoredatas&apos;;</span><br></pre></td></tr></table></figure>

<p><code>进行表的修复</code>(<a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">建立表与数据文件之间的一个关系映射</a>)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msck  repair   table  score4;</span><br></pre></td></tr></table></figure>

<h4 id="3-2-6-分桶表操作"><a href="#3-2-6-分桶表操作" class="headerlink" title="3.2.6 分桶表操作"></a>3.2.6 分桶表操作</h4><p>分桶，就是将数据按照指定的字段进行划分到多个文件当中去,<a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">分桶就是MapReduce中的分区</a>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">开启 Hive 的分桶功能</span><br><span class="line">set hive.enforce.bucketing=true;</span><br><span class="line">设置 Reduce 个数</span><br><span class="line">set mapreduce.job.reduces=3;</span><br><span class="line">创建分桶表</span><br><span class="line">create table course (c_id string,c_name string,t_id string) clustered by(c_id) into 3 buckets row format delimited fields terminated by &apos;\t&apos;;</span><br></pre></td></tr></table></figure>

<p>桶表的数据加载，由于通标的数据加载通过hdfs dfs -put文件或者通过load data均不好使，只能通过insert overwrite</p>
<p>创建普通表，并通过insert overwriter的方式将普通表的数据通过查询的方式加载到桶表当中去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">创建普通表</span><br><span class="line">create table course_common (c_id string,c_name string,t_id string) row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line">普通表中加载数据</span><br><span class="line">load data local inpath &apos;/export/servers/hivedatas/course.csv&apos; into table course_common;</span><br><span class="line">通过insert overwrite给桶表中加载数据</span><br><span class="line">insert overwrite table course select * from course_common cluster by(c_id);</span><br></pre></td></tr></table></figure>

<h3 id="3-3-修改表结构"><a href="#3-3-修改表结构" class="headerlink" title="3.3 修改表结构"></a>3.3 修改表结构</h3><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">重命名:</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter  table  old_table_name  rename  to  new_table_name;</span><br></pre></td></tr></table></figure>

<p>把表score4修改成score5</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table score4 rename to score5;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">增加/修改列信息:</a></p>
<ul>
<li>查询表结构</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc score5;</span><br></pre></td></tr></table></figure>

<ul>
<li>添加列</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table score5 add columns (mycol string, mysco int);</span><br></pre></td></tr></table></figure>

<ul>
<li>更新列</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table score5 change column mysco mysconew int;</span><br></pre></td></tr></table></figure>

<ul>
<li>删除表</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table score5;</span><br></pre></td></tr></table></figure>

<p>1.8. hive表中加载数据</p>
<p>直接向分区表中插入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create table score3 like score;</span><br><span class="line"></span><br><span class="line">insert into table score3 partition(month =&apos;201807&apos;) values (&apos;001&apos;,&apos;002&apos;,&apos;100&apos;);</span><br></pre></td></tr></table></figure>

<p>通过查询插入数据</p>
<p>通过load方式加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/export/servers/hivedatas/score.csv&apos; overwrite into table score partition(month=&apos;201806&apos;);</span><br></pre></td></tr></table></figure>

<p>通过查询方式加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table score4 like score;</span><br><span class="line">insert overwrite table score4 partition(month = &apos;201806&apos;) select s_id,c_id,s_score from score;</span><br></pre></td></tr></table></figure>

<h2 id="4-Hive-查询语法"><a href="#4-Hive-查询语法" class="headerlink" title="4. Hive 查询语法"></a>4. Hive 查询语法</h2><h3 id="4-1-SELECT"><a href="#4-1-SELECT" class="headerlink" title="4.1. SELECT"></a>4.1. SELECT</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SELECT [ALL | DISTINCT] select_expr, select_expr, ...</span><br><span class="line">FROM table_reference</span><br><span class="line">[WHERE where_condition]</span><br><span class="line">[GROUP BY col_list [HAVING condition]]</span><br><span class="line">[CLUSTER BY col_list</span><br><span class="line">| [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]</span><br><span class="line">]</span><br><span class="line">[LIMIT number]</span><br></pre></td></tr></table></figure>

<ol>
<li>order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。</li>
<li>sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。</li>
<li>distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。</li>
<li>cluster by(字段) 除了具有distribute by的功能外，还会对该字段进行排序.</li>
</ol>
<p>因此，如果distribute 和sort字段是同一个时，此时，<code>cluster by = distribute by + sort by</code></p>
<h3 id="4-2-查询语法"><a href="#4-2-查询语法" class="headerlink" title="4.2. 查询语法"></a>4.2. 查询语法</h3><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">全表查询</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">选择特定列</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s_id ,c_id from score;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">列别名</a></p>
<p>1）重命名一个列。<br>2）便于计算。<br>3）紧跟列名，也可以在列名和别名之间加入关键字‘AS’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s_id as myid ,c_id from score;</span><br></pre></td></tr></table></figure>

<h3 id="4-3-常用函数"><a href="#4-3-常用函数" class="headerlink" title="4.3. 常用函数"></a>4.3. 常用函数</h3><ul>
<li>求总行数（count）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(1) from score;</span><br></pre></td></tr></table></figure>

<ul>
<li>求分数的最大值（max）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select max(s_score) from score;</span><br></pre></td></tr></table></figure>

<ul>
<li>求分数的最小值（min）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select min(s_score) from score;</span><br></pre></td></tr></table></figure>

<ul>
<li>求分数的总和（sum）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sum(s_score) from score;</span><br></pre></td></tr></table></figure>

<ul>
<li>求分数的平均值（avg）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select avg(s_score) from score;</span><br></pre></td></tr></table></figure>

<h3 id="4-4-LIMIT语句"><a href="#4-4-LIMIT语句" class="headerlink" title="4.4. LIMIT语句"></a>4.4. LIMIT语句</h3><p>典型的查询会返回多行数据。LIMIT子句用于限制返回的行数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score limit 3;</span><br></pre></td></tr></table></figure>

<h3 id="4-5-WHERE语句"><a href="#4-5-WHERE语句" class="headerlink" title="4.5. WHERE语句"></a>4.5. WHERE语句</h3><ol>
<li>使用WHERE 子句，将不满足条件的行过滤掉。</li>
<li>WHERE 子句紧随 FROM 子句。</li>
<li>案例实操</li>
</ol>
<p>查询出分数大于60的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score &gt; 60;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">比较运算符</a></p>
<table>
<thead>
<tr>
<th align="left">操作符</th>
<th align="left">支持的数据类型</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">A=B</td>
<td align="left">基本数据类型</td>
<td align="left">如果A等于B则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;=&gt;B</td>
<td align="left">基本数据类型</td>
<td align="left">如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td>
</tr>
<tr>
<td align="left">A&lt;&gt;B, A!=B</td>
<td align="left">基本数据类型</td>
<td align="left">A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;B</td>
<td align="left">基本数据类型</td>
<td align="left">A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&lt;=B</td>
<td align="left">基本数据类型</td>
<td align="left">A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&gt;B</td>
<td align="left">基本数据类型</td>
<td align="left">A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A&gt;=B</td>
<td align="left">基本数据类型</td>
<td align="left">A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A [NOT] BETWEEN B AND C</td>
<td align="left">基本数据类型</td>
<td align="left">如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td align="left">A IS NULL</td>
<td align="left">所有数据类型</td>
<td align="left">如果A等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">A IS NOT NULL</td>
<td align="left">所有数据类型</td>
<td align="left">如果A不等于NULL，则返回TRUE，反之返回FALSE</td>
</tr>
<tr>
<td align="left">IN(数值1, 数值2)</td>
<td align="left">所有数据类型</td>
<td align="left">使用 IN运算显示列表中的值</td>
</tr>
<tr>
<td align="left">A [NOT] LIKE B</td>
<td align="left">STRING 类型</td>
<td align="left">B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td>
</tr>
<tr>
<td align="left">A RLIKE B, A REGEXP B</td>
<td align="left">STRING</td>
<td align="left">类型 B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td>
</tr>
</tbody></table>
<ul>
<li>查询分数等于80的所有的数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score = 80;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询分数在80到100的所有数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score between 80 and 100;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询成绩为空的所有数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score is null;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询成绩是80和90的数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score in(80,90);</span><br></pre></td></tr></table></figure>

<h3 id="4-6-LIKE-和-RLIKE"><a href="#4-6-LIKE-和-RLIKE" class="headerlink" title="4.6. LIKE 和 RLIKE"></a>4.6. LIKE 和 RLIKE</h3><ol>
<li>使用LIKE运算选择类似的值</li>
<li>选择条件可以包含字符或数字:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">% 代表零个或多个字符(任意个字符)。</span><br><span class="line">_ 代表一个字符。</span><br></pre></td></tr></table></figure>

<ol>
<li><p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p>
</li>
<li><p>案例实操</p>
<ol>
<li>查找以8开头的所有成绩</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score like &apos;8%&apos;;</span><br></pre></td></tr></table></figure>

<ol>
<li>查找第二个数值为9的所有成绩数据</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score like &apos;_9%&apos;;</span><br></pre></td></tr></table></figure>

<ol>
<li>查找s_id中含1的数据</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_id rlike &apos;[1]&apos;;  #  like &apos;%1%&apos;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="4-7-逻辑运算符"><a href="#4-7-逻辑运算符" class="headerlink" title="4.7. 逻辑运算符"></a>4.7. 逻辑运算符</h3><table>
<thead>
<tr>
<th align="left">操作符</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left">AND</td>
<td align="left">逻辑并</td>
</tr>
<tr>
<td align="left">OR</td>
<td align="left">逻辑或</td>
</tr>
<tr>
<td align="left">NOT</td>
<td align="left">逻辑否</td>
</tr>
</tbody></table>
<ul>
<li>查询成绩大于80，并且s_id是01的数据</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score &gt;80 and s_id = &apos;01&apos;;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询成绩大于80，或者s_id 是01的数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_score &gt; 80 or s_id = &apos;01&apos;;</span><br></pre></td></tr></table></figure>

<ul>
<li>查询s_id 不是 01和02的学生</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score where s_id not in (&apos;01&apos;,&apos;02&apos;);</span><br></pre></td></tr></table></figure>

<h3 id="4-8-分组"><a href="#4-8-分组" class="headerlink" title="4.8. 分组"></a>4.8. 分组</h3><h4 id="GROUP-BY-语句"><a href="#GROUP-BY-语句" class="headerlink" title="GROUP BY 语句"></a>GROUP BY 语句</h4><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。<br>案例实操：</p>
<ul>
<li>计算每个学生的平均分数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s_id ,avg(s_score) from score group by s_id;</span><br></pre></td></tr></table></figure>

<ul>
<li>计算每个学生最高成绩</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s_id ,max(s_score) from score group by s_id;</span><br></pre></td></tr></table></figure>

<h4 id="HAVING-语句"><a href="#HAVING-语句" class="headerlink" title="HAVING 语句"></a>HAVING 语句</h4><ol>
<li><p>having与where不同点</p>
<ol>
<li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</li>
<li>where后面不能写分组函数，而having后面可以使用分组函数。</li>
<li>having只用于group by分组统计语句。</li>
</ol>
</li>
<li><p>案例实操：</p>
<ul>
<li>求每个学生的平均分数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s_id ,avg(s_score) from score group by s_id;</span><br></pre></td></tr></table></figure>

<ul>
<li>求每个学生平均分数大于85的人</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s_id ,avg(s_score) avgscore from score group by s_id having avgscore &gt; 85;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="4-9-JOIN-语句"><a href="#4-9-JOIN-语句" class="headerlink" title="4.9. JOIN 语句"></a>4.9. JOIN 语句</h3><h4 id="4-9-1-等值-JOIN"><a href="#4-9-1-等值-JOIN" class="headerlink" title="4.9.1. 等值 JOIN"></a>4.9.1. 等值 JOIN</h4><p>Hive支持通常的SQL JOIN语句，但是只支持等值连接，不支持非等值连接。</p>
<p>案例操作: 查询分数对应的姓名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s.s_id,s.s_score,stu.s_name,stu.s_birth  from score s  join student stu on s.s_id = stu.s_id;</span><br></pre></td></tr></table></figure>

<h4 id="4-9-2-表的别名"><a href="#4-9-2-表的别名" class="headerlink" title="4.9.2. 表的别名"></a>4.9.2. 表的别名</h4><ul>
<li><p>好处</p>
<ul>
<li>使用别名可以简化查询。</li>
<li>使用表名前缀可以提高执行效率。</li>
</ul>
</li>
<li><p>案例实操</p>
<ul>
<li>合并老师与课程表</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from techer t join course c on t.t_id = c.t_id;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="4-9-3-内连接"><a href="#4-9-3-内连接" class="headerlink" title="4.9.3. 内连接"></a>4.9.3. 内连接</h4><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from techer t inner join course c on t.t_id = c.t_id;</span><br></pre></td></tr></table></figure>

<h4 id="4-9-4-左外连接"><a href="#4-9-4-左外连接" class="headerlink" title="4.9.4. 左外连接"></a>4.9.4. 左外连接</h4><p>左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。<br>查询老师对应的课程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from techer t left join course c on t.t_id = c.t_id;</span><br></pre></td></tr></table></figure>

<h4 id="4-9-5-右外连接"><a href="#4-9-5-右外连接" class="headerlink" title="4.9.5. 右外连接"></a>4.9.5. 右外连接</h4><p>右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from teacher t right join course c on t.t_id = c.t_id;</span><br></pre></td></tr></table></figure>

<h4 id="4-9-6-多表连接"><a href="#4-9-6-多表连接" class="headerlink" title="4.9.6. 多表连接"></a>4.9.6. 多表连接</h4><p>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p>
<p>多表连接查询，查询老师对应的课程，以及对应的分数，对应的学生</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select * from teacher t</span><br><span class="line">left join course c</span><br><span class="line">on t.t_id = c.t_id</span><br><span class="line">left join score s</span><br><span class="line">on s.c_id = c.c_id</span><br><span class="line">left join student stu</span><br><span class="line">on s.s_id = stu.s_id;</span><br></pre></td></tr></table></figure>

<p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表techer和表course进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表score;进行连接操作。</p>
<h3 id="4-10-排序"><a href="#4-10-排序" class="headerlink" title="4.10. 排序"></a>4.10. 排序</h3><h4 id="4-10-1-全局排序"><a href="#4-10-1-全局排序" class="headerlink" title="4.10.1. 全局排序"></a>4.10.1. 全局排序</h4><p>Order By：全局排序，一个reduce</p>
<ol>
<li><p>使用 ORDER BY 子句排序<br>ASC（ascend）: 升序（默认）<br>DESC（descend）: 降序</p>
</li>
<li><p>ORDER BY 子句在SELECT语句的结尾。</p>
</li>
<li><p>案例实操</p>
<ol>
<li>查询学生的成绩，并按照分数降序排列</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM student s LEFT JOIN score sco ON s.s_id = sco.s_id ORDER BY sco.s_score DESC;</span><br></pre></td></tr></table></figure>

<ol>
<li>查询学生的成绩，并按照分数升序排列</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM student s LEFT JOIN score sco ON s.s_id = sco.s_id ORDER BY sco.s_score asc;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="4-10-2-按照别名排序"><a href="#4-10-2-按照别名排序" class="headerlink" title="4.10.2. 按照别名排序"></a>4.10.2. 按照别名排序</h4><p>按照分数的平均值排序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s_id ,avg(s_score) avg from score group by s_id order by avg;</span><br></pre></td></tr></table></figure>

<h4 id="4-10-3-多个列排序"><a href="#4-10-3-多个列排序" class="headerlink" title="4.10.3. 多个列排序"></a>4.10.3. 多个列排序</h4><p>按照学生id和平均成绩进行排序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select s_id ,avg(s_score) avg from score group by s_id order by s_id,avg;</span><br></pre></td></tr></table></figure>

<h4 id="4-10-4-每个MapReduce内部排序（Sort-By）局部排序"><a href="#4-10-4-每个MapReduce内部排序（Sort-By）局部排序" class="headerlink" title="4.10.4. 每个MapReduce内部排序（Sort By）局部排序"></a>4.10.4. 每个MapReduce内部排序（Sort By）局部排序</h4><p>Sort By：每个MapReduce内部进行排序，对全局结果集来说不是排序。</p>
<ol>
<li>设置reduce个数</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces=3;</span><br></pre></td></tr></table></figure>

<ol>
<li>查看设置reduce个数</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces;</span><br></pre></td></tr></table></figure>

<ol>
<li>查询成绩按照成绩降序排列</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from score sort by s_score;</span><br></pre></td></tr></table></figure>

<ol>
<li>将查询结果导入到文件中（按照成绩降序排列）</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory &apos;/export/servers/hivedatas/sort&apos; select * from score sort by s_score;</span><br></pre></td></tr></table></figure>

<h4 id="4-10-5-分区排序（DISTRIBUTE-BY）"><a href="#4-10-5-分区排序（DISTRIBUTE-BY）" class="headerlink" title="4.10.5. 分区排序（DISTRIBUTE BY）"></a>4.10.5. 分区排序（DISTRIBUTE BY）</h4><p>Distribute By：类似MR中partition，进行分区，结合sort by使用。</p>
<p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</p>
<p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p>
<p>案例实操：先按照学生id进行分区，再按照学生成绩进行排序。</p>
<ol>
<li>设置reduce的个数，将我们对应的s_id划分到对应的reduce当中去</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces=7;</span><br></pre></td></tr></table></figure>

<ol>
<li>通过distribute by 进行数据的分区</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory &apos;/export/servers/hivedatas/sort&apos; select * from score distribute by s_id sort by s_score;</span><br></pre></td></tr></table></figure>

<h4 id="4-10-6-CLUSTER-BY"><a href="#4-10-6-CLUSTER-BY" class="headerlink" title="4.10.6. CLUSTER BY"></a>4.10.6. CLUSTER BY</h4><p>当distribute by和sort by字段相同时，可以使用cluster by方式。</p>
<p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。</p>
<p>以下两种写法等价</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from score cluster by s_id;</span><br><span class="line">select * from score distribute by s_id sort by s_id;</span><br></pre></td></tr></table></figure>

<h2 id="5-Hive-Shell参数"><a href="#5-Hive-Shell参数" class="headerlink" title="5.Hive Shell参数"></a>5.Hive Shell参数</h2><h3 id="5-1-Hive命令行"><a href="#5-1-Hive命令行" class="headerlink" title="5.1 Hive命令行"></a><strong>5.1 Hive命令行</strong></h3><p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">语法结构</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S]</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">说明：</a></p>
<p>1、 -i 从文件初始化HQL。</p>
<p>2、 <code>-e从命令行执行指定的HQL</code></p>
<p>3、 <code>-f 执行HQL脚本</code></p>
<p>4、 -v 输出执行的HQL语句到控制台</p>
<p>5、 -p connect to Hive Server on port number</p>
<p>6、 -hiveconf x=y Use this to set hive/hadoop configuration variables. 设置hive运行时候的参数配置</p>
<h3 id="5-2-Hive参数配置方式"><a href="#5-2-Hive参数配置方式" class="headerlink" title="5.2 Hive参数配置方式"></a>5.2 Hive参数配置方式</h3><p>开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。</p>
<p><strong>对于一般参数，有以下三种设定方式：</strong></p>
<ul>
<li><p>配置文件</p>
</li>
<li><p>命令行参数</p>
</li>
<li><p>参数声明</p>
<p><code>配置文件</code>：Hive的配置文件包括</p>
</li>
<li><p>用户自定义配置文件：<a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">$HIVE_CONF_DIR/hive-site.xml</a></p>
</li>
<li><p>默认配置文件： <a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">$HIVE_CONF_DIR/hive-default.xml</a></p>
<p><strong>用户自定义配置会覆盖默认配置。</strong></p>
</li>
</ul>
<p>另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。</p>
<p>配置文件的设定对本机启动的所有Hive进程都有效。</p>
<p><code>命令行参数：</code>启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -hiveconf hive.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。</p>
<p><code>参数声明</code>：可以在HQL中使用SET关键字设定参数，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapred.reduce.tasks=100;</span><br></pre></td></tr></table></figure>

<p>这一设定的作用域也是session级的。</p>
<p>上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。</p>
<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">参数声明 &gt; 命令行参数 &gt; 配置文件参数（hive）</a></p>
<h2 id="6-Hive-函数"><a href="#6-Hive-函数" class="headerlink" title="6. Hive 函数"></a>6. Hive 函数</h2><h3 id="6-1-内置函数"><a href="#6-1-内置函数" class="headerlink" title="6.1. 内置函数"></a>6.1. 内置函数</h3><p>内容较多，见《Hive官方文档》</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</span><br></pre></td></tr></table></figure>

<ol>
<li><p>查看系统自带的函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show functions;</span><br></pre></td></tr></table></figure>
</li>
<li><p>显示自带的函数的用法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc function upper;</span><br></pre></td></tr></table></figure>
</li>
<li><p>详细显示自带的函数的用法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc function extended upper;</span><br></pre></td></tr></table></figure>

<p>4:常用内置函数</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#字符串连接函数： concat </span><br><span class="line">  select concat(&apos;abc&apos;,&apos;def’,&apos;gh&apos;);</span><br><span class="line">#带分隔符字符串连接函数： concat_ws </span><br><span class="line">  select concat_ws(&apos;,&apos;,&apos;abc&apos;,&apos;def&apos;,&apos;gh&apos;);</span><br><span class="line">#cast类型转换</span><br><span class="line">  select cast(1.5 as int);</span><br><span class="line">#get_json_object(json 解析函数，用来处理json，必须是json格式)</span><br><span class="line">   select get_json_object(&apos;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;age&quot;:&quot;20&quot;&#125;&apos;,&apos;$.name&apos;);</span><br><span class="line">#URL解析函数</span><br><span class="line">   select parse_url(&apos;http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&apos;, &apos;HOST&apos;);</span><br><span class="line">#explode：把map集合中每个键值对或数组中的每个元素都单独生成一行的形式</span><br></pre></td></tr></table></figure>

<h3 id="6-2-自定义函数"><a href="#6-2-自定义函数" class="headerlink" title="6.2. 自定义函数"></a>6.2. 自定义函数</h3><h4 id="6-2-1-概述"><a href="#6-2-1-概述" class="headerlink" title="6.2.1 概述:"></a>6.2.1 概述:</h4><ol>
<li>Hive 自带了一些函数，比如：max/min等，当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数(UDF).</li>
<li>根据用户自定义函数类别分为以下三种：<ol>
<li>UDF（User-Defined-Function）<ul>
<li>一进一出</li>
</ul>
</li>
<li>UDAF（User-Defined Aggregation Function）<ul>
<li>聚集函数，多进一出</li>
<li>类似于：<code>count</code>/<code>max</code>/<code>min</code></li>
</ul>
</li>
<li>UDTF（User-Defined Table-Generating Functions）<ul>
<li>一进多出</li>
<li>如 <code>lateral</code> <code>view</code> <code>explore()</code></li>
</ul>
</li>
</ol>
</li>
<li>编程步骤：<ol>
<li>继承org.apache.hadoop.hive.ql.UDF</li>
<li>需要实现evaluate函数；evaluate函数支持重载；</li>
</ol>
</li>
<li>注意事项<ol>
<li>UDF必须要有返回类型，可以返回null，但是返回类型不能为void；</li>
<li>UDF中常用Text/LongWritable等类型，不推荐使用java类型；</li>
</ol>
</li>
</ol>
<h4 id="6-2-2-UDF-开发实例"><a href="#6-2-2-UDF-开发实例" class="headerlink" title="6.2.2 UDF 开发实例"></a>6.2.2 UDF 开发实例</h4><h5 id="Step-1-创建-Maven-工程"><a href="#Step-1-创建-Maven-工程" class="headerlink" title="Step 1 创建 Maven 工程"></a>Step 1 创建 Maven 工程</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.0&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br></pre></td></tr></table></figure>

<h5 id="Step-2-开发-Java-类集成-UDF"><a href="#Step-2-开发-Java-类集成-UDF" class="headerlink" title="Step 2 开发 Java 类集成 UDF"></a>Step 2 开发 Java 类集成 UDF</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class MyUDF  extends UDF&#123;</span><br><span class="line">    public Text evaluate(final Text str)&#123;</span><br><span class="line">        String tmp_str = str.toString();</span><br><span class="line">        if(str != null &amp;&amp; !tmp_str.equals(&quot;&quot;))&#123;</span><br><span class="line">          String str_ret =   tmp_str.substring(0, 1).toUpperCase() + tmp_str.substring(1);</span><br><span class="line">          return  new Text(str_ret);</span><br><span class="line">        &#125;</span><br><span class="line">        return  new Text(&quot;&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-3-项目打包，并上传到hive的lib目录下"><a href="#Step-3-项目打包，并上传到hive的lib目录下" class="headerlink" title="Step 3 项目打包，并上传到hive的lib目录下"></a>Step 3 项目打包，并上传到hive的lib目录下</h5><p><a href="http://ppw6n93dt.bkt.clouddn.com/8dda7bfdfab0655e99a3c3b17afc422e.png" target="_blank" rel="noopener"><img src="http://ppw6n93dt.bkt.clouddn.com/8dda7bfdfab0655e99a3c3b17afc422e.png" alt="img"></a></p>
<h5 id="Step-4-添加jar包"><a href="#Step-4-添加jar包" class="headerlink" title="Step 4 添加jar包"></a>Step 4 添加jar包</h5><p>重命名我们的jar包名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/apache-hive-2.7.5-bin/lib</span><br><span class="line">mv original-day_10_hive_udf-1.0-SNAPSHOT.jar my_upper.jar</span><br></pre></td></tr></table></figure>

<p>hive的客户端添加我们的jar包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /export/servers/apache-hive-2.7.5-bin/lib/my_upper.jar;</span><br></pre></td></tr></table></figure>

<h5 id="Step-5-设置函数与我们的自定义函数关联"><a href="#Step-5-设置函数与我们的自定义函数关联" class="headerlink" title="Step 5 设置函数与我们的自定义函数关联"></a>Step 5 设置函数与我们的自定义函数关联</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create temporary function my_upper as &apos;cn.itcast.udf.ItcastUDF&apos;;</span><br></pre></td></tr></table></figure>

<h5 id="Step-6-使用自定义函数"><a href="#Step-6-使用自定义函数" class="headerlink" title="Step 6 使用自定义函数"></a>Step 6 使用自定义函数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select my_upper(&apos;abc&apos;);</span><br></pre></td></tr></table></figure>

<h2 id="7-hive的数据压缩"><a href="#7-hive的数据压缩" class="headerlink" title="7.hive的数据压缩"></a>7.hive的数据压缩</h2><p>在实际工作当中，hive当中处理的数据，一般都需要经过压缩，前期我们在学习hadoop的时候，已经配置过hadoop的压缩，我们这里的hive也是一样的可以使用压缩来节省我们的MR处理的网络带宽</p>
<h3 id="7-1-MR支持的压缩编码"><a href="#7-1-MR支持的压缩编码" class="headerlink" title="*7.1 *MR支持的压缩编码"></a>*<em>7.1 *</em>MR支持的压缩编码</h3><table>
<thead>
<tr>
<th align="left">压缩格式</th>
<th align="left">工具</th>
<th align="left">算法</th>
<th align="left">文件扩展名</th>
<th align="left">是否可切分</th>
</tr>
</thead>
<tbody><tr>
<td align="left">DEFAULT</td>
<td align="left">无</td>
<td align="left">DEFAULT</td>
<td align="left">.deflate</td>
<td align="left">否</td>
</tr>
<tr>
<td align="left">Gzip</td>
<td align="left">gzip</td>
<td align="left">DEFAULT</td>
<td align="left">.gz</td>
<td align="left">否</td>
</tr>
<tr>
<td align="left">bzip2</td>
<td align="left">bzip2</td>
<td align="left">bzip2</td>
<td align="left">.bz2</td>
<td align="left">是</td>
</tr>
<tr>
<td align="left">LZO</td>
<td align="left">lzop</td>
<td align="left">LZO</td>
<td align="left">.lzo</td>
<td align="left">否</td>
</tr>
<tr>
<td align="left">LZ4</td>
<td align="left">无</td>
<td align="left">LZ4</td>
<td align="left">.lz4</td>
<td align="left">否</td>
</tr>
<tr>
<td align="left">Snappy</td>
<td align="left">无</td>
<td align="left">Snappy</td>
<td align="left">.snappy</td>
<td align="left">否</td>
</tr>
</tbody></table>
<p>为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示</p>
<table>
<thead>
<tr>
<th align="left">压缩格式</th>
<th align="left">对应的编码/解码器</th>
</tr>
</thead>
<tbody><tr>
<td align="left">DEFLATE</td>
<td align="left">org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td align="left">gzip</td>
<td align="left">org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td align="left">bzip2</td>
<td align="left">org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td align="left">LZO</td>
<td align="left">com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td align="left">LZ4</td>
<td align="left">org.apache.hadoop.io.compress.Lz4Codec</td>
</tr>
<tr>
<td align="left">Snappy</td>
<td align="left">org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<p>压缩性能的比较</p>
<table>
<thead>
<tr>
<th align="left">压缩算法</th>
<th align="left">原始文件大小</th>
<th align="left">压缩文件大小</th>
<th align="left">压缩速度</th>
<th align="left">解压速度</th>
</tr>
</thead>
<tbody><tr>
<td align="left">gzip</td>
<td align="left">8.3GB</td>
<td align="left">1.8GB</td>
<td align="left">17.5MB/s</td>
<td align="left">58MB/s</td>
</tr>
<tr>
<td align="left">bzip2</td>
<td align="left">8.3GB</td>
<td align="left">1.1GB</td>
<td align="left">2.4MB/s</td>
<td align="left">9.5MB/s</td>
</tr>
<tr>
<td align="left">LZO</td>
<td align="left">8.3GB</td>
<td align="left">2.9GB</td>
<td align="left">49.3MB/s</td>
<td align="left">74.6MB/s</td>
</tr>
</tbody></table>
<p><a href="http://google.github.io/snappy/" target="_blank" rel="noopener">http://google.github.io/snappy/</a></p>
<p>On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about <code>250 MB/sec</code> or more and decompresses at about <code>500 MB/se</code>c or more.</p>
<h3 id="7-2-压缩配置参数"><a href="#7-2-压缩配置参数" class="headerlink" title="7.2 压缩配置参数"></a><strong>7.2 压缩配置</strong>参数</h3><p>要在Hadoop中启用压缩，可以配置如下参数（mapred-site.xml文件中）：</p>
<table>
<thead>
<tr>
<th align="left">参数</th>
<th align="left">默认值</th>
<th align="left">阶段</th>
<th align="left">建议</th>
</tr>
</thead>
<tbody><tr>
<td align="left">io.compression.codecs （在core-site.xml中配置）</td>
<td align="left">org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.Lz4Codec</td>
<td align="left">输入压缩</td>
<td align="left">Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td align="left">mapreduce.map.output.compress</td>
<td align="left">false</td>
<td align="left">mapper输出</td>
<td align="left">这个参数设为true启用压缩</td>
</tr>
<tr>
<td align="left">mapreduce.map.output.compress.codec</td>
<td align="left">org.apache.hadoop.io.compress.DefaultCodec</td>
<td align="left">mapper输出</td>
<td align="left">使用LZO、LZ4或snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress</td>
<td align="left">false</td>
<td align="left">reducer输出</td>
<td align="left">这个参数设为true启用压缩</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress.codec</td>
<td align="left">org.apache.hadoop.io.compress. DefaultCodec</td>
<td align="left">reducer输出</td>
<td align="left">使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
<tr>
<td align="left">mapreduce.output.fileoutputformat.compress.type</td>
<td align="left">RECORD</td>
<td align="left">reducer输出</td>
<td align="left">SequenceFile输出使用的压缩类型：NONE和BLOCK</td>
</tr>
</tbody></table>
<h3 id="7-3-开启Map输出阶段压缩"><a href="#7-3-开启Map输出阶段压缩" class="headerlink" title="7.3 开启Map输出阶段压缩"></a><strong>7.3 开启Map</strong>输出阶段压缩</h3><p>开启map输出阶段压缩可以减少job中map和Reduce task间数据传输量。具体配置如下：</p>
<p><strong>案例实操：</strong></p>
<p>1）开启hive中间传输数据压缩功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.compress.intermediate=true;</span><br></pre></td></tr></table></figure>

<p>2）开启mapreduce中map输出压缩功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.map.output.compress=true;</span><br></pre></td></tr></table></figure>

<p>3）设置mapreduce中map输出数据的压缩方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.map.output.compress.codec= org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>4）执行查询语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(1) from score;</span><br></pre></td></tr></table></figure>

<h3 id="7-4-开启Reduce输出阶段压缩"><a href="#7-4-开启Reduce输出阶段压缩" class="headerlink" title="7.4 开启Reduce输出阶段压缩"></a><strong>7.4</strong> 开启Reduce输出阶段压缩</h3><p>当Hive将输出写入到表中时，输出内容同样可以进行压缩。属性hive.exec.compress.output控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为true，来开启输出结果压缩功能。</p>
<p><strong>案例实操</strong>：</p>
<p>1）开启hive最终输出数据压缩功能</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.compress.output=true;</span><br></pre></td></tr></table></figure>

<p>2）开启mapreduce最终输出数据压缩</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure>

<p>3）设置mapreduce最终数据输出压缩方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.output.fileoutputformat.compress.codec = org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<p>4）设置mapreduce最终数据输出压缩为块压缩</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure>

<p>5）测试一下输出结果是否是压缩文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite local directory &apos;/export/servers/snappy&apos; select * from score distribute by s_id sort by s_id desc;</span><br></pre></td></tr></table></figure>

<h2 id="8-hive的数据存储格式"><a href="#8-hive的数据存储格式" class="headerlink" title="8.hive的数据存储格式"></a><strong>8.hive的数据存储格式</strong></h2><p>Hive支持的存储数的格式主要有：TEXTFILE（行式存储） 、SEQUENCEFILE(行式存储)、ORC（列式存储）、PARQUET（列式存储）。</p>
<h3 id="8-1-列式存储和行式存储"><a href="#8-1-列式存储和行式存储" class="headerlink" title="8.1 列式存储和行式存储"></a><strong>8.1</strong> <strong>列式存储</strong>和行式存储</h3><p><a href="https://manzhong.github.io/images/hive/wps1.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hive/wps1.jpg" alt="img"></a></p>
<p>上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p>
<p><strong>行存储的特点：</strong> 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
<p><strong>列存储的特点：</strong> 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；</span><br><span class="line">ORC和PARQUET是基于列式存储的。</span><br></pre></td></tr></table></figure>

<h3 id="8-2-常用数据存储格式"><a href="#8-2-常用数据存储格式" class="headerlink" title="8.2 常用数据存储格式"></a>8.2 常用数据存储格式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TEXTFILE格式</span><br></pre></td></tr></table></figure>

<p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ORC格式</span><br></pre></td></tr></table></figure>

<p>Orc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。</p>
<p>可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer：</p>
<ul>
<li><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">indexData</a>：某些列的索引数据</li>
<li><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">rowData</a> :真正的数据存储</li>
<li><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">StripFooter</a>：stripe的元数据信息</li>
</ul>
<p><a href="https://manzhong.github.io/images/hive/wps2.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hive/wps2.jpg" alt="img"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PARQUET格式</span><br></pre></td></tr></table></figure>

<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，</p>
<p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p>
<p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。</p>
<p><a href="https://manzhong.github.io/images/hive/wps3.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hive/wps3.jpg" alt="img"></a></p>
<h2 id="9-文件存储格式与数据压缩结合"><a href="#9-文件存储格式与数据压缩结合" class="headerlink" title="*9. *文件存储格式与数据压缩结合"></a>*<em>9. *</em>文件存储格式与数据压缩结合</h2><h3 id="9-1-压缩比和查询速度对比"><a href="#9-1-压缩比和查询速度对比" class="headerlink" title="9.1 压缩比和查询速度对比"></a>9.1 压缩比和查询速度对比</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1）TextFile</span><br></pre></td></tr></table></figure>

<p>（1）创建表，存储数据格式为TEXTFILE</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table log_text (</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">STORED AS TEXTFILE ;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &apos;/export/servers/hivedatas/log.data&apos; into table log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_text;</span><br><span class="line">2）ORC</span><br></pre></td></tr></table></figure>

<p>（1）创建表，存储数据格式为ORC</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table log_orc(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">STORED AS orc ;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into table log_orc select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_orc;</span><br><span class="line">3）Parquet</span><br></pre></td></tr></table></figure>

<p>（1）创建表，存储数据格式为parquet</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table log_parquet(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">STORED AS PARQUET ;</span><br></pre></td></tr></table></figure>

<p>（2）向表中加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into table log_parquet select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看表中数据大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_parquet;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">存储文件的压缩比总结：</a></p>
<p><strong>ORC &gt; Parquet &gt; textFile</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4)存储文件的查询速度测试：</span><br></pre></td></tr></table></figure>

<p>1）TextFile</p>
<p>hive (default)&gt; select count(*) from log_text;</p>
<p>Time taken: 21.54 seconds, Fetched: 1 row(s)</p>
<p>2）ORC</p>
<p>hive (default)&gt; select count(*) from log_orc;</p>
<p>Time taken: 20.867 seconds, Fetched: 1 row(s)</p>
<p>3）Parquet</p>
<p>hive (default)&gt; select count(*) from log_parquet;</p>
<p>Time taken: 22.922 seconds, Fetched: 1 row(s)</p>
<p><a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">存储文件的查询速度总结：</a></p>
<p><strong>ORC &gt; TextFile &gt; Parquet</strong></p>
<h3 id="9-2-ORC存储指定压缩方式"><a href="#9-2-ORC存储指定压缩方式" class="headerlink" title="9.2 ORC存储指定压缩方式"></a>9.2 ORC存储指定压缩方式</h3><p>官网：<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>
<p>ORC存储方式的压缩：</p>
<table>
<thead>
<tr>
<th align="left">Key</th>
<th align="left">Default</th>
<th align="left">Notes</th>
</tr>
</thead>
<tbody><tr>
<td align="left">orc.compress</td>
<td align="left"><code>ZLIB</code></td>
<td align="left">high level compression (one of NONE, ZLIB, SNAPPY)</td>
</tr>
<tr>
<td align="left">orc.compress.size</td>
<td align="left">262,144</td>
<td align="left">number of bytes in each compression chunk</td>
</tr>
<tr>
<td align="left">orc.stripe.size</td>
<td align="left">67,108,864</td>
<td align="left">number of bytes in each stripe</td>
</tr>
<tr>
<td align="left">orc.row.index.stride</td>
<td align="left">10,000</td>
<td align="left">number of rows between index entries (must be &gt;= 1000)</td>
</tr>
<tr>
<td align="left">orc.create.index</td>
<td align="left">true</td>
<td align="left">whether to create row indexes</td>
</tr>
<tr>
<td align="left">orc.bloom.filter.columns</td>
<td align="left">“”</td>
<td align="left">comma separated list of column names for which bloom filter should be created</td>
</tr>
<tr>
<td align="left">orc.bloom.filter.fpp</td>
<td align="left">0.05</td>
<td align="left">false positive probability for bloom filter (must &gt;0.0 and &lt;1.0)</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1）创建一个非压缩的的ORC存储方式</span><br></pre></td></tr></table></figure>

<p>（1）建表语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table log_orc_none(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">STORED AS orc tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;);</span><br></pre></td></tr></table></figure>

<p>（2）插入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into table log_orc_none select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看插入后数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_orc_none;</span><br><span class="line">2）创建一个SNAPPY压缩的ORC存储方式</span><br></pre></td></tr></table></figure>

<p>（1）建表语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table log_orc_snappy(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos;</span><br><span class="line">STORED AS orc tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;);</span><br></pre></td></tr></table></figure>

<p>（2）插入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into table log_orc_snappy select * from log_text ;</span><br></pre></td></tr></table></figure>

<p>（3）查看插入后数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -du -h /user/hive/warehouse/myhive.db/log_orc_snappy ;</span><br></pre></td></tr></table></figure>

<h3 id="9-3-存储方式和压缩总结："><a href="#9-3-存储方式和压缩总结：" class="headerlink" title="9.3 存储方式和压缩总结："></a>9.3 存储方式和压缩总结：</h3><p> 在实际的项目开发当中，hive表的数据存储格式一般选择：<a href="https://manzhong.github.io/2017/07/07/Hive/" target="_blank" rel="noopener">orc或parquet。压缩方式一般选择snappy</a>。</p>
<h2 id="10-hive调优"><a href="#10-hive调优" class="headerlink" title="10.hive调优"></a>10.hive调优</h2><h3 id="10-1-Fetch抓取"><a href="#10-1-Fetch抓取" class="headerlink" title="10.1 Fetch抓取"></a><strong>10.1</strong> Fetch抓取</h3><p>Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM score;在这种情况下，Hive可以简单地读取score对应的存储目录下的文件，然后输出查询结果到控制台。通过设置hive.fetch.task.conversion参数,可以控制查询语句是否走MapReduce.</p>
<p>案例实操：</p>
<p>1）把hive.fetch.task.conversion设置成none，然后执行查询语句，都会执行mapreduce程序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">set hive.fetch.task.conversion=none;</span><br><span class="line"></span><br><span class="line">select * from score;</span><br><span class="line">select s_score from score;</span><br><span class="line">select s_score from score limit 3;</span><br></pre></td></tr></table></figure>

<p>2）把hive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">set hive.fetch.task.conversion=more;</span><br><span class="line"></span><br><span class="line">select * from score;</span><br><span class="line">select s_score from score;</span><br><span class="line">select s_score from score limit 3;</span><br></pre></td></tr></table></figure>

<h3 id="10-2-本地模式"><a href="#10-2-本地模式" class="headerlink" title="10.2 本地模式"></a>10.2 本地模式</h3><p>大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。</p>
<p>用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。</p>
<p>案例实操：</p>
<p>1）开启本地模式，并执行查询语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.mode.local.auto=true; </span><br><span class="line">select * from score cluster by s_id;</span><br></pre></td></tr></table></figure>

<p>2）关闭本地模式，并执行查询语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.mode.local.auto=false; </span><br><span class="line">select * from score cluster by s_id;</span><br></pre></td></tr></table></figure>

<h3 id="10-3-MapJoin"><a href="#10-3-MapJoin" class="headerlink" title="10.3 MapJoin"></a>10.3 MapJoin</h3><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会在Reduce阶段完成join,容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。</p>
<p>1）开启MapJoin参数设置：</p>
<p>（1）设置自动选择Mapjoin</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.auto.convert.join = true;</span><br></pre></td></tr></table></figure>

<p>（2）大表小表的阈值设置（默认25M以下认为是小表）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.mapjoin.smalltable.filesize=25123456;</span><br></pre></td></tr></table></figure>

<h3 id="10-4-Group-By"><a href="#10-4-Group-By" class="headerlink" title="10.4 Group By"></a>10.4 Group By</h3><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。</p>
<p>开启Map端聚合参数设置</p>
<p>（1）是否在Map端进行聚合，默认为True</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.map.aggr = true;</span><br></pre></td></tr></table></figure>

<p>（2）在Map端进行聚合操作的条目数目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.groupby.mapaggr.checkinterval = 100000;</span><br></pre></td></tr></table></figure>

<p>（3）有数据倾斜的时候进行负载均衡（默认是false）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.groupby.skewindata = true;</span><br></pre></td></tr></table></figure>

<p>当选项设定为 true，生成的查询计划会有两个MR Job。</p>
<p>第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；</p>
<p>第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p>
<h3 id="10-5-Count-distinct"><a href="#10-5-Count-distinct" class="headerlink" title="10.5 Count(distinct)"></a>10.5 Count(distinct)</h3><p>数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct s_id) from score;</span><br><span class="line">select count(s_id) from (select id from score group by s_id) a;</span><br></pre></td></tr></table></figure>

<p>虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得的。</p>
<h3 id="10-6-笛卡尔积"><a href="#10-6-笛卡尔积" class="headerlink" title="10.6 笛卡尔积"></a>10.6 笛卡尔积</h3><p>尽量避免笛卡尔积，即避免join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。</p>
<h3 id="10-7-动态分区调整"><a href="#10-7-动态分区调整" class="headerlink" title="10.7 动态分区调整"></a>10.7 <strong>动态分区</strong>调整</h3><p>往hive分区表中插入数据时，hive提供了一个动态分区功能，其可以基于查询参数的位置去推断分区的名称，从而建立分区。使用Hive的动态分区，需要进行相应的配置。</p>
<p>Hive的动态分区是以第一个表的分区规则，来对应第二个表的分区规则，将第一个表的所有分区，全部拷贝到第二个表中来，第二个表在加载数据的时候，不需要指定分区了，直接用第一个表的分区即可</p>
<h4 id="10-7-1-开启动态分区参数设置"><a href="#10-7-1-开启动态分区参数设置" class="headerlink" title="10.7.1 开启动态分区参数设置"></a>10.7.1 开启动态分区参数设置</h4><p>（1）开启动态分区功能（默认true，开启）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition=true;</span><br></pre></td></tr></table></figure>

<p>（2）设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br></pre></td></tr></table></figure>

<p>（3）在所有执行MR的节点上，最大一共可以创建多少个动态分区。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set  hive.exec.max.dynamic.partitions=1000;</span><br></pre></td></tr></table></figure>

<p>（4）在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.max.dynamic.partitions.pernode=100</span><br></pre></td></tr></table></figure>

<p>（5）整个MR Job中，最大可以创建多少个HDFS文件。</p>
<p> 在linux系统当中，每个linux用户最多可以开启1024个进程，每一个进程最多可以打开2048个文件，即持有2048个文件句柄，下面这个值越大，就可以打开文件句柄越大</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.max.created.files=100000;</span><br></pre></td></tr></table></figure>

<p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.error.on.empty.partition=false;</span><br></pre></td></tr></table></figure>

<h4 id="10-7-2-案例操作"><a href="#10-7-2-案例操作" class="headerlink" title="10.7.2 案例操作"></a>10.7.2 案例操作</h4><p>需求：将ori中的数据按照时间(如：20111231234568)，插入到目标表ori_partitioned的相应分区中。</p>
<p>（1）准备数据原表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table ori_partitioned(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) </span><br><span class="line">PARTITIONED BY (p_time bigint) </span><br><span class="line">row format delimited fields terminated by &apos;\t&apos;;</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/export/servers/hivedatas/small_data&apos; into  table ori_partitioned partition (p_time=&apos;20111230000010&apos;);</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/export/servers/hivedatas/small_data&apos; into  table ori_partitioned partition (p_time=&apos;20111230000011&apos;);</span><br></pre></td></tr></table></figure>

<p>（2）创建目标分区表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table ori_partitioned_target(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) PARTITIONED BY (p_time STRING) row format delimited fields terminated by &apos;\t&apos;</span><br></pre></td></tr></table></figure>

<p>（3）向目标分区表加载数据</p>
<p>如果按照之前介绍的往指定一个分区中Insert数据，那么这个需求很不容易实现。这时候就需要使用动态分区来实现。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INSERT overwrite TABLE ori_partitioned_target PARTITION (p_time)</span><br><span class="line">SELECT id, time, uid, keyword, url_rank, click_num, click_url, p_time</span><br><span class="line">FROM ori_partitioned;</span><br></pre></td></tr></table></figure>

<p>注意：在SELECT子句的最后几个字段，必须对应前面<strong>PARTITION (p_time)</strong>中指定的分区字段，包括顺序。</p>
<p>(4)查看分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show partitions ori_partitioned_target;</span><br></pre></td></tr></table></figure>

<h3 id="10-8-并行执行"><a href="#10-8-并行执行" class="headerlink" title="10.8 并行执行"></a>10.8 并行执行</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p>
<p> 通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.parallel = true;</span><br></pre></td></tr></table></figure>

<p>当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。</p>
<h3 id="10-9-严格模式"><a href="#10-9-严格模式" class="headerlink" title="10.9 严格模式"></a>10.9 严格模式</h3><p>Hive提供了一个严格模式，可以防止用户执行那些可能意向不到的不好的影响的查询。</p>
<p> 通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hive.mapred.mode = strict; #开启严格模式</span><br><span class="line">set hive.mapred.mode = nostrict; #开启非严格模式</span><br></pre></td></tr></table></figure>

<p>1）<code>对于分区表，在where语句中必须含有分区字段作为过滤条件来限制范围，否则不允许执行</code>。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p>
<p>2）<code>对于使用了order by语句的查询，要求必须使用limit语句</code>。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p>
<p>3）<code>限制笛卡尔积的查询</code>。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p>
<h3 id="10-10-JVM重用"><a href="#10-10-JVM重用" class="headerlink" title="10.10 JVM重用"></a>10.10 <strong>JVM重用</strong></h3><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p>
<p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p>
<p>我们也可以在hive当中通过</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set  mapred.job.reuse.jvm.num.tasks=10;</span><br></pre></td></tr></table></figure>

<p>这个设置来设置我们的jvm重用</p>
<p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p>
<h3 id="10-11-推测执行"><a href="#10-11-推测执行" class="headerlink" title="10.11 推测执行"></a>10.11 <strong>推测执行</strong></h3><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制<code>，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</code></p>
<p>设置开启推测执行参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set mapred.map.tasks.speculative.execution=true</span><br><span class="line">set mapred.reduce.tasks.speculative.execution=true</span><br><span class="line">set hive.mapred.reduce.tasks.speculative.execution=true;</span><br></pre></td></tr></table></figure>

<p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Hive/" data-id="cjz2c0w9j000xagu5a4yqeqde" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Flume" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Flume/" class="article-date">
  <time datetime="2019-08-08T03:41:40.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Flume/">Flume</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Apache Flume</p>
<h1 id="一-概述"><a href="#一-概述" class="headerlink" title="一 概述"></a>一 概述</h1><p> flume是一个高可用,高可靠的,分布式的海量日志采集,聚合和传输的软件.特别指的是数据流转的过程，或者说是数据搬运的过程。把数据从一个存储介质通过flume传递到另一个存储介质中。</p>
<p> 核心是把数据从数据源(source)收集过来,再将收集的数据送到指定的目的地(sink).为保证一定能送到目的地,再送到目的地之前,会先缓存数据(channel),等到数据到达目的地后,flume删除缓存.</p>
<p> flume支持定义各类数据发送方,用于收集各种类型数据,同时，Flume支持定制各种数据接受方，用于最终存储数据。一般的采集需求，通过对flume的简单配置即可实现。针对特殊场景也具备良好的自定义扩展能力。因此，flume可以适用于大部分的日常数据采集场景。</p>
<h1 id="二-运行机制"><a href="#二-运行机制" class="headerlink" title="二 运行机制"></a>二 运行机制</h1><p><a href="https://manzhong.github.io/images/flume/flume%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/flume/flume%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6.png" alt="img"></a></p>
<p> 1 核心组件</p>
<ul>
<li><p>source ：用于对接各个不同的数据源</p>
</li>
<li><p>sink：用于对接各个不同存储数据的目的地（数据下沉地）</p>
</li>
<li><p>channel：用于中间临时存储缓存数据</p>
<p>2 机制</p>
</li>
<li><p>flume本身是java程序 在需要采集数据机器上启动 —–&gt;agent进程</p>
</li>
<li><p>agent进程里面包含了：source sink channel</p>
</li>
<li><p>在flume中，数据被包装成event 真是的数据是放在event body中<br>event是flume中最小的数据单元</p>
<p>3 架构</p>
</li>
<li><p>简单架构</p>
<p>只需要部署一个agent进程即可</p>
<p><a href="https://manzhong.github.io/images/flume/%E7%AE%80%E5%8D%95%E6%9E%B6%E6%9E%84.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/flume/%E7%AE%80%E5%8D%95%E6%9E%B6%E6%9E%84.png" alt="img"></a></p>
</li>
<li><p>复杂架构</p>
<p>多个agent之间的串联 相当于大家手拉手共同完成数据的采集传输工作</p>
<p>在串联的架构中没有主从之分 大家的地位都是一样的</p>
</li>
</ul>
<p><a href="https://manzhong.github.io/images/flume/%E5%A4%8D%E6%9D%82%E6%9E%B6%E6%9E%84.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/flume/%E5%A4%8D%E6%9D%82%E6%9E%B6%E6%9E%84.png" alt="img"></a></p>
<h1 id="三-安装部署"><a href="#三-安装部署" class="headerlink" title="三 安装部署"></a>三 安装部署</h1><p> 上传安装包到数据源所在节点上然后解压 tar -zxvf apache-flume-1.8.0-bin.tar.gz然后进入flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME</p>
<p>flume开发步骤</p>
<p>在conf中,就是根据业务需求编写采集方案配置文件</p>
<ul>
<li>文件名见名知意 通常以souce——sink.conf</li>
<li>具体需要描述清楚sink source channel组件配置信息 结合官网配置</li>
</ul>
<p>启动命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf --conf-file conf/netcat-logger.conf --name a1 -Dflume.root.logger=INFO,console  命令完整版</span><br><span class="line">bin/flume-ng agent -c ./conf -f ./conf/spool-hdfs.conf -n a1 -Dflume.root.logger=INFO,console  命令精简版</span><br><span class="line">--conf指定配置文件的目录</span><br><span class="line">--conf-file指定采集方案路径</span><br><span class="line">--name  agent进程名字 要跟采集方案中保持一致</span><br></pre></td></tr></table></figure>

<h1 id="四-测试环境"><a href="#四-测试环境" class="headerlink" title="四 测试环境:"></a>四 测试环境:</h1><p>在conf目录下配置:vi netcat-logger.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 定义这个agent中各组件的名字</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># 描述和配置source组件：r1</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># 描述和配置sink组件：k1</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># 描述和配置channel组件，此处使用是内存缓存的方式</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># 描述和配置source  channel   sink之间的连接关系</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>启动agent去采集数据:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf --conf-file conf/netcat-logger.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>测试:</p>
<p>先要往agent采集监听的端口上发送数据，让agent有数据可采。</p>
<p>随便在一个能跟agent节点联网的机器上：</p>
<p>telnetanget-hostname port （telnet localhost 44444）</p>
<h1 id="五-采集目录到hdfs"><a href="#五-采集目录到hdfs" class="headerlink" title="五 采集目录到hdfs:"></a>五 采集目录到hdfs:</h1><p>需求:<strong>服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去</strong></p>
<p>l 采集源，即source——监控文件目录 : <strong>spooldir</strong></p>
<p>l 下沉目标，即sink——HDFS文件系统 : <strong>hdfs sink</strong></p>
<p>l source和sink之间的传递通道——channel，可用file channel 也可以用内存channel</p>
<p>编写配置文件:vim spooldir-hdfs.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">##注意：不能往监控目中重复丢同名文件</span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.spoolDir = /root/logs</span><br><span class="line">a1.sources.r1.fileHeader = true</span><br><span class="line"></span><br><span class="line"># Describe the sink 目的地</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 3</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 20</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 5</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 1</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory </span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>参数解释(sink):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">roll控制写入hdfs文件 以何种方式进行滚动</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 3  以时间间隔</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 20     以文件大小</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 5     </span><br><span class="line">以event个数如果三个都配置  谁先满足谁触发滚动如果不想以某种属性滚动  设置为0即可</span><br><span class="line"></span><br><span class="line">是否开启时间上的舍弃  控制文件夹以多少时间间隔滚动</span><br><span class="line">以下述为例：就会每10分钟生成一个文件夹</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br></pre></td></tr></table></figure>

<p>Channel参数解释：</p>
<p>capacity：默认该通道中最大的可以存储的event数量</p>
<p>trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量</p>
<p>注意:</p>
<ul>
<li>注意其监控的文件夹下面不能有同名文件的产生</li>
<li>如果有 报错且罢工 后续就不再进行数据的监视采集了</li>
<li>在企业中通常给文件追加时间戳命名的方式保证文件不会重名</li>
</ul>
<p>启动同上:</p>
<p>测试 :往源目录里存入文件</p>
<h1 id="六-采集文件到HDFS"><a href="#六-采集文件到HDFS" class="headerlink" title="六 采集文件到HDFS"></a>六 采集文件到HDFS</h1><p>需求:比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs</p>
<p>l 采集源，即source——监控文件内容更新 : exec ‘tail-F file’</p>
<p>l 下沉目标，即sink——HDFS文件系统 : hdfssink</p>
<p>l Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel</p>
<p>编写配置文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /root/logs/test.log</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/tailout/%y-%m-%d/%H%M/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = events-</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 10</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 3</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 20</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 5</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 1</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>参数解析:</p>
<p>· rollInterval</p>
<p>默认值：30</p>
<p>hdfssink间隔多长将临时文件滚动成最终目标文件，单位：秒；</p>
<p>如果设置成0，则表示不根据时间来滚动文件；</p>
<p>注：滚动（roll）指的是，hdfs sink将临时文件重命名成最终目标文件，并新打开一个临时文件来写入数据；</p>
<p>· <strong>rollSize\</strong>****</p>
<p>默认值：1024</p>
<p>当临时文件达到该大小（单位：bytes）时，滚动成目标文件；</p>
<p>如果设置成0，则表示不根据临时文件大小来滚动文件；</p>
<p>· <strong>rollCount\</strong>****</p>
<p>默认值：10</p>
<p>当events数据达到该数量时候，将临时文件滚动成目标文件；</p>
<p>如果设置成0，则表示不根据events数据来滚动文件；</p>
<p>· <strong>round\</strong>****</p>
<p>默认值：false</p>
<p>是否启用时间上的“舍弃”，这里的“舍弃”，类似于“四舍五入”。</p>
<p>· <strong>roundValue\</strong>****</p>
<p>默认值：1</p>
<p>时间上进行“舍弃”的值；</p>
<p>· <strong>roundUnit\</strong>****</p>
<p>默认值：seconds</p>
<p>时间上进行“舍弃”的单位，包含：second,minute,hour</p>
<p>示例：</p>
<p>a1.sinks.k1.hdfs.path= /flume/events/%y-%m-%d/%H%M/%S</p>
<p>a1.sinks.k1.hdfs.round= true</p>
<p>a1.sinks.k1.hdfs.roundValue= 10</p>
<p>a1.sinks.k1.hdfs.roundUnit= minute</p>
<p>当时间为2015-10-16 17:38:59时候，hdfs.path依然会被解析为：</p>
<p>/flume/events/20151016/17:30/00</p>
<p>因为设置的是舍弃10分钟内的时间，因此，该目录每10分钟新生成一个。</p>
<p>启动同上:</p>
<p>测试:exec source 可以执行指定的linux command 把命令的结果作为数据进行收集</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">while true; do date &gt;&gt; /root/logs/test.log;done</span><br><span class="line">使用该脚本模拟数据实时变化的过程</span><br></pre></td></tr></table></figure>

<h1 id="七-Flume的load-balance、failover"><a href="#七-Flume的load-balance、failover" class="headerlink" title="七 Flume的load-balance、failover"></a>七 Flume的load-balance、failover</h1><h2 id="1-负载均衡"><a href="#1-负载均衡" class="headerlink" title="1 负载均衡"></a>1 负载均衡</h2><ul>
<li>所谓的负载均衡 用于解决一个进程或者程序处理不了所有请求 多个进程一起处理的场景</li>
<li>同一个请求只能交给一个进行处理 避免数据重复</li>
<li>如何分配请求就涉及到了负载均衡的算法：轮询（round_robin） 随机（random） 权重</li>
</ul>
<p><a href="https://manzhong.github.io/images/flume/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/flume/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1.png" alt="img"></a></p>
<p>例如三节点配置:</p>
<p>主节点:vim exec-avro.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#agent1 name</span><br><span class="line">agent1.channels = c1</span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set gruop</span><br><span class="line">agent1.sinkgroups = g1</span><br><span class="line"></span><br><span class="line">#set channel</span><br><span class="line">agent1.channels.c1.type = memory</span><br><span class="line">agent1.channels.c1.capacity = 1000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># set sources</span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line">agent1.sources.r1.type = exec</span><br><span class="line">agent1.sources.r1.command = tail -F /root/logs/123.log</span><br><span class="line"></span><br><span class="line"># set sink1</span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.type = avro</span><br><span class="line">agent1.sinks.k1.hostname = node02</span><br><span class="line">agent1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"># set sink2</span><br><span class="line">agent1.sinks.k2.channel = c1</span><br><span class="line">agent1.sinks.k2.type = avro</span><br><span class="line">agent1.sinks.k2.hostname = node03</span><br><span class="line">agent1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line">#set sink group</span><br><span class="line">agent1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set failover</span><br><span class="line">agent1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">agent1.sinkgroups.g1.processor.backoff = true  #如果开启，则将失败的sink放入黑名单</span><br><span class="line"># round_robin 轮询  还支持random 随机</span><br><span class="line">agent1.sinkgroups.g1.processor.selector = round_robin </span><br><span class="line">agent1.sinkgroups.g1.processor.selector.maxTimeOut=10000 #在黑名单放置的超时时间，超时结束时，若仍然无法接收，则超时时间呈指数增长</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#启动</span><br><span class="line">bin/flume-ng agent -c conf -f conf/exec-avro.conf -n agent1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>第二节点:vim avro-logger.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.bind = node02</span><br><span class="line">a1.sources.r1.port = 52020</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line">#启动bin/flume-ng agent -c conf -f conf/avro-logger.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>第三节点:vim avro-logger.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.bind = node03</span><br><span class="line">a1.sources.r1.port = 52020</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#启动</span><br><span class="line">bin/flume-ng agent -c conf -f conf/avro-logger.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>flume串联跨网络传输数据</p>
<ul>
<li><p>avro sink</p>
</li>
<li><p>avro source</p>
<p>使用上述两个组件指定绑定的端口ip 就可以满足数据跨网络的传递 通常用于flume串联架构中</p>
</li>
<li><p>flume串联启动</p>
<p>通常从远离数据源的那一级开始启动</p>
</li>
</ul>
<h2 id="2-flume-failover"><a href="#2-flume-failover" class="headerlink" title="2 flume failover"></a>2 flume failover</h2><ul>
<li>容错又称之为故障转移 容忍错误的发生。</li>
<li>通常用于解决单点故障 给容易出故障的地方设置备份</li>
<li>备份越多 容错能力越强 但是资源的浪费越严重</li>
</ul>
<p>具体流程类似loadbalance，但是内部处理机制与load balance完全不同。</p>
<p> Failover Sink Processor维护一个优先级Sink组件列表，只要有一个Sink组件可用，Event就被传递到下一个组件。故障转移机制的作用是将失败的Sink降级到一个池，在这些池中它们被分配一个冷却时间，随着故障的连续，在重试之前冷却时间增加。一旦Sink成功发送一个事件，它将恢复到活动池。 Sink具有与之相关的优先级，数量越大，优先级越高。</p>
<p> 例如，具有优先级为100的sink在优先级为80的Sink之前被激活。如果在发送事件时汇聚失败，则接下来将尝试下一个具有最高优先级的Sink发送事件。如果没有指定优先级，则根据在配置中指定Sink的顺序来确定优先级。</p>
<p>主节点: vim exec-avro.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">#agent1 name</span><br><span class="line">agent1.channels = c1</span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set gruop</span><br><span class="line">agent1.sinkgroups = g1</span><br><span class="line"></span><br><span class="line">#set channel</span><br><span class="line">agent1.channels.c1.type = memory</span><br><span class="line">agent1.channels.c1.capacity = 1000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># set sources</span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line">agent1.sources.r1.type = exec</span><br><span class="line">agent1.sources.r1.command = tail -F /root/logs/456.log</span><br><span class="line"></span><br><span class="line"># set sink1</span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.type = avro</span><br><span class="line">agent1.sinks.k1.hostname = node02</span><br><span class="line">agent1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"># set sink2</span><br><span class="line">agent1.sinks.k2.channel = c1</span><br><span class="line">agent1.sinks.k2.type = avro</span><br><span class="line">agent1.sinks.k2.hostname = node03</span><br><span class="line">agent1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line">#set sink group</span><br><span class="line">agent1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set failover</span><br><span class="line">agent1.sinkgroups.g1.processor.type = failover</span><br><span class="line">agent1.sinkgroups.g1.processor.priority.k1 = 10  #优先级值, 绝对值越大表示优先级越高</span><br><span class="line">agent1.sinkgroups.g1.processor.priority.k2 = 1</span><br><span class="line">agent1.sinkgroups.g1.processor.maxpenalty = 10000  #失败的Sink的最大回退期（millis）</span><br><span class="line"></span><br><span class="line">#启动</span><br><span class="line">bin/flume-ng agent -c conf -f conf/exec-avro.conf -n agent1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>第二节点与第三节点配置 同load-balance的</p>
<h1 id="八-flume拦截器"><a href="#八-flume拦截器" class="headerlink" title="八 flume拦截器"></a>八 flume拦截器</h1><h2 id="1-静态拦截器-日志的采集与汇总"><a href="#1-静态拦截器-日志的采集与汇总" class="headerlink" title="1 静态拦截器 日志的采集与汇总"></a>1 静态拦截器 日志的采集与汇总</h2><p>需求:</p>
<p>A、B两台日志服务机器实时生产日志主要类型为access.log、nginx.log、web.log</p>
<p>把A、B 机器中的access.log、nginx.log、web.log 采集汇总到C机器上然后统一收集到hdfs中</p>
<p>数据流程处理分析:</p>
<p><a href="https://manzhong.github.io/images/flume/flume%E6%8B%A6%E6%88%AA%E5%99%A8%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/flume/flume%E6%8B%A6%E6%88%AA%E5%99%A8%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6.png" alt="img"></a></p>
<p>在第一台节点上配置 vim exec_source_avro_sink.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent </span><br><span class="line">a1.sources = r1 r2 r3</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /root/logs/access.log</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = static</span><br><span class="line">a1.sources.r1.interceptors.i1.key = type</span><br><span class="line">a1.sources.r1.interceptors.i1.value = access</span><br><span class="line"></span><br><span class="line">a1.sources.r2.type = exec</span><br><span class="line">a1.sources.r2.command = tail -F /root/logs/nginx.log</span><br><span class="line">a1.sources.r2.interceptors = i2</span><br><span class="line">a1.sources.r2.interceptors.i2.type = static</span><br><span class="line">a1.sources.r2.interceptors.i2.key = type</span><br><span class="line">a1.sources.r2.interceptors.i2.value = nginx</span><br><span class="line"></span><br><span class="line">a1.sources.r3.type = exec</span><br><span class="line">a1.sources.r3.command = tail -F /root/logs/web.log</span><br><span class="line">a1.sources.r3.interceptors = i3</span><br><span class="line">a1.sources.r3.interceptors.i3.type = static</span><br><span class="line">a1.sources.r3.interceptors.i3.key = type</span><br><span class="line">a1.sources.r3.interceptors.i3.value = web</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = node02</span><br><span class="line">a1.sinks.k1.port = 41414</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 2000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r2.channels = c1</span><br><span class="line">a1.sources.r3.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>在第三台上配置vim avro_source_hdfs_sink.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">#定义agent名， source、channel、sink的名称</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义source</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = node02</span><br><span class="line">a1.sources.r1.port =41414</span><br><span class="line"></span><br><span class="line">#添加时间拦截器</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = org.apache.flume.interceptor.TimestampInterceptor$Builder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#定义channels</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 20000</span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line"></span><br><span class="line">#定义sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=hdfs://node01:9000/source/logs/%&#123;type&#125;/%Y%m%d</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix =events</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat = Text</span><br><span class="line">#时间类型</span><br><span class="line">#a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#生成的文件不按条数生成</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">#生成的文件不按时间生成</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 20</span><br><span class="line">#生成的文件按大小生成</span><br><span class="line">a1.sinks.k1.hdfs.rollSize  = 10485760</span><br><span class="line">#批量写入hdfs的个数</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 20</span><br><span class="line">flume操作hdfs的线程数（包括新建，写入等）</span><br><span class="line">a1.sinks.k1.hdfs.threadsPoolSize=10</span><br><span class="line">#操作hdfs超时时间</span><br><span class="line">a1.sinks.k1.hdfs.callTimeout=30000</span><br><span class="line"></span><br><span class="line">#组装source、channel、sink</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>使用静态拦截器前后对比:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如果没有使用静态拦截器</span><br><span class="line">Event: &#123; headers:&#123;&#125; body:  36 Sun Jun  2 18:26 &#125;</span><br><span class="line"></span><br><span class="line">使用静态拦截器之后 自己添加kv标识对</span><br><span class="line">Event: &#123; headers:&#123;type=access&#125; body:  36 Sun Jun  2 18:26 &#125;</span><br><span class="line">Event: &#123; headers:&#123;type=nginx&#125; body:  36 Sun Jun  2 18:26 &#125;</span><br><span class="line">Event: &#123; headers:&#123;type=web&#125; body:  36 Sun Jun  2 18:26 &#125;</span><br></pre></td></tr></table></figure>

<p>后续在存放数据的时候可以使用flume的规则语法获取到拦截器添加的kv内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%&#123;type&#125;</span><br></pre></td></tr></table></figure>

<p>模拟数据实时产生:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while true; do echo &quot;access access.....&quot; &gt;&gt; /root/logs/access.log;sleep 0.5;done</span><br><span class="line">while true; do echo &quot;web web.....&quot; &gt;&gt; /root/logs/web.log;sleep 0.5;done</span><br><span class="line">while true; do echo &quot;nginx nginx.....&quot; &gt;&gt; /root/logs/nginx.log;sleep 0.5;done</span><br></pre></td></tr></table></figure>

<p>配置完成后:</p>
<p>配置完成之后，在服务器A上的/root/data有数据文件access.log、nginx.log、web.log。</p>
<p>先启动服务器C上的flume，启动命令在flume安装目录下执行 ：</p>
<p>bin/flume-ng agent -c conf -f conf/avro_source_hdfs_sink.conf -name a1 -Dflume.root.logger=DEBUG,console</p>
<p>然后在启动服务器上的A，启动命令</p>
<p>在flume安装目录下执行 ：</p>
<p>bin/flume-ng agent -c conf -f conf/exec_source_avro_sink.conf -name a1 -Dflume.root.logger=DEBUG,console</p>
<h2 id="2-自定义拦截器"><a href="#2-自定义拦截器" class="headerlink" title="2 自定义拦截器"></a>2 自定义拦截器</h2><p>拦截器简介:</p>
<p> Flume有各种自带的拦截器，比如：TimestampInterceptor、HostInterceptor、RegexExtractorInterceptor等，通过使用不同的拦截器，实现不同的功能。但是以上的这些拦截器，不能改变原有日志数据的内容或者对日志信息添加一定的处理逻辑，当一条日志信息有几十个甚至上百个字段的时候，在传统的Flume处理下，收集到的日志还是会有对应这么多的字段，也不能对你想要的字段进行对应的处理。</p>
<p>自定义拦截器:</p>
<p>根据实际业务的需求，为了更好的满足数据在应用层的处理，通过自定义Flume拦截器，过滤掉不需要的字段，并对指定字段加密处理，将源数据进行预处理。减少了数据的传输量，降低了存储的开销.</p>
<p>分为两部分;</p>
<p>1 编写java代码,自定义拦截器</p>
<p>pom.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</span><br><span class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;cn.baidu.cloud&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;example-flume-intercepter&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.8.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.8.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.0&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">                    &lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.1&lt;/version&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                        &lt;/goals&gt;</span><br><span class="line">                        &lt;configuration&gt;</span><br><span class="line">                            &lt;filters&gt;</span><br><span class="line">                                &lt;filter&gt;</span><br><span class="line">                                    &lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class="line">                                    &lt;excludes&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span><br><span class="line">                                    &lt;/excludes&gt;</span><br><span class="line">                                &lt;/filter&gt;</span><br><span class="line">                            &lt;/filters&gt;</span><br><span class="line">                            &lt;transformers&gt;</span><br><span class="line">                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;</span><br><span class="line">                                    &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">                                &lt;/transformer&gt;</span><br><span class="line">                            &lt;/transformers&gt;</span><br><span class="line">                        &lt;/configuration&gt;</span><br><span class="line">                    &lt;/execution&gt;</span><br><span class="line">                &lt;/executions&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure>

<p>java代码:</p>
<ol>
<li>定义一个类CustomParameterInterceptor实现Interceptor接口。</li>
<li>在CustomParameterInterceptor类中定义变量，这些变量是需要到<br>Flume的配置文件中进行配置使用的。每一行字段间的分隔符(fields_separator)、通过分隔符分隔后，所需要列字段的下标（indexs）、多个下标使用的分隔符（indexs_separator)、多个下标使用的分隔符（indexs_separator)。</li>
<li>添加CustomParameterInterceptor的有参构造方法。并对相应的变量进行处理</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import com.google.common.base.Charsets;</span><br><span class="line">import org.apache.flume.Context;</span><br><span class="line">import org.apache.flume.Event;</span><br><span class="line">import org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line">import java.security.MessageDigest;</span><br><span class="line">import java.security.NoSuchAlgorithmException;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.regex.Matcher;</span><br><span class="line">import java.util.regex.Pattern;</span><br><span class="line"></span><br><span class="line">import static cn.baidu.interceptor.CustomParameterInterceptor.Constants.*;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Created by itcast</span><br><span class="line"> */</span><br><span class="line">public class CustomParameterInterceptor implements Interceptor&#123;</span><br><span class="line">    /** The field_separator.指明每一行字段的分隔符 */</span><br><span class="line">  private final String fields_separator;</span><br><span class="line"></span><br><span class="line">  /** The indexs.通过分隔符分割后，指明需要那列的字段 下标*/</span><br><span class="line">  private final String indexs;</span><br><span class="line"></span><br><span class="line">  /** The indexs_separator. 多个下标的分隔符*/</span><br><span class="line">  private final String indexs_separator;</span><br><span class="line"></span><br><span class="line">  /** The encrypted_field_index. 需要加密的字段下标*/</span><br><span class="line">  private final String encrypted_field_index;</span><br><span class="line">  /**</span><br><span class="line">   *</span><br><span class="line">   */</span><br><span class="line">  public CustomParameterInterceptor( String fields_separator,</span><br><span class="line">                                     String indexs, String indexs_separator,String encrypted_field_index) &#123;</span><br><span class="line">      String f = fields_separator.trim();</span><br><span class="line">      String i = indexs_separator.trim();</span><br><span class="line">      this.indexs = indexs;</span><br><span class="line">      this.encrypted_field_index=encrypted_field_index.trim();</span><br><span class="line">      if (!f.equals(&quot;&quot;)) &#123;</span><br><span class="line">          f = UnicodeToString(f);</span><br><span class="line">      &#125;</span><br><span class="line">      this.fields_separator =f;</span><br><span class="line">      if (!i.equals(&quot;&quot;)) &#123;</span><br><span class="line">          i = UnicodeToString(i);</span><br><span class="line">      &#125;</span><br><span class="line">      this.indexs_separator = i;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>\t 制表符 的UNicode编码格式为(‘\u0009’)</p>
<p>将配置文件中传过来的unicode编码进行转换为字符串。</p>
<p>写具体的要处理的逻辑intercept()方法，一个是单个处理的，一个是批量处理。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">public static String UnicodeToString(String str) &#123;</span><br><span class="line">     Pattern pattern = Pattern.compile(&quot;(\\\\u(\\p&#123;XDigit&#125;&#123;4&#125;))&quot;);</span><br><span class="line">     Matcher matcher = pattern.matcher(str);</span><br><span class="line">     char ch;</span><br><span class="line">     while (matcher.find()) &#123;</span><br><span class="line">         ch = (char) Integer.parseInt(matcher.group(2), 16);</span><br><span class="line">         str = str.replace(matcher.group(1), ch + &quot;&quot;);</span><br><span class="line">     &#125;</span><br><span class="line">     return str;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> /*</span><br><span class="line">  * @see org.apache.flume.interceptor.Interceptor#intercept(org.apache.flume.Event)</span><br><span class="line">  */</span><br><span class="line"> public Event intercept(Event event) &#123;</span><br><span class="line">     if (event == null) &#123;</span><br><span class="line">         return null;</span><br><span class="line">     &#125;</span><br><span class="line">     try &#123;</span><br><span class="line">         String line = new String(event.getBody(), Charsets.UTF_8);</span><br><span class="line">         String[] fields_spilts = line.split(fields_separator);</span><br><span class="line">         String[] indexs_split = indexs.split(indexs_separator);</span><br><span class="line">         String newLine=&quot;&quot;;</span><br><span class="line">         for (int i = 0; i &lt; indexs_split.length; i++) &#123;</span><br><span class="line">             int parseInt = Integer.parseInt(indexs_split[i]);</span><br><span class="line">             //对加密字段进行加密</span><br><span class="line">             if(!&quot;&quot;.equals(encrypted_field_index)&amp;&amp;encrypted_field_index.equals(indexs_split[i]))&#123;</span><br><span class="line">                 newLine+=StringUtils.GetMD5Code(fields_spilts[parseInt]);</span><br><span class="line">             &#125;else&#123;</span><br><span class="line">                 newLine+=fields_spilts[parseInt];</span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line">             if(i!=indexs_split.length-1)&#123;</span><br><span class="line">                 newLine+=fields_separator;</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         event.setBody(newLine.getBytes(Charsets.UTF_8));</span><br><span class="line">         return event;</span><br><span class="line">     &#125; catch (Exception e) &#123;</span><br><span class="line">         return event;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> /*</span><br><span class="line">  * @see org.apache.flume.interceptor.Interceptor#intercept(java.util.List)</span><br><span class="line">  */</span><br><span class="line"> public List&lt;Event&gt; intercept(List&lt;Event&gt; events) &#123;</span><br><span class="line">     List&lt;Event&gt; out = new ArrayList&lt;Event&gt;();</span><br><span class="line">     for (Event event : events) &#123;</span><br><span class="line">         Event outEvent = intercept(event);</span><br><span class="line">         if (outEvent != null) &#123;</span><br><span class="line">             out.add(outEvent);</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     return out;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> /*</span><br><span class="line">  * @see org.apache.flume.interceptor.Interceptor#initialize()</span><br><span class="line">  */</span><br><span class="line"> public void initialize() &#123;</span><br><span class="line">     // TODO Auto-generated method stub</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> /*</span><br><span class="line">  * @see org.apache.flume.interceptor.Interceptor#close()</span><br><span class="line">  */</span><br><span class="line"> public void close() &#123;</span><br><span class="line">     // TODO Auto-generated method stub</span><br><span class="line"></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>接口中定义了一个内部接口Builder，在configure方法中，进行一些参数配置。并给出，在flume的conf中没配置一些参数时，给出其默认值。通过其builder方法，返回一个CustomParameterInterceptor对象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">public static class Builder implements Interceptor.Builder &#123;</span><br><span class="line"></span><br><span class="line">      /** The fields_separator.指明每一行字段的分隔符 */</span><br><span class="line">      private  String fields_separator;</span><br><span class="line"></span><br><span class="line">      /** The indexs.通过分隔符分割后，指明需要那列的字段 下标*/</span><br><span class="line">      private  String indexs;</span><br><span class="line"></span><br><span class="line">      /** The indexs_separator. 多个下标下标的分隔符*/</span><br><span class="line">      private  String indexs_separator;</span><br><span class="line"></span><br><span class="line">      /** The encrypted_field. 需要加密的字段下标*/</span><br><span class="line">      private  String encrypted_field_index;</span><br><span class="line"></span><br><span class="line">      /*</span><br><span class="line">       * @see org.apache.flume.conf.Configurable#configure(org.apache.flume.Context)</span><br><span class="line">       */</span><br><span class="line">      public void configure(Context context) &#123;</span><br><span class="line">          fields_separator = context.getString(FIELD_SEPARATOR, DEFAULT_FIELD_SEPARATOR);</span><br><span class="line">          indexs = context.getString(INDEXS, DEFAULT_INDEXS);</span><br><span class="line">          indexs_separator = context.getString(INDEXS_SEPARATOR, DEFAULT_INDEXS_SEPARATOR);</span><br><span class="line">          encrypted_field_index= context.getString(ENCRYPTED_FIELD_INDEX, DEFAULT_ENCRYPTED_FIELD_INDEX);</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      /*</span><br><span class="line">       * @see org.apache.flume.interceptor.Interceptor.Builder#build()</span><br><span class="line">       */</span><br><span class="line">      public Interceptor build() &#123;</span><br><span class="line"></span><br><span class="line">          return new CustomParameterInterceptor(fields_separator, indexs, indexs_separator,encrypted_field_index);</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">/**</span><br><span class="line"> * The Class Constants.</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public static class Constants &#123;</span><br><span class="line">    /** The Constant FIELD_SEPARATOR. */</span><br><span class="line">    public static final String FIELD_SEPARATOR = &quot;fields_separator&quot;;</span><br><span class="line"></span><br><span class="line">    /** The Constant DEFAULT_FIELD_SEPARATOR. */</span><br><span class="line">    public static final String DEFAULT_FIELD_SEPARATOR =&quot; &quot;;</span><br><span class="line"></span><br><span class="line">    /** The Constant INDEXS. */</span><br><span class="line">    public static final String INDEXS = &quot;indexs&quot;;</span><br><span class="line"></span><br><span class="line">    /** The Constant DEFAULT_INDEXS. */</span><br><span class="line">    public static final String DEFAULT_INDEXS = &quot;0&quot;;</span><br><span class="line"></span><br><span class="line">    /** The Constant INDEXS_SEPARATOR. */</span><br><span class="line">    public static final String INDEXS_SEPARATOR = &quot;indexs_separator&quot;;</span><br><span class="line"></span><br><span class="line">    /** The Constant DEFAULT_INDEXS_SEPARATOR. */</span><br><span class="line">    public static final String DEFAULT_INDEXS_SEPARATOR = &quot;,&quot;;</span><br><span class="line"></span><br><span class="line">    /** The Constant ENCRYPTED_FIELD_INDEX. */</span><br><span class="line">    public static final String ENCRYPTED_FIELD_INDEX = &quot;encrypted_field_index&quot;;</span><br><span class="line"></span><br><span class="line">    /** The Constant DEFAUL_TENCRYPTED_FIELD_INDEX. */</span><br><span class="line">    public static final String DEFAULT_ENCRYPTED_FIELD_INDEX = &quot;&quot;;</span><br><span class="line"></span><br><span class="line">    /** The Constant PROCESSTIME. */</span><br><span class="line">    public static final String PROCESSTIME = &quot;processTime&quot;;</span><br><span class="line">    /** The Constant PROCESSTIME. */</span><br><span class="line">    public static final String DEFAULT_PROCESSTIME = &quot;a&quot;;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>定义一个静态类，类中封装MD5加密方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * 字符串md5加密</span><br><span class="line">  */</span><br><span class="line"> public static class StringUtils &#123;</span><br><span class="line">     // 全局数组</span><br><span class="line">     private final static String[] strDigits = &#123; &quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;,</span><br><span class="line">             &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot; &#125;;</span><br><span class="line"></span><br><span class="line">     // 返回形式为数字跟字符串</span><br><span class="line">     private static String byteToArrayString(byte bByte) &#123;</span><br><span class="line">         int iRet = bByte;</span><br><span class="line">         // System.out.println(&quot;iRet=&quot;+iRet);</span><br><span class="line">         if (iRet &lt; 0) &#123;</span><br><span class="line">             iRet += 256;</span><br><span class="line">         &#125;</span><br><span class="line">         int iD1 = iRet / 16;</span><br><span class="line">         int iD2 = iRet % 16;</span><br><span class="line">         return strDigits[iD1] + strDigits[iD2];</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     // 返回形式只为数字</span><br><span class="line">     private static String byteToNum(byte bByte) &#123;</span><br><span class="line">         int iRet = bByte;</span><br><span class="line">         System.out.println(&quot;iRet1=&quot; + iRet);</span><br><span class="line">         if (iRet &lt; 0) &#123;</span><br><span class="line">             iRet += 256;</span><br><span class="line">         &#125;</span><br><span class="line">         return String.valueOf(iRet);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     // 转换字节数组为16进制字串</span><br><span class="line">     private static String byteToString(byte[] bByte) &#123;</span><br><span class="line">         StringBuffer sBuffer = new StringBuffer();</span><br><span class="line">         for (int i = 0; i &lt; bByte.length; i++) &#123;</span><br><span class="line">             sBuffer.append(byteToArrayString(bByte[i]));</span><br><span class="line">         &#125;</span><br><span class="line">         return sBuffer.toString();</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     public static String GetMD5Code(String strObj) &#123;</span><br><span class="line">         String resultString = null;</span><br><span class="line">         try &#123;</span><br><span class="line">             resultString = new String(strObj);</span><br><span class="line">             MessageDigest md = MessageDigest.getInstance(&quot;MD5&quot;);</span><br><span class="line">             // md.digest() 该函数返回值为存放哈希值结果的byte数组</span><br><span class="line">             resultString = byteToString(md.digest(strObj.getBytes()));</span><br><span class="line">         &#125; catch (NoSuchAlgorithmException ex) &#123;</span><br><span class="line">             ex.printStackTrace();</span><br><span class="line">         &#125;</span><br><span class="line">         return resultString;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>通过以上步骤，自定义拦截器的代码开发已完成，然后打包成jar，放到Flume的根目录下的lib中</p>
<p>新增配置文件: vim spool-interceptor-hdfs.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">a1.channels = c1</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = s1</span><br><span class="line"></span><br><span class="line">#channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity=1000</span><br><span class="line">a1.channels.c1.transactionCapacity=200</span><br><span class="line"></span><br><span class="line">#source</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.spoolDir = /root/logs4/</span><br><span class="line">a1.sources.r1.batchSize= 50</span><br><span class="line">a1.sources.r1.inputCharset = UTF-8</span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors =i1 i2</span><br><span class="line">a1.sources.r1.interceptors.i1.type =cn.baidu.interceptor.CustomParameterInterceptor$Builder</span><br><span class="line">a1.sources.r1.interceptors.i1.fields_separator=\\u0009</span><br><span class="line">a1.sources.r1.interceptors.i1.indexs =0,1,3,5,6</span><br><span class="line">a1.sources.r1.interceptors.i1.indexs_separator =\\u002c</span><br><span class="line">a1.sources.r1.interceptors.i1.encrypted_field_index =0</span><br><span class="line"></span><br><span class="line">a1.sources.r1.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Builder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#sink</span><br><span class="line">a1.sinks.s1.channel = c1</span><br><span class="line">a1.sinks.s1.type = hdfs</span><br><span class="line">a1.sinks.s1.hdfs.path =hdfs://node01:9000/intercept/%Y%m%d</span><br><span class="line">a1.sinks.s1.hdfs.filePrefix = itcasr</span><br><span class="line">a1.sinks.s1.hdfs.fileSuffix = .dat</span><br><span class="line">a1.sinks.s1.hdfs.rollSize = 10485760</span><br><span class="line">a1.sinks.s1.hdfs.rollInterval =20</span><br><span class="line">a1.sinks.s1.hdfs.rollCount = 0</span><br><span class="line">a1.sinks.s1.hdfs.batchSize = 2</span><br><span class="line">a1.sinks.s1.hdfs.round = true</span><br><span class="line">a1.sinks.s1.hdfs.roundUnit = minute</span><br><span class="line">a1.sinks.s1.hdfs.threadsPoolSize = 25</span><br><span class="line">a1.sinks.s1.hdfs.useLocalTimeStamp = true</span><br><span class="line">a1.sinks.s1.hdfs.minBlockReplicas = 1</span><br><span class="line">a1.sinks.s1.hdfs.fileType =DataStream</span><br><span class="line">a1.sinks.s1.hdfs.writeFormat = Text</span><br><span class="line">a1.sinks.s1.hdfs.callTimeout = 60000</span><br><span class="line">a1.sinks.s1.hdfs.idleTimeout =60</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//启动</span><br><span class="line">bin/flume-ng agent -c conf -f conf/spool-interceptor-hdfs.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>#九 自定义组件</p>
<h2 id="1flume自定义source"><a href="#1flume自定义source" class="headerlink" title="1flume自定义source"></a>1flume自定义source</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明:"></a>说明:</h3><p> Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。</p>
<p>如：实时监控MySQL，从MySQL中获取数据传输到HDFS或者其他存储框架，所以此时需要我们自己实现<strong>MySQLSource</strong>。</p>
<p>官方也提供了自定义source的接口：</p>
<p>官网说明：<a href="https://flume.apache.org/FlumeDeveloperGuide.html#source" target="_blank" rel="noopener">https://flume.apache.org/FlumeDeveloperGuide.html#source</a></p>
<h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理:"></a>实现原理:</h3><p>根据官方说明自定义mysqlsource需要继承AbstractSource类并实现Configurable和PollableSource接口。</p>
<p>实现相应方法：</p>
<p>getBackOffSleepIncrement() //此处暂不用</p>
<p>getMaxBackOffSleepInterval() //此处暂不用</p>
<p>configure(Context context) //初始化context</p>
<p>process() //获取数据（从mysql获取数据，业务处理比较复杂，所以我们定义一个专门的类——QueryMysql来处理跟mysql的交互），封装成event并写入channel，这个方法被循环调用</p>
<p>stop() //关闭相关的资源</p>
<p>###实现:</p>
<p>创建数据库及表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE `mysqlsource`;</span><br><span class="line"></span><br><span class="line">USE `mysqlsource`;</span><br><span class="line"></span><br><span class="line">/*Table structure for table `flume_meta` */</span><br><span class="line">DROP TABLE</span><br><span class="line">IF EXISTS `flume_meta`;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `flume_meta` (</span><br><span class="line">	`source_tab` VARCHAR (255) NOT NULL,</span><br><span class="line">	`currentIndex` VARCHAR (255) NOT NULL,</span><br><span class="line">	PRIMARY KEY (`source_tab`)</span><br><span class="line">) ENGINE = INNODB DEFAULT CHARSET = utf8;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/*Table structure for table `student` */</span><br><span class="line">DROP TABLE</span><br><span class="line">IF EXISTS `student`;</span><br><span class="line"></span><br><span class="line">CREATE TABLE `student` (</span><br><span class="line">	`id` INT (11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">	`name` VARCHAR (255) NOT NULL,</span><br><span class="line">	PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE = INNODB AUTO_INCREMENT = 5 DEFAULT CHARSET = utf8;</span><br><span class="line"></span><br><span class="line">/*Data for the table `student` */</span><br><span class="line">INSERT INTO `student` (`id`, `name`)</span><br><span class="line">VALUES</span><br><span class="line">	(1, &apos;zhangsan&apos;),</span><br><span class="line">	(2, &apos;lisi&apos;),</span><br><span class="line">	(3, &apos;wangwu&apos;),</span><br><span class="line">	(4, &apos;zhaoliu&apos;);</span><br></pre></td></tr></table></figure>

<p>pom.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;</span><br><span class="line">         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;cn.baidu.cloud&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;example-flume&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.1&lt;/version&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.flume&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.8.0&lt;/version&gt;</span><br><span class="line">            &lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;5.1.38&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.6&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">         &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;lombok&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.16.22&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.0&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">                    &lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.1.1&lt;/version&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                        &lt;/goals&gt;</span><br><span class="line">                        &lt;configuration&gt;</span><br><span class="line">                            &lt;filters&gt;</span><br><span class="line">                                &lt;filter&gt;</span><br><span class="line">                                    &lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class="line">                                    &lt;excludes&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span><br><span class="line">                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span><br><span class="line">                                    &lt;/excludes&gt;</span><br><span class="line">                                &lt;/filter&gt;</span><br><span class="line">                            &lt;/filters&gt;</span><br><span class="line">                            &lt;transformers&gt;</span><br><span class="line">                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;</span><br><span class="line">                                    &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">                                &lt;/transformer&gt;</span><br><span class="line">                            &lt;/transformers&gt;</span><br><span class="line">                        &lt;/configuration&gt;</span><br><span class="line">                    &lt;/execution&gt;</span><br><span class="line">                &lt;/executions&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br><span class="line"></span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure>

<p>自定义QueryMySql工具类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br></pre></td><td class="code"><pre><span class="line">package cn.baidu.flumesource;</span><br><span class="line"></span><br><span class="line">import org.apache.flume.Context;</span><br><span class="line">import org.apache.flume.conf.ConfigurationException;</span><br><span class="line">import org.apache.http.ParseException;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line">import org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line">import java.sql.*;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Properties;</span><br><span class="line">@Data</span><br><span class="line">@Getter</span><br><span class="line">@Setter</span><br><span class="line">public class QueryMySql &#123;</span><br><span class="line">    private static final Logger LOG = LoggerFactory.getLogger(QueryMySql.class);</span><br><span class="line"></span><br><span class="line">    private int runQueryDelay, //两次查询的时间间隔</span><br><span class="line">            startFrom,            //开始id</span><br><span class="line">            currentIndex,	     //当前id</span><br><span class="line">            recordSixe = 0,      //每次查询返回结果的条数</span><br><span class="line">            maxRow;                //每次查询的最大条数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    private String table,       //要操作的表</span><br><span class="line">            columnsToSelect,     //用户传入的查询的列</span><br><span class="line">            customQuery,          //用户传入的查询语句</span><br><span class="line">            query,                 //构建的查询语句</span><br><span class="line">            defaultCharsetResultSet;//编码集</span><br><span class="line"></span><br><span class="line">    //上下文，用来获取配置文件</span><br><span class="line">    private Context context;</span><br><span class="line"></span><br><span class="line">    //为定义的变量赋值（默认值），可在flume任务的配置文件中修改</span><br><span class="line">    private static final int DEFAULT_QUERY_DELAY = 10000;</span><br><span class="line">    private static final int DEFAULT_START_VALUE = 0;</span><br><span class="line">    private static final int DEFAULT_MAX_ROWS = 2000;</span><br><span class="line">    private static final String DEFAULT_COLUMNS_SELECT = &quot;*&quot;;</span><br><span class="line">    private static final String DEFAULT_CHARSET_RESULTSET = &quot;UTF-8&quot;;</span><br><span class="line"></span><br><span class="line">    private static Connection conn = null;</span><br><span class="line">    private static PreparedStatement ps = null;</span><br><span class="line">    private static String connectionURL, connectionUserName, connectionPassword;</span><br><span class="line"></span><br><span class="line">    //加载静态资源</span><br><span class="line">    static &#123;</span><br><span class="line">        Properties p = new Properties();</span><br><span class="line">        try &#123;</span><br><span class="line">            p.load(QueryMySql.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;));</span><br><span class="line">            connectionURL = p.getProperty(&quot;dbUrl&quot;);</span><br><span class="line">            connectionUserName = p.getProperty(&quot;dbUser&quot;);</span><br><span class="line">            connectionPassword = p.getProperty(&quot;dbPassword&quot;);</span><br><span class="line">            Class.forName(p.getProperty(&quot;dbDriver&quot;));</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            LOG.error(e.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //获取JDBC连接</span><br><span class="line">    private static Connection InitConnection(String url, String user, String pw) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            Connection conn = DriverManager.getConnection(url, user, pw);</span><br><span class="line">            if (conn == null)</span><br><span class="line">                throw new SQLException();</span><br><span class="line">            return conn;</span><br><span class="line">        &#125; catch (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        return null;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //构造方法</span><br><span class="line">    QueryMySql(Context context) throws ParseException &#123;</span><br><span class="line">        //初始化上下文</span><br><span class="line">        this.context = context;</span><br><span class="line"></span><br><span class="line">        //有默认值参数：获取flume任务配置文件中的参数，读不到的采用默认值</span><br><span class="line">        this.columnsToSelect = context.getString(&quot;columns.to.select&quot;, DEFAULT_COLUMNS_SELECT);</span><br><span class="line">        this.runQueryDelay = context.getInteger(&quot;run.query.delay&quot;, DEFAULT_QUERY_DELAY);</span><br><span class="line">        this.startFrom = context.getInteger(&quot;start.from&quot;, DEFAULT_START_VALUE);</span><br><span class="line">        this.defaultCharsetResultSet = context.getString(&quot;default.charset.resultset&quot;, DEFAULT_CHARSET_RESULTSET);</span><br><span class="line"></span><br><span class="line">        //无默认值参数：获取flume任务配置文件中的参数</span><br><span class="line">        this.table = context.getString(&quot;table&quot;);</span><br><span class="line">        this.customQuery = context.getString(&quot;custom.query&quot;);</span><br><span class="line">        connectionURL = context.getString(&quot;connection.url&quot;);</span><br><span class="line">        connectionUserName = context.getString(&quot;connection.user&quot;);</span><br><span class="line">        connectionPassword = context.getString(&quot;connection.password&quot;);</span><br><span class="line">        conn = InitConnection(connectionURL, connectionUserName, connectionPassword);</span><br><span class="line"></span><br><span class="line">        //校验相应的配置信息，如果没有默认值的参数也没赋值，抛出异常</span><br><span class="line">        checkMandatoryProperties();</span><br><span class="line">        //获取当前的id</span><br><span class="line">        currentIndex = getStatusDBIndex(startFrom);</span><br><span class="line">        //构建查询语句</span><br><span class="line">        query = buildQuery();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //校验相应的配置信息（表，查询语句以及数据库连接的参数）</span><br><span class="line">    private void checkMandatoryProperties() &#123;</span><br><span class="line">        if (table == null) &#123;</span><br><span class="line">            throw new ConfigurationException(&quot;property table not set&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        if (connectionURL == null) &#123;</span><br><span class="line">            throw new ConfigurationException(&quot;connection.url property not set&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        if (connectionUserName == null) &#123;</span><br><span class="line">            throw new ConfigurationException(&quot;connection.user property not set&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        if (connectionPassword == null) &#123;</span><br><span class="line">            throw new ConfigurationException(&quot;connection.password property not set&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //构建sql语句</span><br><span class="line">    private String buildQuery() &#123;</span><br><span class="line">        String sql = &quot;&quot;;</span><br><span class="line">        //获取当前id</span><br><span class="line">        currentIndex = getStatusDBIndex(startFrom);</span><br><span class="line">        LOG.info(currentIndex + &quot;&quot;);</span><br><span class="line">        if (customQuery == null) &#123;</span><br><span class="line">            sql = &quot;SELECT &quot; + columnsToSelect + &quot; FROM &quot; + table;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            sql = customQuery;</span><br><span class="line">        &#125;</span><br><span class="line">        StringBuilder execSql = new StringBuilder(sql);</span><br><span class="line">        //以id作为offset</span><br><span class="line">        if (!sql.contains(&quot;where&quot;)) &#123;</span><br><span class="line">            execSql.append(&quot; where &quot;);</span><br><span class="line">            execSql.append(&quot;id&quot;).append(&quot;&gt;&quot;).append(currentIndex);</span><br><span class="line">            return execSql.toString();</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            int length = execSql.toString().length();</span><br><span class="line">            return execSql.toString().substring(0, length - String.valueOf(currentIndex).length()) + currentIndex;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //执行查询</span><br><span class="line">    List&lt;List&lt;Object&gt;&gt; executeQuery() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            //每次执行查询时都要重新生成sql，因为id不同</span><br><span class="line">            customQuery = buildQuery();</span><br><span class="line">            //存放结果的集合</span><br><span class="line">            List&lt;List&lt;Object&gt;&gt; results = new ArrayList&lt;&gt;();</span><br><span class="line">            if (ps == null) &#123;</span><br><span class="line">                //</span><br><span class="line">                ps = conn.prepareStatement(customQuery);</span><br><span class="line">            &#125;</span><br><span class="line">            ResultSet result = ps.executeQuery(customQuery);</span><br><span class="line">            while (result.next()) &#123;</span><br><span class="line">                //存放一条数据的集合（多个列）</span><br><span class="line">                List&lt;Object&gt; row = new ArrayList&lt;&gt;();</span><br><span class="line">                //将返回结果放入集合</span><br><span class="line">                for (int i = 1; i &lt;= result.getMetaData().getColumnCount(); i++) &#123;</span><br><span class="line">                    row.add(result.getObject(i));</span><br><span class="line">                &#125;</span><br><span class="line">                results.add(row);</span><br><span class="line">            &#125;</span><br><span class="line">            LOG.info(&quot;execSql:&quot; + customQuery + &quot;\nresultSize:&quot; + results.size());</span><br><span class="line">            return results;</span><br><span class="line">        &#125; catch (SQLException e) &#123;</span><br><span class="line">            LOG.error(e.toString());</span><br><span class="line">            // 重新连接</span><br><span class="line">            conn = InitConnection(connectionURL, connectionUserName, connectionPassword);</span><br><span class="line">        &#125;</span><br><span class="line">        return null;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //将结果集转化为字符串，每一条数据是一个list集合，将每一个小的list集合转化为字符串</span><br><span class="line">    List&lt;String&gt; getAllRows(List&lt;List&lt;Object&gt;&gt; queryResult) &#123;</span><br><span class="line">        List&lt;String&gt; allRows = new ArrayList&lt;&gt;();</span><br><span class="line">        if (queryResult == null || queryResult.isEmpty())</span><br><span class="line">            return allRows;</span><br><span class="line">        StringBuilder row = new StringBuilder();</span><br><span class="line">        for (List&lt;Object&gt; rawRow : queryResult) &#123;</span><br><span class="line">            Object value = null;</span><br><span class="line">            for (Object aRawRow : rawRow) &#123;</span><br><span class="line">                value = aRawRow;</span><br><span class="line">                if (value == null) &#123;</span><br><span class="line">                    row.append(&quot;,&quot;);</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    row.append(aRawRow.toString()).append(&quot;,&quot;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            allRows.add(row.toString());</span><br><span class="line">            row = new StringBuilder();</span><br><span class="line">        &#125;</span><br><span class="line">        return allRows;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //更新offset元数据状态，每次返回结果集后调用。必须记录每次查询的offset值，为程序中断续跑数据时使用，以id为offset</span><br><span class="line">    void updateOffset2DB(int size) &#123;</span><br><span class="line">        //以source_tab做为KEY，如果不存在则插入，存在则更新（每个源表对应一条记录）</span><br><span class="line">        String sql = &quot;insert into flume_meta(source_tab,currentIndex) VALUES(&apos;&quot;</span><br><span class="line">                + this.table</span><br><span class="line">                + &quot;&apos;,&apos;&quot; + (recordSixe += size)</span><br><span class="line">                + &quot;&apos;) on DUPLICATE key update source_tab=values(source_tab),currentIndex=values(currentIndex)&quot;;</span><br><span class="line">        LOG.info(&quot;updateStatus Sql:&quot; + sql);</span><br><span class="line">        execSql(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //执行sql语句</span><br><span class="line">    private void execSql(String sql) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            ps = conn.prepareStatement(sql);</span><br><span class="line">            LOG.info(&quot;exec::&quot; + sql);</span><br><span class="line">            ps.execute();</span><br><span class="line">        &#125; catch (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //获取当前id的offset</span><br><span class="line">    private Integer getStatusDBIndex(int startFrom) &#123;</span><br><span class="line">        //从flume_meta表中查询出当前的id是多少</span><br><span class="line">        String dbIndex = queryOne(&quot;select currentIndex from flume_meta where source_tab=&apos;&quot; + table + &quot;&apos;&quot;);</span><br><span class="line">        if (dbIndex != null) &#123;</span><br><span class="line">            return Integer.parseInt(dbIndex);</span><br><span class="line">        &#125;</span><br><span class="line">        //如果没有数据，则说明是第一次查询或者数据表中还没有存入数据，返回最初传入的值</span><br><span class="line">        return startFrom;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //查询一条数据的执行语句(当前id)</span><br><span class="line">    private String queryOne(String sql) &#123;</span><br><span class="line">        ResultSet result = null;</span><br><span class="line">        try &#123;</span><br><span class="line">            ps = conn.prepareStatement(sql);</span><br><span class="line">            result = ps.executeQuery();</span><br><span class="line">            while (result.next()) &#123;</span><br><span class="line">                return result.getString(1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        return null;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //关闭相关资源</span><br><span class="line">    void close() &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">            conn.close();</span><br><span class="line">        &#125; catch (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义MySqlSource主类</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">package cn.baidu.flumesource;</span><br><span class="line"></span><br><span class="line">import org.apache.flume.Context;</span><br><span class="line">import org.apache.flume.Event;</span><br><span class="line">import org.apache.flume.EventDeliveryException;</span><br><span class="line">import org.apache.flume.PollableSource;</span><br><span class="line">import org.apache.flume.conf.Configurable;</span><br><span class="line">import org.apache.flume.event.SimpleEvent;</span><br><span class="line">import org.apache.flume.source.AbstractSource;</span><br><span class="line">import org.slf4j.Logger;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">import static org.slf4j.LoggerFactory.*;</span><br><span class="line"></span><br><span class="line">public class MySqlSource extends AbstractSource implements Configurable, PollableSource &#123;</span><br><span class="line"></span><br><span class="line">    //打印日志</span><br><span class="line">    private static final Logger LOG = getLogger(MySqlSource.class);</span><br><span class="line">    //定义sqlHelper</span><br><span class="line">    private QueryMySql sqlSourceHelper;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public long getBackOffSleepIncrement() &#123;</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public long getMaxBackOffSleepInterval() &#123;</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void configure(Context context) &#123;</span><br><span class="line">        //初始化</span><br><span class="line">        sqlSourceHelper = new QueryMySql(context);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public PollableSource.Status process() throws EventDeliveryException &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            //查询数据表</span><br><span class="line">            List&lt;List&lt;Object&gt;&gt; result = sqlSourceHelper.executeQuery();</span><br><span class="line">            //存放event的集合</span><br><span class="line">            List&lt;Event&gt; events = new ArrayList&lt;&gt;();</span><br><span class="line">            //存放event头集合</span><br><span class="line">            HashMap&lt;String, String&gt; header = new HashMap&lt;&gt;();</span><br><span class="line">            //如果有返回数据，则将数据封装为event</span><br><span class="line">            if (!result.isEmpty()) &#123;</span><br><span class="line">                List&lt;String&gt; allRows = sqlSourceHelper.getAllRows(result);</span><br><span class="line">                Event event = null;</span><br><span class="line">                for (String row : allRows) &#123;</span><br><span class="line">                    event = new SimpleEvent();</span><br><span class="line">                    event.setBody(row.getBytes());</span><br><span class="line">                    event.setHeaders(header);</span><br><span class="line">                    events.add(event);</span><br><span class="line">                &#125;</span><br><span class="line">                //将event写入channel</span><br><span class="line">                this.getChannelProcessor().processEventBatch(events);</span><br><span class="line">                //更新数据表中的offset信息</span><br><span class="line">                sqlSourceHelper.updateOffset2DB(result.size());</span><br><span class="line">            &#125;</span><br><span class="line">            //等待时长</span><br><span class="line">            Thread.sleep(sqlSourceHelper.getRunQueryDelay());</span><br><span class="line">            return Status.READY;</span><br><span class="line">        &#125; catch (InterruptedException e) &#123;</span><br><span class="line">            LOG.error(&quot;Error procesing row&quot;, e);</span><br><span class="line">            return Status.BACKOFF;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public synchronized void stop() &#123;</span><br><span class="line">        LOG.info(&quot;Stopping sql source &#123;&#125; ...&quot;, getName());</span><br><span class="line">        try &#123;</span><br><span class="line">            //关闭资源</span><br><span class="line">            sqlSourceHelper.close();</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            super.stop();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用maven对工程进行打包，需要将mysql的依赖包一起打到jar包里，然后将打包好的jar包放到flume的lib目录下。</p>
<p>配置文件: vim mysqlsource.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = cn.baidu.flumesource.MySqlSource</span><br><span class="line">a1.sources.r1.connection.url = jdbc:mysql://node01:3306/mysqlsource</span><br><span class="line">a1.sources.r1.connection.user = root</span><br><span class="line">a1.sources.r1.connection.password = hadoop</span><br><span class="line">a1.sources.r1.table = student</span><br><span class="line">a1.sources.r1.columns.to.select = *</span><br><span class="line">a1.sources.r1.incremental.column.name = id</span><br><span class="line">a1.sources.r1.incremental.value = 0</span><br><span class="line">a1.sources.r1.run.query.delay=3000</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Describe the channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>启动:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -c conf -fconf/mysqlsource.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h2 id="2flume自定义sink"><a href="#2flume自定义sink" class="headerlink" title="2flume自定义sink"></a>2flume自定义sink</h2><p>###说明:</p>
<p>同自定义source类似，对于某些sink如果没有我们想要的，我们也可以自定义sink实现将数据保存到我们想要的地方去，例如kafka，或者mysql，或者文件等等都可以</p>
<p>需求：从网络端口当中发送数据，自定义sink，使用sink从网络端口接收数据，然后将数据保存到本地文件当中去。</p>
<p>pom.xml 同自定义source</p>
<p>自定义mysink</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">package cn.baidu.flumesink;</span><br><span class="line"></span><br><span class="line">import org.apache.commons.io.FileUtils;</span><br><span class="line">import org.apache.flume.*;</span><br><span class="line">import org.apache.flume.conf.Configurable;</span><br><span class="line">import org.apache.flume.sink.AbstractSink;</span><br><span class="line"></span><br><span class="line">import java.io.*;</span><br><span class="line"></span><br><span class="line">public class MySink extends AbstractSink implements Configurable &#123;</span><br><span class="line">    private Context context ;</span><br><span class="line">    private String filePath = &quot;&quot;;</span><br><span class="line">    private String fileName = &quot;&quot;;</span><br><span class="line">    private File fileDir;</span><br><span class="line"></span><br><span class="line">    //这个方法会在初始化调用，主要用于初始化我们的Context，获取我们的一些配置参数</span><br><span class="line">    @Override</span><br><span class="line">    public void configure(Context context) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            this.context = context;</span><br><span class="line">            filePath = context.getString(&quot;filePath&quot;);</span><br><span class="line">            fileName = context.getString(&quot;fileName&quot;);</span><br><span class="line">            fileDir = new File(filePath);</span><br><span class="line">            if(!fileDir.exists())&#123;</span><br><span class="line">                fileDir.mkdirs();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //这个方法会被反复调用</span><br><span class="line">    @Override</span><br><span class="line">    public Status process() throws EventDeliveryException &#123;</span><br><span class="line">        Event event = null;</span><br><span class="line">        Channel channel = this.getChannel();</span><br><span class="line">        Transaction transaction = channel.getTransaction();</span><br><span class="line">        transaction.begin();</span><br><span class="line">        while(true)&#123;</span><br><span class="line">            event = channel.take();</span><br><span class="line">            if(null != event)&#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        byte[] body = event.getBody();</span><br><span class="line">        String line = new String(body);</span><br><span class="line">        try &#123;</span><br><span class="line">            FileUtils.write(new File(filePath+File.separator+fileName),line,true);</span><br><span class="line">            transaction.commit();</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            transaction.rollback();</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            return Status.BACKOFF;</span><br><span class="line">        &#125;finally &#123;</span><br><span class="line">            transaction.close();</span><br><span class="line">        &#125;</span><br><span class="line">        return Status.READY;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>将代码使用打包插件，打成jar包，注意一定要将commons-langs这个依赖包打进去，放到flume的lib目录下</p>
<p>配置conf文件 vim filesink.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = node01</span><br><span class="line">a1.sources.r1.port = 5678</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"># # Describe the sink</span><br><span class="line">a1.sinks.k1.type = cn.baidu.flumesink.MySink</span><br><span class="line">a1.sinks.k1.filePath=/export/servers</span><br><span class="line">a1.sinks.k1.fileName=filesink.txt</span><br><span class="line"># # Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"># # Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>启动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -c conf -f conf/filesink.conf -n a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>Telnet node01 5678 连接到机器端口上输入数据</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Flume/" data-id="cjz2c0w9f000uagu5hvfc1wbk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Sqoop" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Sqoop/" class="article-date">
  <time datetime="2019-08-08T03:41:25.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Sqoop/">Sqoop</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h1><h1 id="一-简介"><a href="#一-简介" class="headerlink" title="一 简介"></a>一 简介</h1><p>Apache Sqoop是在Hadoop生态体系和RDBMS体系之间传送数据的一种工具。</p>
<p> Sqoop工作机制是将导入或导出命令翻译成mapreduce程序来实现。在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。</p>
<p>Hadoop生态系统包括：HDFS、Hive、Hbase等</p>
<p>RDBMS体系包括(关系型数据库)：Mysql、Oracle、DB2等</p>
<p>Sqoop也可以理解为：“SQL 到 Hadoop 和 Hadoop 到SQL”。</p>
<p>站在Apache的立场数据可以分为导入和导出:</p>
<p>Import：数据导入。RDBMS—–&gt;Hadoop</p>
<p>Export：数据导出。Hadoop—-&gt;RDBMS</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>前提: 安装过java和Hadoop</p>
<p>版本 1.4.6</p>
<p>解压及安装</p>
<p>配置sqoop中的conf中的</p>
<p>mv sqoop-env-template.sh sqoop-env.sh</p>
<p>vi sqoop-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME= /export/servers/hadoop-2.7.5 </span><br><span class="line">export HADOOP_MAPRED_HOME= /export/servers/hadoop-2.7.5</span><br><span class="line">export HIVE_HOME= /export/servers/hive</span><br><span class="line">##还可以配置hbase等</span><br></pre></td></tr></table></figure>

<p>把数据库的驱动加入 sqoop的lib中</p>
<p>测试:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop list-databases --connect jdbc:mysql://node03:3306/ --username root --password 123456</span><br></pre></td></tr></table></figure>

<p>本命令会列出所有(mysql)(orole)等的数据库。</p>
<h1 id="二-sqoop的导入"><a href="#二-sqoop的导入" class="headerlink" title="二 sqoop的导入"></a>二 sqoop的导入</h1><h2 id="1-从数据库导入hdfs"><a href="#1-从数据库导入hdfs" class="headerlink" title="1 从数据库导入hdfs"></a>1 从数据库导入hdfs</h2><ul>
<li>mysql的地址尽量不要使用localhost 请使用ip或者host</li>
<li>如果不指定 导入到hdfs默认分隔符是 “,”</li>
<li>可以通过– fields-terminated-by ‘\ t‘ 指定具体的分隔符</li>
<li>如果表的数据比较大 可以并行启动多个maptask执行导入操作，如果表没有主键，请指定根据哪个字段进行切分</li>
</ul>
<p>全量导入</p>
<p><strong>若是换行 每行结尾必须加 \ 否则报错</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /sqoopresult1 \   ##指定存在hdfs的目录</span><br><span class="line">--table emp --m 1   ###指定要导入的表  并指定要运行几个MapTask --m 1</span><br></pre></td></tr></table></figure>

<p>指定分隔符导入 sqoop的默认分隔符为 “,”</p>
<p>–fields-terminated-by ‘\t’ \ ##指定存在hdfs 上的文件的分隔符</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /sqoopresout2 \</span><br><span class="line">--fields-terminated-by &apos;\t&apos; \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></figure>

<p>指定分割字段和启动几个MapReduce</p>
<p>sqoop命令中，–split-by<br>id通常配合-m 10参数使用。用于指定根据哪个字段进行划分并启动多少个maptask。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /sqoopresult214 \</span><br><span class="line">--fields-terminated-by &apos;\t&apos; \</span><br><span class="line">--split-by id \                 ##指定分割字段</span><br><span class="line">--table emp --m 2               ##--m 2 启动两个maptask</span><br></pre></td></tr></table></figure>

<p>##2从数据库导入hive</p>
<p>全量导入</p>
<h3 id="1将表结构复制到hive中"><a href="#1将表结构复制到hive中" class="headerlink" title="1将表结构复制到hive中"></a>1将表结构复制到hive中</h3><p>hive 中的数据库为test 必须存在 emp_add_sp表 可以不存在</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop create-hive-table \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table emp_add \</span><br><span class="line">--hive-table test.emp_add_sp</span><br></pre></td></tr></table></figure>

<p>从关系数据库导入文件到hive中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table emp_add \</span><br><span class="line">--hive-table test.emp_add_sp \</span><br><span class="line">--hive-import \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure>

<h3 id="2直接从数据库导入数据到hive中-包括表结构和数据"><a href="#2直接从数据库导入数据到hive中-包括表结构和数据" class="headerlink" title="2直接从数据库导入数据到hive中 包括表结构和数据"></a>2直接从数据库导入数据到hive中 包括表结构和数据</h3><p>不用指定hive的表名,会根据数据库的表名自动创建 若test库不存在会在root下创建表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table emp_conn \</span><br><span class="line">--hive-import \</span><br><span class="line">--m 1 \</span><br><span class="line">--hive-database test</span><br></pre></td></tr></table></figure>

<p>##3导入表数据子集(导入hdfs)</p>
<h3 id="1where条件过滤导入"><a href="#1where条件过滤导入" class="headerlink" title="1where条件过滤导入"></a>1where条件过滤导入</h3><p>–where可以指定从关系数据库导入数据时的查询条件。它执行在数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--where &quot;city=&apos;sec-bad&apos;&quot; \</span><br><span class="line">--target-dir /wherequery \</span><br><span class="line">--table emp_add \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure>

<p>###2query查询过滤导入</p>
<p>使用 query sql 语句来进行查找不能加参数–table ;<br>并且必须要添加 where 条件;<br>并且 where 条件后面必须带一个$CONDITIONS 这个字符串;<br>并且这个 sql 语句必须用单引号，不能用双引号;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /query1 \</span><br><span class="line">--query &apos;select id,name,deg from emp where id&gt;1203 and $CONDITIONS&apos; \</span><br><span class="line">--split-by id \</span><br><span class="line">--fields-terminated-by &apos;\001&apos; \</span><br><span class="line">--m 2</span><br></pre></td></tr></table></figure>

<p>sqoop命令中–split-by id通常配合-m 10参数使用。<br>首先sqoop会向关系型数据库比如mysql发送一个命令:select max(id),min(id) from test。<br>然后会把max、min之间的区间平均分为10分，最后10个并行的map去找数据库，导数据就正式开始。</p>
<h2 id="4增量导入"><a href="#4增量导入" class="headerlink" title="4增量导入"></a>4增量导入</h2><h3 id="1-Append-模式"><a href="#1-Append-模式" class="headerlink" title="1 Append,模式"></a>1 Append,模式</h3><p>就是追加导入,在原有的基础上 <strong>根据数值类型字段进行追加导入 大于指定的last-value</strong></p>
<p>例子</p>
<p>先把数据库数据导入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /appendresult \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></figure>

<p>在数据库emp 中在插入两条数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">insert into `userdb`.`emp` (`id`, `name`, `deg`, `salary`, `dept`) values (&apos;1206&apos;, &apos;allen&apos;, &apos;admin&apos;, &apos;30000&apos;, &apos;tp&apos;);</span><br><span class="line">insert into `userdb`.`emp` (`id`, `name`, `deg`, `salary`, `dept`) values (&apos;1207&apos;, &apos;woon&apos;, &apos;admin&apos;, &apos;40000&apos;, &apos;tp&apos;);</span><br></pre></td></tr></table></figure>

<p>执行追加导入</p>
<p>–incremental append \ 增量导入的模式<br>–check-column id \ 数值类型字段进行追加导入<br>–last-value 1205 最后字段值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table emp --m 1 \</span><br><span class="line">--target-dir /appendresult \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">--last-value 1205</span><br></pre></td></tr></table></figure>

<h3 id="3-lastmodified模式"><a href="#3-lastmodified模式" class="headerlink" title="3 lastmodified模式"></a>3 lastmodified模式</h3><p>1append模式(附加)</p>
<p>lastmodified 根据时间戳类型字段进行追加 <strong>大于等于</strong>指定的last-value</p>
<ul>
<li>注意在lastmodified 模式下 还分为两种情形：append merge-key</li>
</ul>
<p>关于lastmodified 中的两种模式：</p>
<ul>
<li><p>append 只会追加增量数据到一个新的文件中 并且会产生数据的重复问题</p>
<p>因为默认是从指定的last-value 大于等于其值的数据开始导入</p>
</li>
<li><p>merge-key 把增量的数据合并到一个文件中 处理追加增量数据之外 如果之前的数据有变化修改</p>
<p>也可以进行修改操作 底层相当于进行了一次完整的mr作业。数据不会重复。</p>
</li>
</ul>
<p>数据库建表:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table customertest(id int,name varchar(20),last_mod timestamp default current_timestamp on update current_timestamp);</span><br><span class="line">此处的时间戳设置为在数据的产生和更新时都会发生改变.</span><br></pre></td></tr></table></figure>

<p>插入数据(一个一个插,可以保证last_mod 字段不一样)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">insert into customertest(id,name) values(1,&apos;neil&apos;);</span><br><span class="line">insert into customertest(id,name) values(2,&apos;jack&apos;);</span><br><span class="line">insert into customertest(id,name) values(3,&apos;martin&apos;);</span><br><span class="line">insert into customertest(id,name) values(4,&apos;tony&apos;);</span><br><span class="line">insert into customertest(id,name) values(5,&apos;eric&apos;);</span><br></pre></td></tr></table></figure>

<p>执行命令导入hdfs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /lastmodifiedresult \</span><br><span class="line">--table customertest --m 1</span><br></pre></td></tr></table></figure>

<p>在mysql中在插入一条数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into customertest(id,name) values(6,&apos;james&apos;)</span><br></pre></td></tr></table></figure>

<p>使用增量导入:</p>
<p>三兄弟:两种导入都要写</p>
<p>–check-column last_mod<br>–incremental lastmodified<br>–last-value “2019-06-05 15:52:58” \</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table customertest \</span><br><span class="line">--target-dir /lastmodifiedresult \</span><br><span class="line">--check-column last_mod \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value &quot;2019-06-05 15:52:58&quot; \</span><br><span class="line">--m 1 \</span><br><span class="line">--append    #追加方式  merge-key 和append</span><br></pre></td></tr></table></figure>

<p>查看结果发现:</p>
<p>此处已经会导入我们最后插入的一条记录,但是我们却发现此处插入了2条数据，这是为什么呢？<br>这是因为采用lastmodified模式去处理增量时，会将大于等于last-value值的数据当做增量插入</p>
<p>注意:<br>*<em>使用lastmodified模式进行增量处理要指定增量数据是以append模式(附加)还是merge-key(合并)模式添加 *</em></p>
<p>2merge-key模式(合并)</p>
<p>数据库操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update customertest set name = &apos;Neil&apos; where id = 1;</span><br></pre></td></tr></table></figure>

<p>增量导入:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table customertest \</span><br><span class="line">--target-dir /lastmodifiedresult \</span><br><span class="line">--check-column last_mod \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value &quot;2019-06-05 15:52:58&quot; \</span><br><span class="line">--m 1 \</span><br><span class="line">--merge-key id</span><br></pre></td></tr></table></figure>

<p>由于merge-key模式是进行了一次完整的mapreduce操作，因此最终我们在lastmodifiedresult文件夹下可以看到生成的为part-r-00000这样的文件，会发现id=1的name已经得到修改，同时新增了id=6的数据。</p>
<h1 id="三sqoop导出"><a href="#三sqoop导出" class="headerlink" title="三sqoop导出"></a>三sqoop导出</h1><p>将数据从Hadoop生态体系导出到RDBMS数据库导出前，目标表必须存在于目标数据库中。</p>
<p>export有三种模式：</p>
<p>默认操作: 是从将文件中的数据使用INSERT语句插入到表中。若为空表底层为insert一条一条的插入</p>
<p>更新模式：Sqoop将生成UPDATE替换数据库中现有记录的语句。底层为updata</p>
<p>调用模式：Sqoop将为每条记录创建一个存储过程调用。</p>
<p>配置参数:</p>
<ul>
<li>导出文件的分隔符 如果不指定 默认以“,”去切割读取数据文件 –input-fields-terminated-by</li>
<li>如果文件的字段顺序和表中顺序不一致 需要–columns 指定 多个字段之间以”,”</li>
<li>导出的时候需要指定导出数据的目的 export-dir 和导出到目标的表名或者存储过程名</li>
<li>针对空字符串类型和非字符串类型的转换 “\n”</li>
</ul>
<h2 id="1-默认模式导出HDFS数据到mysql"><a href="#1-默认模式导出HDFS数据到mysql" class="headerlink" title="1 默认模式导出HDFS数据到mysql"></a>1 默认模式导出HDFS数据到mysql</h2><p>默认情况下，sqoop export将每行输入记录转换成一条INSERT语句，添加到目标数据库表中。如果数据库中的表具有约束条件（例如，其值必须唯一的主键列）并且已有数据存在，则必须注意避免插入违反这些约束条件的记录。如果INSERT语句失败，导出过程将失败。<strong>此模式主要用于将记录导出到可以接收这些结果的空表中</strong>。通常用于全表数据导出。</p>
<p>导出时可以是将Hive表中的全部记录或者HDFS数据（可以是全部字段也可以部分字段）导出到Mysql目标表。</p>
<p>1准备hdfs数据</p>
<p>在HDFS文件系统中“/emp/”目录的下创建一个文件emp_data.txt：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1201,gopal,manager,50000,TP</span><br><span class="line">1202,manisha,preader,50000,TP</span><br><span class="line">1203,kalil,php dev,30000,AC</span><br><span class="line">1204,prasanth,php dev,30000,AC</span><br><span class="line">1205,kranthi,admin,20000,TP</span><br><span class="line">1206,satishp,grpdes,20000,GR</span><br></pre></td></tr></table></figure>

<p>2手动创建数据库中的表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; USE userdb;</span><br><span class="line">mysql&gt; CREATE TABLE employee ( </span><br><span class="line">   id INT NOT NULL PRIMARY KEY, </span><br><span class="line">   name VARCHAR(20), </span><br><span class="line">   deg VARCHAR(20),</span><br><span class="line">   salary INT,</span><br><span class="line">   dept VARCHAR(10));</span><br></pre></td></tr></table></figure>

<p>3执行导出命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table employee \</span><br><span class="line">--export-dir /emp/</span><br></pre></td></tr></table></figure>

<p>若是数据库中字段与emp_data.txt文件中字段类型一致,上述做法可以 若不一致</p>
<p>当导出数据文件和目标表字段列顺序完全一致的时候上述做法可以 若不一致。以逗号为间隔选择和排列各个列。加一下配置</p>
<p>–columns id,name,deg,salary,dept \ 指定emp_data.txt 中个字段的名字</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table employee1 \</span><br><span class="line">--columns id,name,deg,salary,dept \</span><br><span class="line">--export-dir /emp/</span><br></pre></td></tr></table></figure>

<p>–input-fields-terminated-by ‘\t’</p>
<p>指定文件中的分隔符</p>
<p>–columns</p>
<p>选择列并控制它们的排序。当导出数据文件和目标表字段列顺序完全一致的时候可以不写。否则以逗号为间隔选择和排列各个列。没有被包含在–columns后面列名或字段要么具备默认值，要么就允许插入空值。否则数据库会拒绝接受sqoop导出的数据，导致Sqoop作业失败</p>
<p>–export-dir 导出目录，在执行导出的时候，必须指定这个参数，同时需要具备–table或–call参数两者之一，–table是指的导出数据库当中对应的表，</p>
<p>–call是指的某个存储过程。</p>
<p>–input-null-string –input-null-non-string</p>
<p>如果没有指定第一个参数，对于字符串类型的列来说，“NULL”这个字符串就回被翻译成空值，如果没有使用第二个参数，无论是“NULL”字符串还是说空字符串也好，对于非字符串类型的字段来说，这两个类型的空串都会被翻译成空值。比如：</p>
<p>–input-null-string “\N” –input-null-non-string “\N”</p>
<h2 id="2更新导出（updateonly模式）"><a href="#2更新导出（updateonly模式）" class="headerlink" title="2更新导出（updateonly模式）"></a>2更新导出（updateonly模式）</h2><p>– update-key，更新标识，即根据某个字段进行更新，例如id，可以指定多个更新标识的字段，多个字段之间用逗号分隔。</p>
<p>– updatemod，指定updateonly（默认模式），<strong>仅仅更新已存在的数据记录，不会插入新纪录。</strong></p>
<p>在HDFS文件系统中“/updateonly_1/”目录的下创建一个文件updateonly_1.txt：<br>1201,gopal,manager,50000<br>1202,manisha,preader,50000<br>1203,kalil,php dev,30000</p>
<p>手动创建mysql中的目标表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; USE userdb;</span><br><span class="line">mysql&gt; CREATE TABLE updateonly ( </span><br><span class="line">   id INT NOT NULL PRIMARY KEY, </span><br><span class="line">   name VARCHAR(20), </span><br><span class="line">   deg VARCHAR(20),</span><br><span class="line">   salary INT);</span><br></pre></td></tr></table></figure>

<p>先执行全部导出操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table updateonly \</span><br><span class="line">--export-dir /updateonly_1/</span><br></pre></td></tr></table></figure>

<p>新增一个文件updateonly_2.txt：修改了前三条数据并且新增了一条记录<br>1201,gopal,manager,1212<br>1202,manisha,preader,1313<br>1203,kalil,php dev,1414<br>1204,allen,java,1515</p>
<p>hadoop fs -put updateonly_2.txt /updateonly_2</p>
<p>执行更新导出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table updateonly \</span><br><span class="line">--export-dir /updateonly_2/ \</span><br><span class="line">--update-key id \</span><br><span class="line">--update-mode updateonly</span><br></pre></td></tr></table></figure>

<h2 id="3-更新导出（allowinsert模式）"><a href="#3-更新导出（allowinsert模式）" class="headerlink" title="3 更新导出（allowinsert模式）"></a>3 更新导出（allowinsert模式）</h2><p>– update-key，更新标识，即根据某个字段进行更新，例如id，可以指定多个更新标识的字段，多个字段之间用逗号分隔。</p>
<p>– updatemod，指定allowinsert，更新已存在的数据记录，同时插入新纪录。<strong>实质上是一个insert &amp; update的操作。</strong></p>
<p>在HDFS “/allowinsert_1/”目录的下创建一个文件allowinsert_1.txt：<br>1201,gopal,manager,50000<br>1202,manisha,preader,50000<br>1203,kalil,php dev,30000</p>
<p>手动创建mysql中的目标表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; USE userdb;</span><br><span class="line">mysql&gt; CREATE TABLE allowinsert ( </span><br><span class="line">   id INT NOT NULL PRIMARY KEY, </span><br><span class="line">   name VARCHAR(20), </span><br><span class="line">   deg VARCHAR(20),</span><br><span class="line">   salary INT);</span><br></pre></td></tr></table></figure>

<p>先执行全部导出操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table allowinsert \</span><br><span class="line">--export-dir /allowinsert_1/</span><br></pre></td></tr></table></figure>

<p>allowinsert_2.txt。修改了前三条数据并且新增了一条记录。上传至/ allowinsert_2/目录下：<br>1201,gopal,manager,1212<br>1202,manisha,preader,1313<br>1203,kalil,php dev,1414<br>1204,allen,java,1515</p>
<p>执行更新导出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root --password 123456 \</span><br><span class="line">--table allowinsert \</span><br><span class="line">--export-dir /allowinsert_2/ \</span><br><span class="line">--update-key id \</span><br><span class="line">--update-mode allowinsert</span><br></pre></td></tr></table></figure>

<h1 id="四sqoop-job-作业"><a href="#四sqoop-job-作业" class="headerlink" title="四sqoop job 作业"></a>四sqoop job 作业</h1><h2 id="1-job-语法"><a href="#1-job-语法" class="headerlink" title="1 job 语法"></a>1 job 语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sqoop job (generic-args) (job-args)</span><br><span class="line">   [-- [subtool-name] (subtool-args)]</span><br><span class="line"></span><br><span class="line">$ sqoop-job (generic-args) (job-args)</span><br><span class="line">   [-- [subtool-name] (subtool-args)]</span><br></pre></td></tr></table></figure>

<h2 id="2-创建job"><a href="#2-创建job" class="headerlink" title="2 创建job"></a>2 创建job</h2><p>在这里，我们创建一个名为jobtest，这可以从RDBMS表的数据导入到HDFS作业。</p>
<p>下面的命令用于创建一个从DB数据库的emp表导入到HDFS文件的作业。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop job --create jobtest -- import --connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /sqoopresult333 \</span><br><span class="line">--table emp --m 1</span><br><span class="line"></span><br><span class="line">注意import前要有空格</span><br></pre></td></tr></table></figure>

<h2 id="3-验证job"><a href="#3-验证job" class="headerlink" title="3 验证job"></a>3 验证job</h2><p><strong>–list’</strong> 参数是用来验证保存的作业。下面的命令用来验证保存Sqoop作业的列表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop job --list</span><br></pre></td></tr></table></figure>

<h2 id="4检查job"><a href="#4检查job" class="headerlink" title="4检查job"></a>4检查job</h2><p><strong>‘–show’</strong> 参数用于检查或验证特定的工作，及其详细信息。以下命令和样本输出用来验证一个名为jobtest的作业。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop job --show jobtest</span><br></pre></td></tr></table></figure>

<h2 id="5-执行job"><a href="#5-执行job" class="headerlink" title="5 执行job"></a>5 执行job</h2><p><strong>‘–exec’</strong> 选项用于执行保存的作业。下面的命令用于执行保存的作业称为jobtest。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop job --exec jobtest</span><br></pre></td></tr></table></figure>

<h2 id="6免密执行job"><a href="#6免密执行job" class="headerlink" title="6免密执行job"></a>6免密执行job</h2><p>sqoop在创建job时，使用–password-file参数，可以避免输入mysql密码，如果使用–password将出现警告，并且每次都要手动输入密码才能执行job，sqoop规定密码文件必须存放在HDFS上，并且权限必须是400。</p>
<p>并且检查sqoop的sqoop-site.xml是否存在如下配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt;</span><br><span class="line"></span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"></span><br><span class="line">    &lt;description&gt;If true, allow saved passwords in the metastore.</span><br><span class="line"></span><br><span class="line">    &lt;/description&gt;</span><br><span class="line"></span><br><span class="line">&lt;/property&gt;</span><br><span class="line">bin/sqoop job --create jobtest -- import --connect jdbc:mysql://node03:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password-file /input/sqoop/pwd/mysqltest.pwd \</span><br><span class="line">--target-dir /sqoopresult333 \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Sqoop/" data-id="cjz2c0w8u000iagu5cgvlngue" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Azkaban" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Azkaban/" class="article-date">
  <time datetime="2019-08-08T03:41:14.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Azkaban/">Azkaban</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="一-概述"><a href="#一-概述" class="headerlink" title="一 概述"></a>一 概述</h2><p>是由领英推出的一款开源免费的工作流调度器软件.</p>
<p>特点:</p>
<ul>
<li>功能强大 可以调度几乎所有软件的执行（command）</li>
<li>配置简单 job配置文件</li>
<li>提供了web页面使用</li>
<li>提供模块化和可插拔的插件机制，原生支持command、Java、Hive、Pig、Hadoop</li>
<li>java语言开发 源码清晰可见 可以进行二次开发</li>
</ul>
<p>工作流概述:</p>
<p> 工作流（Workflow），指“业务过程的部分或整体在计算机应用环境下的<strong>自动化</strong>”。</p>
<p> 一个完整的数据分析系统通常都是由多个前后依赖的模块组合构成的：数据采集、数据预处理、数据分析、数据展示等。各个模块单元之间存在时间先后依赖关系，且存在着周期性重复。为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行。</p>
<p>实现方式:</p>
<p> 简单的任务调度：直接使用linux的crontab来定义,但是缺点也是比较明显，无法设置依赖。</p>
<p> 复杂的任务调度：自主开发调度平台，使用开源调度系统，比如<strong>azkaban</strong>、Apache Oozie、Cascading、Hamake等。其中知名度比较高的是Apache Oozie，但是其配置工作流的过程是编写大量的XML配置，而且代码复杂度比较高，不易于二次开发。</p>
<p>工作流之间的对比:</p>
<table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left">Hamake</th>
<th align="left">Oozie</th>
<th align="left">Azkaban</th>
<th align="left">Cascading</th>
</tr>
</thead>
<tbody><tr>
<td align="left">工作流描述语言</td>
<td align="left">XML</td>
<td align="left">XML (xPDL based)</td>
<td align="left">text file with key/value pairs</td>
<td align="left">Java API</td>
</tr>
<tr>
<td align="left">依赖机制</td>
<td align="left">data-driven</td>
<td align="left">explicit</td>
<td align="left">explicit</td>
<td align="left">explicit</td>
</tr>
<tr>
<td align="left">是否要web容器</td>
<td align="left">No</td>
<td align="left">Yes</td>
<td align="left">Yes</td>
<td align="left">No</td>
</tr>
<tr>
<td align="left">进度跟踪</td>
<td align="left">console/log messages</td>
<td align="left">web page</td>
<td align="left">web page</td>
<td align="left">Java API</td>
</tr>
<tr>
<td align="left">Hadoop job调度支持</td>
<td align="left">no</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">运行模式</td>
<td align="left">command line utility</td>
<td align="left">daemon</td>
<td align="left">daemon</td>
<td align="left">API</td>
</tr>
<tr>
<td align="left">Pig支持</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">事件通知</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">需要安装</td>
<td align="left">no</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">no</td>
</tr>
<tr>
<td align="left">支持的hadoop版本</td>
<td align="left">0.18+</td>
<td align="left">0.20+</td>
<td align="left">currently unknown</td>
<td align="left">0.18+</td>
</tr>
<tr>
<td align="left">重试支持</td>
<td align="left">no</td>
<td align="left">workflownode evel</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">运行任意命令</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">Amazon EMR支持</td>
<td align="left">yes</td>
<td align="left">no</td>
<td align="left">currently unknown</td>
<td align="left">yes</td>
</tr>
</tbody></table>
<p>##二 Azkaban调度器</p>
<p>###1原理架构:</p>
<p>mysql服务器: 存储元数据，如项目名称、项目描述、项目权限、任务状态、SLA规则等</p>
<p>AzkabanWebServer: 对外提供web服务，使用户可以通过web页面管理。职责包括项目管理、权限授权、任务调度、监控executor。</p>
<p>AzkabanExecutorServer: 负责具体的工作流的提交、执行。</p>
<h3 id="2部署方式-3种"><a href="#2部署方式-3种" class="headerlink" title="2部署方式(3种)"></a>2部署方式(3种)</h3><ul>
<li>单节点模式：web、executor在同一个进程 适用于测试体验 使用自带的H2数据库</li>
<li>two-server: web、executor在不同的进程中 <strong>可以使用第三方数据库</strong></li>
<li>mutil-executor-server: web、executor在不同的机器上 <strong>可以部署多个executor服务器</strong> 可使用第三方数据库</li>
</ul>
<h3 id="3源码编译"><a href="#3源码编译" class="headerlink" title="3源码编译"></a>3源码编译</h3><p>1.Azkaban3.x在安装前需要自己编译成二进制包。并且提前安装好Maven,Ant,Node等软件</p>
<p>2 编译环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install –y git</span><br><span class="line"></span><br><span class="line">yum install –y gcc-c++</span><br></pre></td></tr></table></figure>

<p>3 下载源码解析</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/azkaban/azkaban/archive/3.51.0.tar.gz</span><br><span class="line">tar -zxvf 3.51.0.tar.gz </span><br><span class="line">cd ./azkaban-3.51.0/</span><br></pre></td></tr></table></figure>

<p>4 编译源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./gradlew build installDist -x test</span><br></pre></td></tr></table></figure>

<p>Gradle是一个基于ApacheAnt和ApacheMaven的项目自动化构建工具。-x test 跳过测试。（注意联网下载jar可能会失败、慢）</p>
<p>5 编译后安装包路径</p>
<p>solo-server模式安装包路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">azkaban-solo-server/build/distributions/</span><br></pre></td></tr></table></figure>

<p>two-server模式和multiple-executor模式web-server安装包路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">azkaban-web-server/build/distributions/</span><br></pre></td></tr></table></figure>

<p>two-server模式和multiple-executor模式exec-server安装包路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">azkaban-exec-server/build/distributions/</span><br></pre></td></tr></table></figure>

<p>数据库相关安装包路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">azkaban-db/build/distributions/</span><br></pre></td></tr></table></figure>

<p>##三 安装部署</p>
<h3 id="1单节点模式"><a href="#1单节点模式" class="headerlink" title="1单节点模式"></a>1单节点模式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir /export/servers/azkaban</span><br><span class="line">tar -zxvf azkaban-solo-server-0.1.0-SNAPSHOT.tar.gz –C /export/servers/azkaban/</span><br><span class="line"></span><br><span class="line">vim conf/azkaban.properties</span><br><span class="line">default.timezone.id=Asia/Shanghai #修改时区</span><br><span class="line"></span><br><span class="line">vim  plugins/jobtypes/commonprivate.properties</span><br><span class="line">添加：memCheck.enabled=false</span><br><span class="line">azkaban默认需要3G的内存，剩余内存不足则会报异常</span><br></pre></td></tr></table></figure>

<p>启动验证:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd azkaban-solo-server-0.1.0-SNAPSHOT/</span><br><span class="line">bin/start-solo.sh</span><br><span class="line">注:启动/关闭必须进到azkaban-solo-server-0.1.0-SNAPSHOT/目录下。</span><br></pre></td></tr></table></figure>

<p>AzkabanSingleServer(对于Azkaban solo‐server模式，Exec Server和Web Server在同一个进程中)</p>
<p>登录页面:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://node01:8081/</span><br><span class="line">默认密码和用户名为  azkaban</span><br></pre></td></tr></table></figure>

<p>测试:</p>
<p>创建两个文件one.job ,two.job,内容如下，打包成zip包。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">##one.job</span><br><span class="line">cat one.job </span><br><span class="line">    type=command                        ##命令类型</span><br><span class="line">    command=echo &quot;this is job one&quot;</span><br><span class="line"></span><br><span class="line">##two.job </span><br><span class="line">cat two.job </span><br><span class="line">    type=command</span><br><span class="line">    dependencies=one                     ## 依赖于one.job one.job 执行完后 在执行自己的</span><br><span class="line">    command=echo &quot;this is job two&quot;</span><br></pre></td></tr></table></figure>

<p>Create Project=&gt;Upload zip包=&gt;execute flow执行一步步操作即可。</p>
<ul>
<li>上传zip压缩包</li>
<li>选择调度schduler或者立即执行executor工程。</li>
</ul>
<h3 id="2-two-server-模式"><a href="#2-two-server-模式" class="headerlink" title="2 two server 模式"></a>2 two server 模式</h3><p>节点规划:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node03        mysql</span><br><span class="line">node02        web-server和exec-server不同进程</span><br></pre></td></tr></table></figure>

<p>node03 上mysql配置初始化:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf azkaban-db-0.1.0-SNAPSHOT.tar.gz –C /export/servers/azkaban/</span><br><span class="line"></span><br><span class="line">Mysql上创建对应的库、增加权限、创建表</span><br><span class="line">mysql&gt; CREATE DATABASE azkaban_two_server; #创建数据库</span><br><span class="line">mysql&gt; use azkaban_two_server;</span><br><span class="line">mysql&gt; source /export/servers/azkaban/azkaban-db-0.1.0-SNAPSHOT/create-all-sql-0.1.0-SNAPSHOT.sql;</span><br><span class="line">#加载初始化sql创建表</span><br></pre></td></tr></table></figure>

<p>node02上 配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf azkaban-web-server-0.1.0-SNAPSHOT.tar.gz –C /export/servers/azkaban/</span><br><span class="line">tar -zxvf azkaban-exec-server-0.1.0-SNAPSHOT.tar.gz –C /export/servers/azkaban/</span><br></pre></td></tr></table></figure>

<p>###2.1web配置:</p>
<p>生成ssl证书：</p>
<p><strong>服务器证书,将ssl证书安装在网站服务器上可以实现网站身份和数据加密传输双重功能</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keytool -keystore keystore -alias jetty -genkey -keyalg RSA</span><br></pre></td></tr></table></figure>

<p>运行此命令后,会提示输入当前生成keystore的密码及相应信息,输入的密码请记住(<strong>所有密码统一以123456输入</strong>)。</p>
<p>完成上述工作后,将在当前目录生成keystore证书文件,将keystore拷贝到 azkaban web服务器根目录中。</p>
<p>配置 web服务器的 conf/azkaban.properties：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"># Azkaban Personalization Settings</span><br><span class="line">azkaban.name=Test</span><br><span class="line">azkaban.label=My Local Azkaban</span><br><span class="line">azkaban.color=#FF3601</span><br><span class="line">azkaban.default.servlet.path=/index</span><br><span class="line">web.resource.dir=web/</span><br><span class="line"># 1要修改的地方</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line"># Azkaban UserManager class</span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager</span><br><span class="line">user.manager.xml.file=conf/azkaban-users.xml</span><br><span class="line"># Loader for projects</span><br><span class="line">executor.global.properties=conf/global.properties</span><br><span class="line">azkaban.project.dir=projects</span><br><span class="line"># Velocity dev mode</span><br><span class="line">velocity.dev.mode=false</span><br><span class="line"># Azkaban Jetty server properties.</span><br><span class="line"># 2要修改的地方</span><br><span class="line">jetty.use.ssl=true</span><br><span class="line">jetty.ssl.port=8443</span><br><span class="line">jetty.maxThreads=25</span><br><span class="line">jetty.port=8081</span><br><span class="line"># Azkaban Executor settings</span><br><span class="line">#3要修改的地方</span><br><span class="line">executor.host=localhost</span><br><span class="line">executor.port=12321</span><br><span class="line"></span><br><span class="line">#  KeyStore for SSL ssl相关配置  注意密码和证书路径</span><br><span class="line"># 4要修改的地方</span><br><span class="line">jetty.keystore=keystore</span><br><span class="line">jetty.password=123456</span><br><span class="line">jetty.keypassword=123456</span><br><span class="line">jetty.truststore=keystore</span><br><span class="line">jetty.trustpassword=123456</span><br><span class="line"># mail settings</span><br><span class="line">mail.sender=</span><br><span class="line">mail.host=</span><br><span class="line"># User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.</span><br><span class="line"># enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081</span><br><span class="line"># when this parameters set then these parameters are used to generate email links.</span><br><span class="line"># if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.</span><br><span class="line"># azkaban.webserver.external_hostname=myazkabanhost.com</span><br><span class="line"># azkaban.webserver.external_ssl_port=443</span><br><span class="line"># azkaban.webserver.external_port=8081</span><br><span class="line">job.failure.email=</span><br><span class="line">job.success.email=</span><br><span class="line">lockdown.create.projects=false</span><br><span class="line">cache.directory=cache</span><br><span class="line"># JMX stats</span><br><span class="line">jetty.connector.stats=true</span><br><span class="line">executor.connector.stats=true</span><br><span class="line"># Azkaban mysql settings by default. Users should configure their own username and password.</span><br><span class="line">#5要修改的地方</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=node03</span><br><span class="line">mysql.database=azkaban_two_server</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123456</span><br><span class="line">mysql.numconnections=100</span><br><span class="line">#Multiple Executor</span><br><span class="line">azkaban.use.multiple.executors=true</span><br><span class="line"># 6修改的地方 注释掉这一行 放弃检查内存</span><br><span class="line">#azkaban.executorselector.filters=StaticRemainingFlowSize,MinimumFreeMemory,CpuStatus</span><br><span class="line">azkaban.executorselector.comparator.NumberOfAssignedFlowComparator=1</span><br><span class="line">azkaban.executorselector.comparator.Memory=1</span><br><span class="line">azkaban.executorselector.comparator.LastDispatched=1</span><br><span class="line">azkaban.executorselector.comparator.CpuUsage=1</span><br></pre></td></tr></table></figure>

<p>在web 的根目录下创建 目录:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p plugins/jobtypes</span><br></pre></td></tr></table></figure>

<p>在这个目录下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim commonprivate.properties</span><br><span class="line">#本地库</span><br><span class="line">azkaban.native.lib=false</span><br><span class="line"></span><br><span class="line">execute.as.user=false</span><br><span class="line">#内存检测</span><br><span class="line">memCheck.enabled=false</span><br></pre></td></tr></table></figure>

<h3 id="2-2-exec配置"><a href="#2-2-exec配置" class="headerlink" title="2.2 exec配置"></a>2.2 exec配置</h3><p>exec服务器下:conf/azkaban.properties：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"># Azkaban Personalization Settings</span><br><span class="line">azkaban.name=Test</span><br><span class="line">azkaban.label=My Local Azkaban</span><br><span class="line">azkaban.color=#FF3601</span><br><span class="line">azkaban.default.servlet.path=/index</span><br><span class="line">web.resource.dir=web/</span><br><span class="line"># 1要修改</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line"># Azkaban UserManager class</span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager</span><br><span class="line">user.manager.xml.file=conf/azkaban-users.xml</span><br><span class="line"># Loader for projects</span><br><span class="line">executor.global.properties=conf/global.properties</span><br><span class="line">azkaban.project.dir=projects</span><br><span class="line"># Velocity dev mode</span><br><span class="line">velocity.dev.mode=false</span><br><span class="line"># Azkaban Jetty server properties.</span><br><span class="line">jetty.use.ssl=false</span><br><span class="line">jetty.maxThreads=25</span><br><span class="line">jetty.port=8081</span><br><span class="line"># Where the Azkaban web server is located </span><br><span class="line"># 2要修改的地方</span><br><span class="line">azkaban.webserver.url=https://node02:8443</span><br><span class="line"># mail settings</span><br><span class="line">mail.sender=</span><br><span class="line">mail.host=</span><br><span class="line"># User facing web server configurations used to construct the user facing server URLs. They are useful when there is a reverse proxy between Azkaban web servers and users.</span><br><span class="line"># enduser -&gt; myazkabanhost:443 -&gt; proxy -&gt; localhost:8081</span><br><span class="line"># when this parameters set then these parameters are used to generate email links.</span><br><span class="line"># if these parameters are not set then jetty.hostname, and jetty.port(if ssl configured jetty.ssl.port) are used.</span><br><span class="line"># azkaban.webserver.external_hostname=myazkabanhost.com</span><br><span class="line"># azkaban.webserver.external_ssl_port=443</span><br><span class="line"># azkaban.webserver.external_port=8081</span><br><span class="line">job.failure.email=</span><br><span class="line">job.success.email=</span><br><span class="line">lockdown.create.projects=false</span><br><span class="line">cache.directory=cache</span><br><span class="line"># JMX stats</span><br><span class="line">jetty.connector.stats=true</span><br><span class="line">executor.connector.stats=true</span><br><span class="line"># Azkaban plugin settings</span><br><span class="line">azkaban.jobtype.plugin.dir=plugins/jobtypes</span><br><span class="line"># Azkaban mysql settings by default. Users should configure their own username and password.</span><br><span class="line"># 3要修改的地方</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=node03</span><br><span class="line">mysql.database=azkaban_two_server</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=123456</span><br><span class="line">mysql.numconnections=100</span><br><span class="line"># Azkaban Executor settings</span><br><span class="line">executor.maxThreads=50</span><br><span class="line"># 4要修改的地方 添加</span><br><span class="line">executor.port=12321</span><br><span class="line">executor.flow.threads=30</span><br></pre></td></tr></table></figure>

<h3 id="2-3启动"><a href="#2-3启动" class="headerlink" title="2.3启动:"></a>2.3启动:</h3><p>先启动exec-server 在根目录下 bin/start-exec.sh</p>
<p>再启动web-server。 在根目录下 bin/start-web.sh</p>
<p>启动webServer之后进程失败消失，可通过安装包根目录下对应启动日志进行排查。</p>
<p>解决:</p>
<p>需要手动激活executor</p>
<p>到exec服务器的根目录下</p>
<p>向executor.port 发送一个executor 将其action改为activate</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -G &quot;node02:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure>

<p>重启web即可</p>
<p>即可在页面测试</p>
<p><strong>测试阶段可能一直处于running状态</strong></p>
<p>解决:</p>
<p>在exec服务器下 cd 到 plugins/jobtypes</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim commonprivate.properties</span><br><span class="line">加入以下配置:</span><br><span class="line">memCheck.enabled=false</span><br></pre></td></tr></table></figure>

<p>重启exec和web 并重新向executor.port 发送一个executor 将其action改为activate</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -G &quot;node02:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure>

<p>特点:</p>
<ul>
<li>该模式的特点是web服务器和executor服务器分别位于不同的进程中</li>
<li>使用第三方的数据库进行数据的保存 ：mysql</li>
</ul>
<p>注意事项:</p>
<ul>
<li>先对mysql进行初始化操作</li>
<li>配置azkaban.properties 注意时区 mysql相关 ssl</li>
<li>启动时候注意需要自己手动的激活executor服务器 在根目录下启动</li>
<li>如果启动出错 通过安装包根目录下的日志进行判断</li>
<li>访问的页面https</li>
</ul>
<h3 id="3multiple-executor模式部署"><a href="#3multiple-executor模式部署" class="headerlink" title="3multiple-executor模式部署"></a>3multiple-executor模式部署</h3><p>multiple-executor模式是多个executor Server分布在不同服务器上，只需要将azkaban-exec-server安装包拷贝到不同机器上即可组成分布式。</p>
<p>节点配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node03       mysql</span><br><span class="line">node02       web-server和 exec-server</span><br><span class="line">node01       exec-server</span><br></pre></td></tr></table></figure>

<p>1 scp executor server安装包到node01</p>
<p>前提 node01 和node02 有相同的目录结构(元数据存在的路径)才可以使用 pwd 否则回传到root路径下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r azkaban-exec-server-0.1.0-SNAPSHOT/ node01:$PWD</span><br></pre></td></tr></table></figure>

<p>启动:</p>
<p>先启动exec 分别启动node01 和node02上的exec 在向executor.port 发送一个executor 将其action改为activate</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -G &quot;node02:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">curl -G &quot;node01:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure>

<p>在启动web即可</p>
<ul>
<li><p>所谓的 multiple-executor指的是可以在多个机器上分别部署executor服务器</p>
<p>相当于做了一个负载均衡</p>
</li>
<li><p>特别注意：executor启动（包括重启）的时候 默认不会激活 需要自己手动激活</p>
<p>对应的mysql中的表executors active ：0 表示未激活 1表示激活</p>
<p>可以自己手动修改数据提交激活 也可以使用官方的命令请求激活</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -G &quot;node01:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="五-实战"><a href="#五-实战" class="headerlink" title="五 实战"></a>五 实战</h2><ul>
<li>理论上任何一款软件，只有可以通过shell command执行 都可以转化成为azkaban的调度执行</li>
<li>type=command command = sh xxx.sh</li>
</ul>
<h2 id="1．-shell-command调度"><a href="#1．-shell-command调度" class="headerlink" title="1． shell command调度"></a>1． shell command调度</h2><p>创建job描述文件</p>
<p>vi command.job</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#command.job</span><br><span class="line">type=command</span><br><span class="line">command=sh hello.sh</span><br><span class="line">hello.sh 内容</span><br><span class="line">#!/bin/bash</span><br><span class="line">date  &gt; /root/hello.txt</span><br></pre></td></tr></table></figure>

<p>将hello.sh和command.job一起打包为zip</p>
<p>上传azkaban</p>
<h2 id="2．-job依赖调度"><a href="#2．-job依赖调度" class="headerlink" title="2． job依赖调度"></a>2． job依赖调度</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># foo.job</span><br><span class="line">type=command</span><br><span class="line">command=echo foo</span><br><span class="line"># bar.job</span><br><span class="line">type=command</span><br><span class="line">dependencies=foo</span><br><span class="line">command=echo bar</span><br><span class="line">type=command</span><br><span class="line">dependencies=bar</span><br><span class="line">command=echo baidu</span><br></pre></td></tr></table></figure>

<p>资源打包 zip上传:</p>
<p>若前一个未成功 后一个不会执行</p>
<h2 id="3-hdfs的调度"><a href="#3-hdfs的调度" class="headerlink" title="3 hdfs的调度"></a>3 hdfs的调度</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># fs.job</span><br><span class="line">type=command</span><br><span class="line">command=sh hdfs.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line">/export/servers/hadoop-2.7.5/bin/hadoop fs -mkdir /azaz666</span><br></pre></td></tr></table></figure>

<p>打包压缩 zip</p>
<p>通过azkaban的web管理平台创建project并上传job压缩包 启动执行该job</p>
<h2 id="4-MAPREDUCE任务调度"><a href="#4-MAPREDUCE任务调度" class="headerlink" title="4 MAPREDUCE任务调度"></a>4 MAPREDUCE任务调度</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># mrwc.job</span><br><span class="line">type=command</span><br><span class="line">command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop  jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout</span><br></pre></td></tr></table></figure>

<p>将jar包和job文件打包zip压缩</p>
<p>上传</p>
<h2 id="5hive"><a href="#5hive" class="headerlink" title="5hive"></a>5hive</h2><p>hive脚本 test.sql</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">use default;</span><br><span class="line">drop table aztest;</span><br><span class="line">create table aztest(id int,name string) row format delimited fields terminated by &apos;,&apos;;</span><br><span class="line">load data inpath &apos;/aztest/hiveinput&apos; into table aztest;</span><br><span class="line">create table azres as select * from aztest;</span><br><span class="line">insert overwrite directory &apos;/aztest/hiveoutput&apos; select count(1) from aztest;</span><br><span class="line"># hivef.job</span><br><span class="line">type=command</span><br><span class="line">command=/home/hadoop/apps/hive/bin/hive -f &apos;test.sql&apos;</span><br></pre></td></tr></table></figure>

<h2 id="6定时任务调度"><a href="#6定时任务调度" class="headerlink" title="6定时任务调度"></a>6定时任务调度</h2><p>页面中选择schedule进行设置</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Azkaban/" data-id="cjz2c0w8f000bagu5ljizotf6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Impala" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Impala/" class="article-date">
  <time datetime="2019-08-08T03:41:03.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Impala/">Impala</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="一-简介"><a href="#一-简介" class="headerlink" title="一 简介"></a>一 简介</h2><p> impala来自于cloudera，后来贡献给了apache</p>
<p> <strong>impala</strong>是cloudera提供的一款高效率的sql查询工具，提供实时的查询效果，官方测试性能比hive快10到100倍，其sql查询比sparkSQL还要更加快速，号称是当前大数据领域最快的查询sql工具，</p>
<p> impala是参照谷歌的新三篇论文（Caffeine–网络搜索引擎、Pregel–分布式图计算、Dremel–交互式分析工具）当中的Dremel实现而来，其中旧三篇论文分别是（BigTable，GFS，MapReduce）分别对应我们即将学的HBase和已经学过的HDFS以及MapReduce。</p>
<p> <strong>impala是基于hive并使用内存进行计算</strong>，兼顾数据仓库，具有实时，批处理，多并发等优点。</p>
<h3 id="1impala与hive的关系"><a href="#1impala与hive的关系" class="headerlink" title="1impala与hive的关系:"></a>1impala与hive的关系:</h3><p><strong>impala工作底层执行依赖于hive 与hive共用一套元数据存储。在使用impala的时候，必须保证hive服务是正常可靠的，至少metastore开启。</strong></p>
<p>impala是基于hive的大数据分析查询引擎，直接使用hive的元数据库metadata，意味着impala元数据都存储在hive的metastore当中，并且impala兼容hive的绝大多数sql语法。所以需要安装impala的话，必须先安装hive，保证hive安装成功，并且还需要启动hive的metastore服务。</p>
<p>Hive元数据包含用Hive创建的database、table等元信息。元数据存储在关系型数据库中，如Derby、MySQL等。</p>
<p>客户端连接metastore服务，metastore再去连接MySQL数据库来存取元数据。有了metastore服务，就可以有多个客户端同时连接，而且这些客户端不需要知道MySQL数据库的用户名和密码，只需要连接metastore 服务即可。</p>
<p><strong>Hive适合于长时间的批处理查询分析，而Impala适合于实时交互式SQL查询。可以先使用hive进行数据转换处理，之后使用Impala在Hive处理后的结果数据集上进行快速的数据分析。</strong></p>
<h3 id="2-impala与hive的异同"><a href="#2-impala与hive的异同" class="headerlink" title="2 impala与hive的异同"></a>2 impala与hive的异同</h3><p>相同:</p>
<p>数据表元数据、ODBC/JDBC驱动、SQL语法、灵活的文件格式、存储资源池等</p>
<p>不同:</p>
<p><strong>impala最大的跟hive的不同在于 不在把sql编译成mr程序执行 编译成执行计划树</strong></p>
<p><strong>impala的运行分为前端和后端 前端为java实现 后端为c++实现</strong></p>
<h3 id="impala的优化技术"><a href="#impala的优化技术" class="headerlink" title="impala的优化技术:"></a>impala的优化技术:</h3><p>使用LLVM产生运行代码，针对特定查询生成特定代码，同时使用Inline的方式减少函数调用的开销，加快执行效率。(C++特性)</p>
<p>充分利用可用的硬件指令（SSE4.2）。</p>
<p>更好的IO调度，Impala知道数据块所在的磁盘位置能够更好的利用多磁盘的优势，同时Impala支持直接数据块读取和本地代码计算checksum。</p>
<p>通过选择合适数据存储格式可以得到最好性能（Impala支持多种存储格式）。</p>
<p>最大使用内存，中间结果不写磁盘，及时通过网络以stream的方式传递。</p>
<h3 id="执行计划"><a href="#执行计划" class="headerlink" title="执行计划:"></a>执行计划:</h3><p><strong>Hive</strong>: 依赖于MapReduce执行框架，执行计划分成 map-&gt;shuffle-&gt;reduce-&gt;map-&gt;shuffle-&gt;reduce…的模型。如果一个Query会 被编译成多轮MapReduce，则会有更多的写中间结果。由于MapReduce执行框架本身的特点，过多的中间过程会增加整个Query的执行时间。</p>
<p><strong>Impala</strong>: 把执行计划表现为一棵完整的执行计划树，可以更自然地分发执行计划到各个Impalad执行查询，而不用像Hive那样把它组合成管道型的 map-&gt;reduce模式，以此保证Impala有更好的并发性和避免不必要的中间sort与shuffle。</p>
<h3 id="数据流"><a href="#数据流" class="headerlink" title="数据流:"></a>数据流:</h3><p><strong>Hive</strong>: 采用推的方式，每一个计算节点计算完成后将数据主动推给后续节点。</p>
<p><strong>Impala</strong>: 采用拉的方式，后续节点通过getNext主动向前面节点要数据，以此方式数据可以流式的返回给客户端，且只要有1条数据被处理完，就可以立即展现出来，而不用等到全部处理完成，更符合SQL交互式查询使用。</p>
<h3 id="内存使用"><a href="#内存使用" class="headerlink" title="内存使用:"></a>内存使用:</h3><p><strong>Hive</strong>: 在执行过程中如果内存放不下所有数据，则会使用外存，以保证Query能顺序执行完。每一轮MapReduce结束，中间结果也会写入HDFS中，同样由于MapReduce执行架构的特性，shuffle过程也会有写本地磁盘的操作。</p>
<p><strong>Impala</strong>: 1.0.1，而不会利用外存，以后版本应该会进行改进。这使用得目前处理会受到一定的限制，最好还是与配合使用</p>
<h3 id="调度"><a href="#调度" class="headerlink" title="调度:"></a>调度:</h3><p><strong>Hive</strong>: 任务调度依赖于Hadoop的调度策略。</p>
<p><strong>Impala</strong>: 调度由自己完成，目前只有一种调度器simple-schedule，它会尽量满足数据的局部性，扫描数据的进程尽量靠近数据本身所在的物理机器。调度器 目前还比较简单，在SimpleScheduler::GetBackend中可以看到，现在还没有考虑负载，网络IO状况等因素进行调度。但目前 Impala已经有对执行过程的性能统计分析，应该以后版本会利用这些统计信息进行调度吧。</p>
<h3 id="容错"><a href="#容错" class="headerlink" title="容错:"></a>容错:</h3><p><strong>Hive</strong>: 依赖于Hadoop的容错能力。</p>
<p><strong>Impala</strong>: 在查询过程中，没有容错逻辑，如果在执行过程中发生故障，则直接返回错误（<strong>这与Impala的设计有关，因为Impala定位于实时查询，一次查询失败， 再查一次就好了，再查一次的成本很低</strong>）。</p>
<h3 id="使用地"><a href="#使用地" class="headerlink" title="使用地:"></a>使用地:</h3><p><strong>Hive</strong>: 复杂的批处理查询任务，数据转换任务。</p>
<p><strong>Impala</strong>：实时数据分析，因为不支持UDF，能处理的问题域有一定的限制，与Hive配合使用,对Hive的结果数据集进行实时分析。</p>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构:"></a>架构:</h3><p>Impala主要由Impalad、 State Store、Catalogd和CLI组成。</p>
<ul>
<li>impala 可以集群部署<ul>
<li>Impalad(impala server):可以部署多个不同机器上，通常与datanode部署在同一个节点 方便数据本地计算，负责具体执行本次查询sql的impalad称之为Coordinator。每个impala server都可以对外提供服务。</li>
<li>impala state store:主要是保存impalad的状态信息 监视其健康状态</li>
<li>impala catalogd :metastore维护的网关 负责跟hive 的metastore进行交互 同步hive的元数据到impala自己的元数据中。</li>
<li>CLI:用户操作impala的方式（impala shell、jdbc、hue）</li>
</ul>
</li>
<li>impala 查询处理流程<ul>
<li>impalad分为java前端（接受解析sql编译成执行计划树），c++后端（负责具体的执行计划树操作）</li>
<li>impala sql—-&gt;impalad（Coordinator）—-&gt;调用java前端编译sql成计划树——&gt;以Thrift数据格式返回给C++后端——&gt;根据执行计划树、数据位于路径（libhdfs和hdfs交互）、impalad状态分配执行计划 查询—–&gt;汇总查询结果—–&gt;返回给java前端—-&gt;用户cli</li>
<li>跟hive不同就在于整个执行中已经没有了mapreduce程序的存在</li>
</ul>
</li>
</ul>
<h2 id="二安装部署"><a href="#二安装部署" class="headerlink" title="二安装部署"></a>二安装部署</h2><p>前提集群提前安装好Hadoop和hive</p>
<p>hive安装包scp在所有需要安装impala的节点上，因为impala需要引用hive的依赖包。</p>
<p>hadoop框架需要支持C程序访问接口，Hadoop/native，如果有该路径下有这么文件，就证明支持C接口。</p>
<p>1 下载安装包和依赖包</p>
<p>由于impala没有提供tar包进行安装，只提供了rpm包。因此在安装impala的时候，需要使用rpm包来进行安装。rpm包只有cloudera公司提供了，所以去cloudera公司网站进行下载rpm包即可。</p>
<p>但是另外一个问题，impala的rpm包依赖非常多的其他的rpm包，可以一个个的将依赖找出来，也可以将所有的rpm包下载下来，制作成我们本地yum源来进行安装。这里就选择制作本地的yum源来进行安装。</p>
<p>所以首先需要下载到所有的rpm包，下载地址如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://archive.cloudera.com/cdh5/repo-as-tarball/5.14.0/cdh5.14.0-centos6.tar.gz</span><br></pre></td></tr></table></figure>

<p>2 配置本地yam源</p>
<h3 id="1-1．-上传安装包解压"><a href="#1-1．-上传安装包解压" class="headerlink" title="1.1． 上传安装包解压"></a>1.1． 上传安装包解压</h3><p>使用sftp的方式把安装包大文件上传到服务器/cloudera_data目录下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /cloudera_data</span><br><span class="line">tar -zxvf cdh5.14.0-centos6.tar.gz</span><br></pre></td></tr></table></figure>

<h3 id="1-2．-配置本地yum源信息"><a href="#1-2．-配置本地yum源信息" class="headerlink" title="1.2． 配置本地yum源信息"></a>1.2． 配置本地yum源信息</h3><p>安装Apache Server服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum  -y install httpd</span><br><span class="line">service httpd start</span><br><span class="line">chkconfig httpd on</span><br></pre></td></tr></table></figure>

<p>配置本地yum源的文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/yum.repos.d</span><br><span class="line">vim localimp.repo </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[localimp]</span><br><span class="line">name=localimp</span><br><span class="line">baseurl=http://node-3/cdh5.14.0/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p>创建apache httpd的读取链接</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /cloudera_data/cdh/5.14.0 /var/www/html/cdh5.14.0</span><br></pre></td></tr></table></figure>

<p><strong>确保**</strong>linux<strong>的</strong>Selinux<strong><em>\</em>关闭</strong></p>
<p>通过浏览器访问本地yum源，如果出现下述页面则成功。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node03/cdh5.14.0</span><br></pre></td></tr></table></figure>

<p>将本地yum源配置文件localimp.repo发放到所有需要安装impala的节点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/yum.repos.d/</span><br><span class="line">scp localimp.repo  node02:$PWD</span><br><span class="line">scp localimp.repo  node01:$PWD</span><br></pre></td></tr></table></figure>

<p>3 安装 impala</p>
<p>节点规划</p>
<table>
<thead>
<tr>
<th align="left">服务名称</th>
<th align="left">从节点</th>
<th align="left">从节点</th>
<th align="left">主节点</th>
</tr>
</thead>
<tbody><tr>
<td align="left">impala-catalog</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">Node-3</td>
</tr>
<tr>
<td align="left">impala-state-store</td>
<td align="left"></td>
<td align="left"></td>
<td align="left">Node-3</td>
</tr>
<tr>
<td align="left">impala-server(impalad)</td>
<td align="left">Node-1</td>
<td align="left">Node-2</td>
<td align="left">Node-3</td>
</tr>
</tbody></table>
<h3 id="1-1．-主节点安装"><a href="#1-1．-主节点安装" class="headerlink" title="1.1． 主节点安装"></a>1.1． 主节点安装</h3><p>在规划的<strong>主节点**</strong>node-3**执行以下命令进行安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install   -y impala impala-server impala-state-store impala-catalog impala-shell</span><br></pre></td></tr></table></figure>

<h3 id="1-2．-从节点安装"><a href="#1-2．-从节点安装" class="headerlink" title="1.2． 从节点安装"></a>1.2． 从节点安装</h3><p>在规划的<strong>从节点**</strong>node-1<strong>、</strong>node-2**执行以下命令进行安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y impala-server</span><br></pre></td></tr></table></figure>

<p>配置hive和Hadoop</p>
<p>1 hive</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">vim /export/servers/hive/conf/hive-site.xml</span><br><span class="line">&lt;configuration&gt; </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;jdbc:mysql://node-1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt;  </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt;  </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;root&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt;  </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;hadoop&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt;  </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;true&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt;  </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;hive.cli.print.header&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;true&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt;  </span><br><span class="line">  &lt;!-- 绑定运行hiveServer2的主机host,默认localhost --&gt;  </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;node-1&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt;  </span><br><span class="line">  &lt;!-- 指定hive metastore服务请求的uri地址 --&gt;  </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;hive.metastore.uris&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;thrift://node-1:9083&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt;  </span><br><span class="line">  &lt;property&gt; </span><br><span class="line">    &lt;name&gt;hive.metastore.client.socket.timeout&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;3600&lt;/value&gt; </span><br><span class="line">  &lt;/property&gt; </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>将hive安装包cp给其他两个机器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/</span><br><span class="line">scp -r hive/ node-2:$PWD</span><br><span class="line">scp -r hive/ node-3:$PWD</span><br></pre></td></tr></table></figure>

<h3 id="1-1．-修改hadoop配置"><a href="#1-1．-修改hadoop配置" class="headerlink" title="1.1． 修改hadoop配置"></a>1.1． 修改hadoop配置</h3><p>所有节点创建下述文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/run/hdfs-sockets</span><br></pre></td></tr></table></figure>

<p>修改所有节点的hdfs-site.xml添加以下配置，修改完之后重启hdfs集群生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim   etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure>

<p><code>dfs.client.read.shortcircuit</code>打开`DFSClient``本地读取数据的控制，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`dfs.domain.socket.path``是``Datanode``和``DFSClient``之间沟通的``Socket``的本地路径。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>把更新hadoop的配置文件，scp给其他机器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5/etc/hadoop</span><br><span class="line">scp -r hdfs-site.xml node-2:$PWD</span><br><span class="line">scp -r hdfs-site.xml node-3:$PWD</span><br></pre></td></tr></table></figure>

<p>注意：root用户不需要下面操作，普通用户需要这一步操作。</p>
<p>给这个文件夹赋予权限，如果用的是普通用户hadoop，那就直接赋予普通用户的权限，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown  -R  hadoop:hadoop   /var/run/hdfs-sockets/</span><br></pre></td></tr></table></figure>

<p>因为这里直接用的root用户，所以不需要赋权限了。</p>
<h3 id="1-1．-重启hadoop、hive"><a href="#1-1．-重启hadoop、hive" class="headerlink" title="1.1． 重启hadoop、hive"></a>1.1． 重启hadoop、hive</h3><p>在node-1上执行下述命令分别启动hive metastore服务和hadoop。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cd  /export/servers/hive</span><br><span class="line">nohup bin/hive --service metastore &amp;</span><br><span class="line">nohup bin/hive --service hiveserver2 &amp;</span><br><span class="line"></span><br><span class="line">清空日志文件：cat /dev/null &gt; nohup.out</span><br><span class="line"></span><br><span class="line">启动hiveserver2 的服务  </span><br><span class="line">nohup  bin/hive --service  hiveserver2 &amp;</span><br><span class="line"></span><br><span class="line">再启动metastore服务</span><br><span class="line">nohup  bin/hive --service  metastore &amp;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果先启动hiveserver2成功再启动metastore报错怎么办？？</span><br><span class="line">先把hiveserver2的服务杀死，然后先启动metastore  再启动hiveserver2 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">如果先启动metastore成功再启动hiveserver2报错怎么办？？</span><br><span class="line">先把metastore的服务杀死，然后先启动hiveserver2  再启动metastore</span><br><span class="line"> </span><br><span class="line">cd /export/servers/hadoop-2.7.5/</span><br><span class="line">sbin/stop-dfs.sh  |  sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<h3 id="1-2．-复制hadoop、hive配置文件"><a href="#1-2．-复制hadoop、hive配置文件" class="headerlink" title="1.2． 复制hadoop、hive配置文件"></a>1.2． 复制hadoop、hive配置文件</h3><p>impala的配置目录为/etc/impala/conf，这个路径下面需要把core-site.xml，hdfs-site.xml以及hive-site.xml。</p>
<p>所有节点执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp -r /export/servers/hadoop-2.7.5/etc/hadoop/core-site.xml /etc/impala/conf/core-site.xml</span><br><span class="line">cp -r /export/servers/hadoop-2.7.5/etc/hadoop/hdfs-site.xml /etc/impala/conf/hdfs-site.xml</span><br><span class="line">cp -r /export/servers/hive/conf/hive-site.xml /etc/impala/conf/hive-site.xml</span><br></pre></td></tr></table></figure>

<h2 id="1．-修改impala配置"><a href="#1．-修改impala配置" class="headerlink" title="1． 修改impala配置"></a>1． 修改impala配置</h2><h3 id="1-1．-修改impala默认配置"><a href="#1-1．-修改impala默认配置" class="headerlink" title="1.1． 修改impala默认配置"></a>1.1． 修改impala默认配置</h3><p>所有节点更改impala默认配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/default/impala</span><br><span class="line">IMPALA_CATALOG_SERVICE_HOST=node-3</span><br><span class="line">IMPALA_STATE_STORE_HOST=node-3</span><br></pre></td></tr></table></figure>

<h3 id="1-2．-添加mysql驱动"><a href="#1-2．-添加mysql驱动" class="headerlink" title="1.2． 添加mysql驱动"></a>1.2． 添加mysql驱动</h3><p>通过配置/etc/default/impala中可以发现已经指定了mysql驱动的位置名字。</p>
<p>使用软链接指向该路径即可（3台机器都需要执行）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/servers/hive/lib/mysql-connector-java-5.1.32.jar /usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>

<h3 id="1-3．-修改bigtop配置"><a href="#1-3．-修改bigtop配置" class="headerlink" title="1.3． 修改bigtop配置"></a>1.3． 修改bigtop配置</h3><p>修改bigtop的java_home路径（3台机器）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/default/bigtop-utils</span><br><span class="line">export JAVA_HOME=/export/servers/jdk1.8.0_65</span><br></pre></td></tr></table></figure>

<h2 id="1．-启动、关闭impala服务"><a href="#1．-启动、关闭impala服务" class="headerlink" title="1． 启动、关闭impala服务"></a>1． 启动、关闭impala服务</h2><p>主节点node-3启动以下三个服务进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">service impala-state-store start</span><br><span class="line">service impala-catalog start</span><br><span class="line">service impala-server start</span><br></pre></td></tr></table></figure>

<p>从节点启动node-1与node-2启动impala-server</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service  impala-server  start</span><br></pre></td></tr></table></figure>

<p>查看impala进程是否存在</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep impala</span><br></pre></td></tr></table></figure>

<p> ``</p>
<p>启动之后所有关于impala的<strong>日志默认都在**</strong>/var/log/impala**</p>
<p>如果需要关闭impala服务 把命令中的start该成stop即可。注意如果关闭之后进程依然驻留，可以采取下述方式删除。正常情况下是随着关闭消失的。</p>
<p>解决方式：</p>
<p><a href="file:///D:/%5Cblog%5Cmyblog%5Csource%5Cimages%5Cimpala%5Cim.png" target="_blank" rel="noopener"><img src="file:///D:/%5Cblog%5Cmyblog%5Csource%5Cimages%5Cimpala%5Cim.png" alt="im"></a></p>
<p>访问impalad的管理界面node03:25000</p>
<p>访问statestored的管理界面node03:25010</p>
<h2 id="四-impala的shell参数"><a href="#四-impala的shell参数" class="headerlink" title="四 impala的shell参数"></a>四 impala的shell参数</h2>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Impala/" data-id="cjz2c0w8k000eagu5oos4hfcj" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Oozie" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Oozie/" class="article-date">
  <time datetime="2019-08-08T03:40:50.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Oozie/">Oozie</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Oozie"><a href="#Oozie" class="headerlink" title="Oozie"></a>Oozie</h1><h2 id="一-概述"><a href="#一-概述" class="headerlink" title="一 概述"></a>一 概述</h2><ul>
<li>是一个工作流调度软件 本身属于cloudera 后来贡献给了apache</li>
<li>oozie目的根据一个定义DAG（有向无环图）执行工作流程</li>
<li>oozie本身的配置是一种xml格式的配置文件 oozie跟hue配合使用将会很方便</li>
<li>oozie特点：顺序执行 周期重复定时 可视化 追踪结果</li>
</ul>
<h1 id="二-架构"><a href="#二-架构" class="headerlink" title="二 架构"></a>二 架构</h1><p><strong>Oozie Client</strong>：提供命令行、java api、rest等方式，对Oozie的工作流流程的提交、启动、运行等操作；</p>
<p><strong>Oozie WebApp</strong>：即 Oozie Server,本质是一个java应用。可以使用内置的web容器，也可以使用外置的web容器；</p>
<p><strong>Hadoop Cluster</strong>：底层执行Oozie编排流程的各个hadoop生态圈组件；launcher Job(single map task no reduce task) 和actual program(mr hive pig sqark etc) <strong>oozie各种类型任务提交底层依赖于mr程序 首先启动一个没有reducetak的mr 通过这个mr把各个不同类型的任务提交到具体的集群上执行</strong></p>
<h1 id="三-原理"><a href="#三-原理" class="headerlink" title="三 原理"></a>三 原理</h1><p> Oozie对工作流的编排，是基于workflow.xml文件来完成的。用户预先将工作流执行规则定制于workflow.xml文件中，并在job.properties配置相关的参数，然后由Oozie Server向MR提交job来启动工作流。</p>
<h2 id="1-流程节点"><a href="#1-流程节点" class="headerlink" title="1 流程节点"></a>1 流程节点</h2><p><strong>Control Flow Nodes</strong>：控制工作流执行路径，包括start，end，kill，decision判断的节点，fork 插值 分开执行不同的任务 并行执行,join 合并。</p>
<p><strong>Action Nodes</strong>：决定每个操作执行的任务类型，包括MapReduce、java、hive、shell等。</p>
<p><a href="https://manzhong.github.io/images/Oozie/lc.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/Oozie/lc.png" alt="img"></a></p>
<p>上述两种类型结合起来 就可以描绘出一个工作流的DAG图。</p>
<h2 id="2-工作流类型"><a href="#2-工作流类型" class="headerlink" title="2 工作流类型"></a>2 工作流类型</h2><ul>
<li><ul>
<li>workflow 基本类型的工作流 只会按照定义顺序执行 无定时触发和条件触发</li>
<li><a href="https://manzhong.github.io/images/Oozie/lx1.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/Oozie/lx1.png" alt="img"></a></li>
</ul>
</li>
<li><p>coordinator Coordinator将多个工作流Job组织起来，称为Coordinator<br>Job，并指定触发时间和频率，还可以配置数据集、并发数等，类似于在工作流外部增加了一个协调器来管理这些工作流的工作流Job的运行。</p>
<ul>
<li><p><a href="https://manzhong.github.io/images/Oozie/lx2.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/Oozie/lx2.png" alt="img"></a></p>
</li>
<li><p>bundle 针对coordinator的批处理工作流。Bundle将多个Coordinator管理起来，这样我们只需要一个Bundle提交即可。</p>
<p><a href="https://manzhong.github.io/images/Oozie/lx3.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/Oozie/lx3.png" alt="img"></a></p>
</li>
</ul>
</li>
</ul>
<h1 id="四-Apache-Oozie-安装"><a href="#四-Apache-Oozie-安装" class="headerlink" title="四 Apache Oozie 安装"></a>四 Apache Oozie 安装</h1><p>版本问题：Apache官方提供的是源码包 需要自己结合hadoop生态圈软件环境进行编译 兼容性问题特别难以处理 因此可以使用第三方商业公司编译好 Cloudera（CDH）</p>
<h2 id="1-修改Hadoop的配置"><a href="#1-修改Hadoop的配置" class="headerlink" title="1 修改Hadoop的配置"></a>1 修改Hadoop的配置</h2><h3 id="1-1-配置httpfs服务"><a href="#1-1-配置httpfs服务" class="headerlink" title="1.1 配置httpfs服务"></a>1.1 配置httpfs服务</h3><p>修改Hadoop的配置文件core-site.xml：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><code>hadoop.proxyuser.root.hosts</code>允许通过httpfs方式访问hdfs的主机名、域名；</p>
<p><code>hadoop.proxyuser.root.groups</code>允许访问的客户端的用户组</p>
<h3 id="1-2-配置jobhistory服务"><a href="#1-2-配置jobhistory服务" class="headerlink" title="1.2 配置jobhistory服务"></a>1.2 配置jobhistory服务</h3><p>修改Hadoop的配置文件mapred-site.xml:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node01:10020&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;MapReduce JobHistory Server IPC host:port&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node01:19888&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;MapReduce JobHistory Server Web UI host:port&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 配置运行过的日志存放在hdfs上的存放路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/export/data/history/done&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 配置正在运行中的日志在hdfs上的存放路径 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/export/data/history/done_intermediate&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="1-3-重启hadoop集群相关服务"><a href="#1-3-重启hadoop集群相关服务" class="headerlink" title="1.3 重启hadoop集群相关服务"></a>1.3 重启hadoop集群相关服务</h3><p>启动history-server</p>
<p>mr-jobhistory-daemon.sh start historyserver</p>
<p>停止history-server</p>
<p>mr-jobhistory-daemon.sh stop historyserver</p>
<p>通过浏览器访问Hadoop Jobhistory的WEBUI</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node01:19888</span><br></pre></td></tr></table></figure>

<h2 id="2-解压-Oozie的安装包"><a href="#2-解压-Oozie的安装包" class="headerlink" title="2 解压 Oozie的安装包"></a>2 解压 Oozie的安装包</h2><p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">oozie的安装包上传到/export/softwares</span><br><span class="line">tar -zxvf oozie-4.1.0-cdh5.14.0.tar.gz -C ../servers/</span><br><span class="line"></span><br><span class="line">解压hadooplibs到与oozie平行的目录</span><br><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">tar -zxvf oozie-hadooplibs-4.1.0-cdh5.14.0.tar.gz -C ../</span><br></pre></td></tr></table></figure>

<h2 id="3-添加相关依赖"><a href="#3-添加相关依赖" class="headerlink" title="3 添加相关依赖"></a>3 添加相关依赖</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">oozie的安装路径下创建libext目录</span><br><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">mkdir -p libext</span><br><span class="line"></span><br><span class="line">拷贝hadoop依赖包到libext</span><br><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">cp -ra hadooplibs/hadooplib-2.6.0-cdh5.14.0.oozie-4.1.0-cdh5.14.0/* libext/</span><br><span class="line"></span><br><span class="line">上传mysql的驱动包到libext</span><br><span class="line">mysql-connector-java-5.1.32.jar</span><br><span class="line">添加ext-2.2.zip压缩包到libext</span><br><span class="line">ext-2.2.zip</span><br></pre></td></tr></table></figure>

<h2 id="4-修改oozie-site-xml文件"><a href="#4-修改oozie-site-xml文件" class="headerlink" title="4 修改oozie-site.xml文件"></a>4 修改oozie-site.xml文件</h2><p>oozie默认使用的是UTC的时区，需要在oozie-site.xml当中配置时区为<strong>GMT+0800</strong>时区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/conf</span><br><span class="line">vim oozie-site.xml</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">        &lt;name&gt;oozie.service.JPAService.jdbc.driver&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">        &lt;name&gt;oozie.service.JPAService.jdbc.url&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://node03:3306/oozie&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;oozie.service.JPAService.jdbc.username&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;root&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;oozie.service.JPAService.jdbc.password&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;123456&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">			&lt;name&gt;oozie.processing.timezone&lt;/name&gt;</span><br><span class="line">			&lt;value&gt;GMT+0800&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">        &lt;name&gt;oozie.service.coord.check.maximum.frequency&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;     </span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;oozie.service.HadoopAccessorService.hadoop.configurations&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*=/export/servers/hadoop-2.7.5/etc/hadoop&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h2 id="5-初始化MySQL相关信息"><a href="#5-初始化MySQL相关信息" class="headerlink" title="5 初始化MySQL相关信息"></a>5 初始化MySQL相关信息</h2><p>上传oozie的解压后目录的下的yarn.tar.gz到hdfs目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/oozie-setup.sh  sharelib create -fs hdfs://node01:8020 -locallib oozie-sharelib-4.1.0-cdh5.14.0-yarn.tar.gz</span><br></pre></td></tr></table></figure>

<p>本质上就是将这些jar包解压到了hdfs上面的路径下面去</p>
<p>创建mysql数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line">create database oozie;</span><br></pre></td></tr></table></figure>

<p>初始化创建oozie的数据库表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">bin/oozie-setup.sh  db create -run -sqlfile oozie.sql</span><br></pre></td></tr></table></figure>

<h2 id="6-打包项目生成war包"><a href="#6-打包项目生成war包" class="headerlink" title="6 打包项目生成war包"></a>6 打包项目生成war包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">bin/oozie-setup.sh  prepare-war</span><br></pre></td></tr></table></figure>

<h2 id="7-配置oozie-环境变量"><a href="#7-配置oozie-环境变量" class="headerlink" title="7 配置oozie 环境变量"></a>7 配置oozie 环境变量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">export OOZIE_HOME=/export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">export OOZIE_URL=http://node01.hadoop.com:11000/oozie</span><br><span class="line">export PATH=$PATH:$OOZIE_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h2 id="8-启动关闭oozie服务"><a href="#8-启动关闭oozie服务" class="headerlink" title="8 启动关闭oozie服务"></a>8 启动关闭oozie服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">启动命令</span><br><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">bin/oozied.sh start </span><br><span class="line">关闭命令</span><br><span class="line">bin/oozied.sh stop</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oozie-4.1.0-cdh5.14.0/oozie-server/temp/oozie.pid</span><br></pre></td></tr></table></figure>

<p>启动的时候产生的 pid文件，如果是kill方式关闭进程 则需要删除该文件重新启动，否则再次启动会报错。启动的时候产生的 pid文件，如果是kill方式关闭进程 则需要删除该文件重新启动，否则再次启动会报错。</p>
<h2 id="9-浏览器web-ui-页面"><a href="#9-浏览器web-ui-页面" class="headerlink" title="9 浏览器web ui 页面"></a>9 浏览器web ui 页面</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node01:11000/oozie</span><br></pre></td></tr></table></figure>

<h2 id="10-解决oozie-页面显示异常"><a href="#10-解决oozie-页面显示异常" class="headerlink" title="10 解决oozie 页面显示异常"></a>10 解决oozie 页面显示异常</h2><p>页面访问的时候，发现oozie使用的还是GMT的时区，与我们现在的时区相差一定的时间，所以需要调整一个js的获取时区的方法，将其改成我们现在的时区。</p>
<p>修改js当中的时区问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd  oozie-server/webapps/oozie</span><br><span class="line">vim oozie-console.js</span><br><span class="line">function getTimeZone() &#123;</span><br><span class="line">    Ext.state.Manager.setProvider(new Ext.state.CookieProvider());</span><br><span class="line">    return Ext.state.Manager.get(&quot;TimezoneId&quot;,&quot;GMT+0800&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重启oozie服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">bin/oozied.sh stop</span><br><span class="line">bin/oozied.sh start</span><br></pre></td></tr></table></figure>

<h1 id="五-Oozie的实战"><a href="#五-Oozie的实战" class="headerlink" title="五 Oozie的实战"></a>五 Oozie的实战</h1><p>oozie安装好了之后，需要测试oozie的功能是否完整好使，官方已经给自带带了各种测试案例，可以通过官方提供的各种案例来学习oozie的使用，后续也可以把这些案例作为模板在企业实际中使用。</p>
<p>先把官方提供的各种案例给解压出来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">tar -zxvf oozie-examples.tar.gz</span><br></pre></td></tr></table></figure>

<p>创建统一的工作目录，便于集中管理oozie。企业中可任意指定路径。这里直接在oozie的安装目录下面创建工作目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">mkdir oozie_works</span><br></pre></td></tr></table></figure>

<h2 id="1-．-优化更新hadoop相关配置"><a href="#1-．-优化更新hadoop相关配置" class="headerlink" title="1 ． 优化更新hadoop相关配置"></a>1 ． 优化更新hadoop相关配置</h2><p>1.1 yarn 容器资源 分配属性</p>
<p>yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;!—节点最大可用内存，结合实际物理内存调整 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3072&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!—每个容器可以申请内存资源的最小值，最大值 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3072&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!—修改为Fair公平调度，动态调整资源，避免yarn上任务等待（多线程执行） --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!—Fair调度时候是否开启抢占功能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!—超过多少开始抢占，默认0.8--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.fair.preemption.cluster-utilization-threshold&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1.0&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>1.2mapreduce资源申请配置</p>
<p>设置mapreduce.map.memory.mb和mapreduce.reduce.memory.mb配置</p>
<p>否则Oozie读取的默认配置 -1, 提交给yarn的时候会抛异常<em>Invalid resource request, requested memory &lt; 0, or requested memory &gt; max configured, requestedMemory=-1, maxMemory=8192</em></p>
<p><strong>mapred-site.xml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;!—单个maptask、reducetask可申请内存大小 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>scp给其他机器 重启集群 （hadoop ） oozie</p>
<p>##1． Oozie调度shell脚本</p>
<p>把shell的任务模板拷贝到oozie的工作目录当中去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">cp -r examples/apps/shell/ oozie_works/</span><br></pre></td></tr></table></figure>

<p>准备待调度的shell脚本文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">vim oozie_works/shell/hello.sh</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong>：这个脚本一定要是在我们oozie工作路径下的shell路径下的位置</p>
<p>#!/bin/bash</p>
<p>echo “hello world” &gt;&gt; /export/servers/hello_oozie.txt</p>
<p>修改job.properties</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/shell</span><br><span class="line">vim job.properties</span><br><span class="line"></span><br><span class="line">nameNode=hdfs://node01:8020</span><br><span class="line">jobTracker=node01:8032</span><br><span class="line">queueName=default</span><br><span class="line">examplesRoot=oozie_works</span><br><span class="line">oozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/shell</span><br><span class="line">EXEC=hello.sh</span><br></pre></td></tr></table></figure>

<p><strong>jobTracker</strong>：在hadoop2当中，jobTracker这种角色已经没有了，只有resourceManager，这里给定resourceManager的IP及端口即可。</p>
<p><strong>queueName</strong>：提交mr任务的队列名；</p>
<p><strong>examplesRoot</strong>：指定oozie的工作目录；</p>
<p><strong>oozie.wf.application.path</strong>：指定oozie调度资源存储于hdfs的工作路径；</p>
<p><strong>EXEC</strong>：指定执行任务的名称。</p>
<p>修改workflow.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt;</span><br><span class="line">&lt;start to=&quot;shell-node&quot;/&gt;</span><br><span class="line">&lt;action name=&quot;shell-node&quot;&gt;</span><br><span class="line">    &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt;</span><br><span class="line">        &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</span><br><span class="line">        &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">            &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</span><br><span class="line">            &lt;/property&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">        &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt;</span><br><span class="line">        &lt;file&gt;/user/root/oozie_works/shell/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt;</span><br><span class="line">        &lt;capture-output/&gt;</span><br><span class="line">    &lt;/shell&gt;</span><br><span class="line">    &lt;ok to=&quot;end&quot;/&gt;</span><br><span class="line">    &lt;error to=&quot;fail&quot;/&gt;</span><br><span class="line">&lt;/action&gt;</span><br><span class="line">&lt;decision name=&quot;check-output&quot;&gt;</span><br><span class="line">    &lt;switch&gt;</span><br><span class="line">        &lt;case to=&quot;end&quot;&gt;</span><br><span class="line">            $&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;] eq &apos;Hello Oozie&apos;&#125;</span><br><span class="line">        &lt;/case&gt;</span><br><span class="line">        &lt;default to=&quot;fail-output&quot;/&gt;</span><br><span class="line">    &lt;/switch&gt;</span><br><span class="line">&lt;/decision&gt;</span><br><span class="line">&lt;kill name=&quot;fail&quot;&gt;</span><br><span class="line">    &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;</span><br><span class="line">&lt;/kill&gt;</span><br><span class="line">&lt;kill name=&quot;fail-output&quot;&gt;</span><br><span class="line">    &lt;message&gt;Incorrect output, expected [Hello Oozie] but was [$&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;]&#125;]&lt;/message&gt;</span><br><span class="line">&lt;/kill&gt;</span><br><span class="line">&lt;end name=&quot;end&quot;/&gt;</span><br><span class="line">&lt;/workflow-app&gt;</span><br></pre></td></tr></table></figure>

<h3 id="1-1．-上传调度任务到hdfs"><a href="#1-1．-上传调度任务到hdfs" class="headerlink" title="1.1． 上传调度任务到hdfs"></a>1.1． 上传调度任务到hdfs</h3><p>注意：上传的hdfs目录为/user/root，因为hadoop启动的时候使用的是root用户，如果hadoop启动的是其他用户，那么就上传到/user/其他用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">hdfs dfs -put oozie_works/ /user/root</span><br></pre></td></tr></table></figure>

<p>通过oozie的命令来执行调度任务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line"></span><br><span class="line">bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/shell/job.properties  -run</span><br></pre></td></tr></table></figure>

<p>杀死任务流:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/oozie job -oozie http://node01:11000/oozie -kill job的ID</span><br></pre></td></tr></table></figure>

<p>##2． Oozie调度hive</p>
<h3 id="2-1-准备模板"><a href="#2-1-准备模板" class="headerlink" title="2.1 准备模板"></a>2.1 准备模板</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">cp -ra examples/apps/hive2/ oozie_works/</span><br></pre></td></tr></table></figure>

<p>2.2 修改模板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">修改job.properties</span><br><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/hive2</span><br><span class="line">vim job.properties</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">nameNode=hdfs://node01:8020</span><br><span class="line">jobTracker=node01:8032</span><br><span class="line">queueName=default</span><br><span class="line">jdbcURL=jdbc:hive2://node01:10000/default</span><br><span class="line">examplesRoot=oozie_works</span><br><span class="line"></span><br><span class="line">oozie.use.system.libpath=true</span><br><span class="line"># 配置我们文件上传到hdfs的保存路径 实际上就是在hdfs 的/user/root/oozie_works/hive2这个路径下</span><br><span class="line">oozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/hive2</span><br></pre></td></tr></table></figure>

<p>修改workflow.xml（实际上无修改）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.5&quot; name=&quot;hive2-wf&quot;&gt;</span><br><span class="line">    &lt;start to=&quot;hive2-node&quot;/&gt;</span><br><span class="line"></span><br><span class="line">    &lt;action name=&quot;hive2-node&quot;&gt;</span><br><span class="line">        &lt;hive2 xmlns=&quot;uri:oozie:hive2-action:0.1&quot;&gt;</span><br><span class="line">            &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</span><br><span class="line">            &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</span><br><span class="line">            &lt;prepare&gt;</span><br><span class="line">                &lt;delete path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/hive2&quot;/&gt;</span><br><span class="line">                &lt;mkdir path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data&quot;/&gt;</span><br><span class="line">            &lt;/prepare&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">            &lt;jdbc-url&gt;$&#123;jdbcURL&#125;&lt;/jdbc-url&gt;</span><br><span class="line">            &lt;script&gt;script.q&lt;/script&gt;</span><br><span class="line">            &lt;param&gt;INPUT=/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/table&lt;/param&gt;</span><br><span class="line">            &lt;param&gt;OUTPUT=/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/hive2&lt;/param&gt;</span><br><span class="line">        &lt;/hive2&gt;</span><br><span class="line">        &lt;ok to=&quot;end&quot;/&gt;</span><br><span class="line">        &lt;error to=&quot;fail&quot;/&gt;</span><br><span class="line">    &lt;/action&gt;</span><br><span class="line"></span><br><span class="line">    &lt;kill name=&quot;fail&quot;&gt;</span><br><span class="line">        &lt;message&gt;Hive2 (Beeline) action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;</span><br><span class="line">    &lt;/kill&gt;</span><br><span class="line">    &lt;end name=&quot;end&quot;/&gt;</span><br><span class="line">&lt;/workflow-app&gt;</span><br></pre></td></tr></table></figure>

<p>编辑hivesql文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim script.q</span><br><span class="line">DROP TABLE IF EXISTS test;</span><br><span class="line">CREATE EXTERNAL TABLE test (a INT) STORED AS TEXTFILE LOCATION &apos;$&#123;INPUT&#125;&apos;;</span><br><span class="line">insert into test values(10);</span><br><span class="line">insert into test values(20);</span><br><span class="line">insert into test values(30);</span><br></pre></td></tr></table></figure>

<p>上传调度任务到hdfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works</span><br><span class="line">hdfs dfs -put hive2/ /user/root/oozie_works/</span><br></pre></td></tr></table></figure>

<p>执行调度任务</p>
<p>oozie调度hive脚本</p>
<ul>
<li><p>首先必须保证hiveserve2服务是启动正常的，如果配置metastore服务，要首先启动metastore</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在hive目录下执行</span><br><span class="line">nohup hive --service metastore &amp;</span><br><span class="line">nohup hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>

<p>测试 hive2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">beeline</span><br><span class="line">! connect jdbc:hive2//node01:10000    看能否进去  前提node01上的hiveserve2服务是启动正常的</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/hive2/job.properties  -run</span><br></pre></td></tr></table></figure>

<h2 id="3-oozie-调度-MapReduce"><a href="#3-oozie-调度-MapReduce" class="headerlink" title="3 oozie 调度 MapReduce"></a>3 oozie 调度 MapReduce</h2><p>1准备模板</p>
<p>准备mr程序的待处理数据。用hadoop自带的MR程序来运行wordcount。</p>
<p>准备数据上传到HDFS的/oozie/input路径下去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /oozie/input</span><br><span class="line">hdfs dfs -put wordcount.txt /oozie/input</span><br></pre></td></tr></table></figure>

<p>拷贝MR的任务模板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">cp -ra examples/apps/map-reduce/ oozie_works/</span><br></pre></td></tr></table></figure>

<p>删掉MR任务模板lib目录下自带的jar包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/map-reduce/lib</span><br><span class="line">rm -rf oozie-examples-4.1.0-cdh5.14.0.jar</span><br></pre></td></tr></table></figure>

<p>拷贝官方自带mr程序jar包到对应目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar </span><br><span class="line"> /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/map-reduce/lib/</span><br></pre></td></tr></table></figure>

<p>2 修改模板</p>
<p>修改job.properties</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/map-reduce</span><br><span class="line">vim job.properties</span><br><span class="line"></span><br><span class="line">nameNode=hdfs://node01:8020</span><br><span class="line">jobTracker=node01:8032</span><br><span class="line">queueName=default</span><br><span class="line">examplesRoot=oozie_works</span><br><span class="line">oozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/map-reduce/workflow.xml</span><br><span class="line">outputDir=/oozie/output</span><br><span class="line">inputdir=/oozie/input</span><br></pre></td></tr></table></figure>

<p>修改workflow.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/map-reduce</span><br><span class="line">vim workflow.xml</span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.5&quot; name=&quot;map-reduce-wf&quot;&gt;</span><br><span class="line">    &lt;start to=&quot;mr-node&quot;/&gt;</span><br><span class="line">    &lt;action name=&quot;mr-node&quot;&gt;</span><br><span class="line">        &lt;map-reduce&gt;</span><br><span class="line">            &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</span><br><span class="line">            &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</span><br><span class="line">            &lt;prepare&gt;</span><br><span class="line">                &lt;delete path=&quot;$&#123;nameNode&#125;/$&#123;outputDir&#125;&quot;/&gt;</span><br><span class="line">            &lt;/prepare&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">				&lt;!--  </span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.mapper.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.reducer.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.map.tasks&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.input.dir&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.output.dir&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">				--&gt;</span><br><span class="line">				</span><br><span class="line">				   &lt;!-- 开启使用新的API来进行配置 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.mapper.new-api&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.reducer.new-api&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定MR的输出key的类型 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapreduce.job.output.key.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.hadoop.io.Text&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定MR的输出的value的类型--&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapreduce.job.output.value.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.hadoop.io.IntWritable&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定输入路径 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.input.dir&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;nameNode&#125;/$&#123;inputdir&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定输出路径 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.output.dir&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;nameNode&#125;/$&#123;outputDir&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定执行的map类 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapreduce.job.map.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.hadoop.examples.WordCount$TokenizerMapper&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定执行的reduce类 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapreduce.job.reduce.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.hadoop.examples.WordCount$IntSumReducer&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">				&lt;!--  配置map task的个数 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.map.tasks&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/map-reduce&gt;</span><br><span class="line">        &lt;ok to=&quot;end&quot;/&gt;</span><br><span class="line">        &lt;error to=&quot;fail&quot;/&gt;</span><br><span class="line">    &lt;/action&gt;</span><br><span class="line">    &lt;kill name=&quot;fail&quot;&gt;</span><br><span class="line">        &lt;message&gt;Map/Reduce failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;</span><br><span class="line">    &lt;/kill&gt;</span><br><span class="line">    &lt;end name=&quot;end&quot;/&gt;</span><br><span class="line">&lt;/workflow-app&gt;</span><br></pre></td></tr></table></figure>

<p>3 上传调度任务到hdfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works hdfs dfs -put map-reduce/ /user/root/oozie_works/</span><br></pre></td></tr></table></figure>

<p>4 执行任务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/map-reduce/job.properties -run</span><br></pre></td></tr></table></figure>

<p>注意:</p>
<p>需要在workflow.xml中开启使用新版的 api hadoop2.x</p>
<h2 id="4-oozie-任务串联"><a href="#4-oozie-任务串联" class="headerlink" title="4 oozie 任务串联"></a>4 oozie 任务串联</h2><p>在实际工作当中，肯定会存在多个任务需要执行，并且存在上一个任务的输出结果作为下一个任务的输入数据这样的情况，所以我们需要在workflow.xml配置文件当中配置多个action，实现多个任务之间的相互依赖关系。</p>
<p>需求：首先执行一个shell脚本，执行完了之后再执行一个MR的程序，最后再执行一个hive的程序。</p>
<p>1 准备工作目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works</span><br><span class="line">mkdir -p sereval-actions</span><br></pre></td></tr></table></figure>

<p>2 准备调度文件</p>
<p>将之前的hive，shell， MR的执行，进行串联成到一个workflow当中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works</span><br><span class="line">cp hive2/script.q sereval-actions/</span><br><span class="line">cp shell/hello.sh sereval-actions/</span><br><span class="line">cp -ra map-reduce/lib sereval-actions/</span><br></pre></td></tr></table></figure>

<p>3修改配置模板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/sereval-actions</span><br><span class="line">vim workflow.xml</span><br><span class="line"></span><br><span class="line">&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.4&quot; name=&quot;shell-wf&quot;&gt;</span><br><span class="line">&lt;start to=&quot;shell-node&quot;/&gt;</span><br><span class="line">&lt;action name=&quot;shell-node&quot;&gt;</span><br><span class="line">    &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt;</span><br><span class="line">        &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</span><br><span class="line">        &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">            &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</span><br><span class="line">            &lt;/property&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">        &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt;</span><br><span class="line">        &lt;!-- &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt; --&gt;</span><br><span class="line">        &lt;file&gt;/user/root/oozie_works/sereval-actions/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt;</span><br><span class="line"></span><br><span class="line">        &lt;capture-output/&gt;</span><br><span class="line">    &lt;/shell&gt;</span><br><span class="line">    &lt;ok to=&quot;mr-node&quot;/&gt;</span><br><span class="line">    &lt;error to=&quot;mr-node&quot;/&gt;</span><br><span class="line">&lt;/action&gt;</span><br><span class="line"></span><br><span class="line">&lt;action name=&quot;mr-node&quot;&gt;</span><br><span class="line">        &lt;map-reduce&gt;</span><br><span class="line">            &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</span><br><span class="line">            &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</span><br><span class="line">            &lt;prepare&gt;</span><br><span class="line">                &lt;delete path=&quot;$&#123;nameNode&#125;/$&#123;outputDir&#125;&quot;/&gt;</span><br><span class="line">            &lt;/prepare&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">				&lt;!--  </span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.mapper.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.oozie.example.SampleMapper&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.reducer.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.oozie.example.SampleReducer&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.map.tasks&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.input.dir&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/text&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.output.dir&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/$&#123;outputDir&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">				--&gt;</span><br><span class="line">				</span><br><span class="line">				   &lt;!-- 开启使用新的API来进行配置 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.mapper.new-api&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.reducer.new-api&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定MR的输出key的类型 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapreduce.job.output.key.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.hadoop.io.Text&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定MR的输出的value的类型--&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapreduce.job.output.value.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.hadoop.io.IntWritable&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定输入路径 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.input.dir&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;nameNode&#125;/$&#123;inputdir&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定输出路径 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.output.dir&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;nameNode&#125;/$&#123;outputDir&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定执行的map类 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapreduce.job.map.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.hadoop.examples.WordCount$TokenizerMapper&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">                &lt;!-- 指定执行的reduce类 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapreduce.job.reduce.class&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;org.apache.hadoop.examples.WordCount$IntSumReducer&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">				&lt;!--  配置map task的个数 --&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.map.tasks&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/map-reduce&gt;</span><br><span class="line">        &lt;ok to=&quot;hive2-node&quot;/&gt;</span><br><span class="line">        &lt;error to=&quot;fail&quot;/&gt;</span><br><span class="line">    &lt;/action&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> &lt;action name=&quot;hive2-node&quot;&gt;</span><br><span class="line">        &lt;hive2 xmlns=&quot;uri:oozie:hive2-action:0.1&quot;&gt;</span><br><span class="line">            &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</span><br><span class="line">            &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</span><br><span class="line">            &lt;prepare&gt;</span><br><span class="line">                &lt;delete path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/hive2&quot;/&gt;</span><br><span class="line">                &lt;mkdir path=&quot;$&#123;nameNode&#125;/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data&quot;/&gt;</span><br><span class="line">            &lt;/prepare&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">            &lt;jdbc-url&gt;$&#123;jdbcURL&#125;&lt;/jdbc-url&gt;</span><br><span class="line">            &lt;script&gt;script.q&lt;/script&gt;</span><br><span class="line">            &lt;param&gt;INPUT=/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/input-data/table&lt;/param&gt;</span><br><span class="line">            &lt;param&gt;OUTPUT=/user/$&#123;wf:user()&#125;/$&#123;examplesRoot&#125;/output-data/hive2&lt;/param&gt;</span><br><span class="line">        &lt;/hive2&gt;</span><br><span class="line">        &lt;ok to=&quot;end&quot;/&gt;</span><br><span class="line">        &lt;error to=&quot;fail&quot;/&gt;</span><br><span class="line">    &lt;/action&gt;</span><br><span class="line">&lt;decision name=&quot;check-output&quot;&gt;</span><br><span class="line">    &lt;switch&gt;</span><br><span class="line">        &lt;case to=&quot;end&quot;&gt;</span><br><span class="line">            $&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;] eq &apos;Hello Oozie&apos;&#125;</span><br><span class="line">        &lt;/case&gt;</span><br><span class="line">        &lt;default to=&quot;fail-output&quot;/&gt;</span><br><span class="line">    &lt;/switch&gt;</span><br><span class="line">&lt;/decision&gt;</span><br><span class="line">&lt;kill name=&quot;fail&quot;&gt;</span><br><span class="line">    &lt;message&gt;Shell action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]&lt;/message&gt;</span><br><span class="line">&lt;/kill&gt;</span><br><span class="line">&lt;kill name=&quot;fail-output&quot;&gt;</span><br><span class="line">    &lt;message&gt;Incorrect output, expected [Hello Oozie] but was [$&#123;wf:actionData(&apos;shell-node&apos;)[&apos;my_output&apos;]&#125;]&lt;/message&gt;</span><br><span class="line">&lt;/kill&gt;</span><br><span class="line">&lt;end name=&quot;end&quot;/&gt;</span><br><span class="line">&lt;/workflow-app&gt;</span><br></pre></td></tr></table></figure>

<p>job.properties配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">nameNode=hdfs://node01:8020</span><br><span class="line">jobTracker=node01:8032</span><br><span class="line">queueName=default</span><br><span class="line">examplesRoot=oozie_works</span><br><span class="line">EXEC=hello.sh</span><br><span class="line">outputDir=/oozie/output</span><br><span class="line">inputdir=/oozie/input</span><br><span class="line">jdbcURL=jdbc:hive2://node01:10000/default</span><br><span class="line">oozie.use.system.libpath=true</span><br><span class="line"># 配置我们文件上传到hdfs的保存路径 实际上就是在hdfs 的/user/root/oozie_works/sereval-actions这个路径下</span><br><span class="line">oozie.wf.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/sereval-actions/workflow.xml</span><br></pre></td></tr></table></figure>

<p>4 上传任务调度到hdfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/</span><br><span class="line">hdfs dfs -put sereval-actions/ /user/root/oozie_works/</span><br></pre></td></tr></table></figure>

<p>5执行任务调度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/</span><br><span class="line">bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/sereval-actions/job.properties -run</span><br></pre></td></tr></table></figure>

<p>通过action节点 成功失败控制执行的流程</p>
<p>如果上一个action成功 跳转到下一个action 这样就可以变成首尾相连的串联任务</p>
<h2 id="5-定时调度"><a href="#5-定时调度" class="headerlink" title="5 定时调度"></a>5 定时调度</h2><p>在oozie当中，主要是通过Coordinator 来实现任务的定时调度， Coordinator 模块主要通过xml来进行配置即可。</p>
<p>Coordinator 的调度主要可以有两种实现方式</p>
<p>第一种：基于时间的定时任务调度：</p>
<p>oozie基于时间的调度主要需要指定三个参数，第一个起始时间，第二个结束时间，第三个调度频率；</p>
<p>第二种：基于数据的任务调度， 这种是基于数据的调度，只要在有了数据才会触发调度任务。</p>
<p>###1 准备配置模板</p>
<p>第一步：拷贝定时任务的调度模板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">cp -r examples/apps/cron oozie_works/cron-job</span><br></pre></td></tr></table></figure>

<p>第二步：拷贝我们的hello.sh脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works</span><br><span class="line">cp shell/hello.sh  cron-job/</span><br></pre></td></tr></table></figure>

<h3 id="2修改配制模板"><a href="#2修改配制模板" class="headerlink" title="2修改配制模板"></a>2修改配制模板</h3><p>修改job.properties</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works/cron-job</span><br><span class="line">vim job.properties</span><br><span class="line"></span><br><span class="line">nameNode=hdfs://node01:8020</span><br><span class="line">jobTracker=node01:8032</span><br><span class="line">queueName=default</span><br><span class="line">examplesRoot=oozie_works</span><br><span class="line"></span><br><span class="line">oozie.coord.application.path=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron-job/coordinator.xml</span><br><span class="line">#start：必须设置为未来时间，否则任务失败</span><br><span class="line">start=2019-05-22T19:20+0800</span><br><span class="line">end=2019-08-22T19:20+0800</span><br><span class="line">EXEC=hello.sh</span><br><span class="line">workflowAppUri=$&#123;nameNode&#125;/user/$&#123;user.name&#125;/$&#123;examplesRoot&#125;/cron-job/workflow.xml</span><br></pre></td></tr></table></figure>

<p>修改coordinator.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">vim coordinator.xml</span><br><span class="line"></span><br><span class="line">&lt;!--</span><br><span class="line">	oozie的frequency 可以支持很多表达式，其中可以通过定时每分，或者每小时，或者每天，或者每月进行执行，也支持可以通过与linux的crontab表达式类似的写法来进行定时任务的执行</span><br><span class="line">	例如frequency 也可以写成以下方式</span><br><span class="line">	frequency=&quot;10 9 * * *&quot;  每天上午的09:10:00开始执行任务</span><br><span class="line">	frequency=&quot;0 1 * * *&quot;  每天凌晨的01:00开始执行任务</span><br><span class="line"> --&gt;</span><br><span class="line">&lt;coordinator-app name=&quot;cron-job&quot; frequency=&quot;$&#123;coord:minutes(1)&#125;&quot; start=&quot;$&#123;start&#125;&quot; end=&quot;$&#123;end&#125;&quot; timezone=&quot;GMT+0800&quot;</span><br><span class="line">                 xmlns=&quot;uri:oozie:coordinator:0.4&quot;&gt;</span><br><span class="line">        &lt;action&gt;</span><br><span class="line">        &lt;workflow&gt;</span><br><span class="line">            &lt;app-path&gt;$&#123;workflowAppUri&#125;&lt;/app-path&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;jobTracker&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;jobTracker&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;nameNode&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;nameNode&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">                &lt;property&gt;</span><br><span class="line">                    &lt;name&gt;queueName&lt;/name&gt;</span><br><span class="line">                    &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</span><br><span class="line">                &lt;/property&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/workflow&gt;</span><br><span class="line">    &lt;/action&gt;</span><br><span class="line">&lt;/coordinator-app&gt;</span><br></pre></td></tr></table></figure>

<p>修改workflow.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">vim workflow.xml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	&lt;workflow-app xmlns=&quot;uri:oozie:workflow:0.5&quot; name=&quot;one-op-wf&quot;&gt;</span><br><span class="line">    &lt;start to=&quot;action1&quot;/&gt;</span><br><span class="line">    &lt;action name=&quot;action1&quot;&gt;</span><br><span class="line">    &lt;shell xmlns=&quot;uri:oozie:shell-action:0.2&quot;&gt;</span><br><span class="line">        &lt;job-tracker&gt;$&#123;jobTracker&#125;&lt;/job-tracker&gt;</span><br><span class="line">        &lt;name-node&gt;$&#123;nameNode&#125;&lt;/name-node&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">            &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapred.job.queue.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;$&#123;queueName&#125;&lt;/value&gt;</span><br><span class="line">            &lt;/property&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">        &lt;exec&gt;$&#123;EXEC&#125;&lt;/exec&gt;</span><br><span class="line">        &lt;!-- &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt; --&gt;</span><br><span class="line">        &lt;file&gt;/user/root/oozie_works/cron-job/$&#123;EXEC&#125;#$&#123;EXEC&#125;&lt;/file&gt;</span><br><span class="line"></span><br><span class="line">        &lt;capture-output/&gt;</span><br><span class="line">    &lt;/shell&gt;</span><br><span class="line">    &lt;ok to=&quot;end&quot;/&gt;</span><br><span class="line">    &lt;error to=&quot;end&quot;/&gt;</span><br><span class="line">&lt;/action&gt;</span><br><span class="line">    &lt;end name=&quot;end&quot;/&gt;</span><br><span class="line">&lt;/workflow-app&gt;</span><br></pre></td></tr></table></figure>

<h3 id="3上传调度任务到hdfs"><a href="#3上传调度任务到hdfs" class="headerlink" title="3上传调度任务到hdfs"></a>3上传调度任务到hdfs</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0/oozie_works</span><br><span class="line">hdfs dfs -put cron-job/ /user/root/oozie_works/</span><br></pre></td></tr></table></figure>

<h3 id="4-执行调度"><a href="#4-执行调度" class="headerlink" title="4 执行调度"></a>4 执行调度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">bin/oozie job -oozie http://node01:11000/oozie -config oozie_works/cron-job/job.properties –run</span><br></pre></td></tr></table></figure>

<ul>
<li><p>oozie基于时间的定时</p>
<p>主要是需要coordinator来配合workflow进行周期性的触发执行</p>
<p>需要注意时间的格式问题 时区的问题</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Oozie/" data-id="cjz2c0w95000magu5dkpdqxor" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Hue" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Hue/" class="article-date">
  <time datetime="2019-08-08T03:40:37.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Hue/">Hue</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Hue"><a href="#Hue" class="headerlink" title="Hue"></a>Hue</h1><h2 id="一概述"><a href="#一概述" class="headerlink" title="一概述"></a>一概述</h2><p>HUE=<strong>Hadoop User Experience</strong></p>
<p>Hue是一个开源的Apache Hadoop UI系统，由Cloudera Desktop演化而来，最后Cloudera公司将其贡献给Apache基金会的Hadoop社区，它是基于Python Web框架Django实现的。</p>
<p>用于与其他各个框架做整合,提供一个web 界面可以供我们去操作其他的大数据框架.</p>
<p>其实就是一个与其他大数据框架做整合的工具,他本身并不提供任何功能,这些功能由他整合的框架完成</p>
<h2 id="二架构"><a href="#二架构" class="headerlink" title="二架构"></a>二架构</h2><p>第一个UI界面：主要是提供我们web界面供我们使用的<br>第二个hue server：就是一个tomcat的服务<br>第三个hue DB: hue的数据库，主要用于保存一起我们提交的任务</p>
<p>核心功能:</p>
<p>· SQL编辑器，支持Hive, Impala, MySQL, Oracle, PostgreSQL, SparkSQL, Solr SQL, Phoenix…</p>
<p>· 搜索引擎Solr的各种图表</p>
<p>· Spark和Hadoop的友好界面支持</p>
<p>· 支持调度系统Apache Oozie，可进行workflow的编辑、查看</p>
<p>HUE提供的这些功能相比Hadoop生态各组件提供的界面更加友好，但是一些需要debug的场景可能还是需要使用原生系统才能更加深入的找到错误的原因。</p>
<p>HUE中查看Oozie workflow时，也可以很方便的看到整个workflow的DAG图，不过在最新版本中已经将DAG图去掉了，只能看到workflow中的action列表和他们之间的跳转关系，想要看DAG图的仍然可以使用oozie原生的界面系统查看。</p>
<p>支持的框架:</p>
<p>1，访问HDFS和文件浏览</p>
<p>2，通过web调试和开发hive以及数据结果展示</p>
<p>3，查询solr和结果展示，报表生成</p>
<p>4，通过web调试和开发impala交互式SQL Query</p>
<p>5，spark调试和开发</p>
<p>7，oozie任务的开发，监控，和工作流协调调度</p>
<p>8，Hbase数据查询和修改，数据展示</p>
<p>9，Hive的元数据（metastore）查询</p>
<p>10，MapReduce任务进度查看，日志追踪</p>
<p>11，创建和提交MapReduce，Streaming，Java job任务</p>
<p>12，Sqoop2的开发和调试</p>
<p>13，Zookeeper的浏览和编辑</p>
<p>14，数据库（，，，）的查询和展示</p>
<p>Hue是一个友好的界面集成框架，可以集成我们各种学习过的以及将要学习的框架，一个界面就可以做到查看以及执行所有的框架.</p>
<h2 id="三-hue-的安装"><a href="#三-hue-的安装" class="headerlink" title="三 hue 的安装"></a>三 hue 的安装</h2><p>安装方式:</p>
<p>1 .RPM</p>
<p>2.tar.gz 安装</p>
<p>3.clouder mananger 安装</p>
<p>这里使用tar.gz安装</p>
<p>1 下载解压</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hue-3.9.0-cdh5.14.0.tar.gz -C ../servers/</span><br></pre></td></tr></table></figure>

<p>2 编译安装启动</p>
<p>2.1 linux安装依赖包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install ant asciidoc cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-plain gcc gcc-c++ krb5-devel libffi-devel libxml2-devel libxslt-devel make  mysql mysql-devel openldap-devel python-devel sqlite-devel gmp-devel</span><br></pre></td></tr></table></figure>

<p>2.2 配值 hue</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /hue/desktop/conf</span><br><span class="line">vim hue.ini</span><br></pre></td></tr></table></figure>

<p>通用配置: 在标签 [desktop]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">secret_key=jFE93j;2[290-eiw.KEiwN2s3[&apos;d;/.q[eIW^y#e=+Iei*@Mn&lt;qW5o</span><br><span class="line">http_host=node03.hadoop.com</span><br><span class="line">is_hue_4=true</span><br><span class="line">time_zone=Asia/Shanghai</span><br><span class="line">server_user=root</span><br><span class="line">server_group=root</span><br><span class="line">default_user=root</span><br><span class="line">default_hdfs_superuser=root</span><br></pre></td></tr></table></figure>

<p>#配置使用mysql作为hue的存储数据库,大概在hue.ini的587行左右 [database]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">engine=mysql</span><br><span class="line">host=node03.hadoop.com</span><br><span class="line">port=3306</span><br><span class="line">user=root</span><br><span class="line">password=123456</span><br><span class="line">name=hue</span><br></pre></td></tr></table></figure>

<p>2.3 配置mysql数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database hue default character set utf8 default collate utf8_general_ci;</span><br></pre></td></tr></table></figure>

<p><strong>注意实际工作中需要为hue 这个数据库创建对应的用户并分配权限,此处不用创建</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grant all on hue.* to &apos;hue&apos;@&apos;%&apos; identified by &apos;hue&apos;;</span><br></pre></td></tr></table></figure>

<p>2.4 准备编译</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /hue</span><br><span class="line">make apps</span><br></pre></td></tr></table></figure>

<p>2.5 linux 添加普通用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">useradd hue</span><br><span class="line">passwd hue</span><br></pre></td></tr></table></figure>

<p>2.6 启动hue</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /hue</span><br><span class="line">bulid/env/bin/supervisor</span><br></pre></td></tr></table></figure>

<p>2.7 页面访问</p>
<p><a href="http://node03:8888/" target="_blank" rel="noopener">http://node03:8888</a></p>
<p>第一次访问的时候，需要设置管理员用户和密码</p>
<p>我们这里的管理员的用户名与密码尽量保持与我们安装hadoop的用户名和密码一致，</p>
<p>我们安装hadoop的用户名与密码分别是root ***</p>
<p>初次登录使用root用户，密码为****</p>
<h2 id="四-hue与其他框架的集成"><a href="#四-hue与其他框架的集成" class="headerlink" title="四 hue与其他框架的集成"></a>四 hue与其他框架的集成</h2><h2 id="1hue-与hadoop的集成"><a href="#1hue-与hadoop的集成" class="headerlink" title="1hue 与hadoop的集成"></a>1hue 与hadoop的集成</h2><h3 id="1-1-hue-与hdfs的集成"><a href="#1-1-hue-与hdfs的集成" class="headerlink" title="1.1 hue 与hdfs的集成"></a>1.1 hue 与hdfs的集成</h3><p>注意修改完HDFS相关配置后，需要把配置scp给集群中每台机器，重启hdfs集群。</p>
<p>修改 hadoop 中的core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!--允许通过httpfs方式访问hdfs的主机名 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!—允许通过httpfs方式访问hdfs的用户组 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">&lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>修改hadoop 中的hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	  &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">	  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>修改hue.ini文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hue-3.9.0-cdh5.14.0/desktop/conf</span><br><span class="line">vim hue.ini</span><br><span class="line"></span><br><span class="line">[[hdfs_clusters]]</span><br><span class="line">    [[[default]]]</span><br><span class="line">fs_defaultfs=hdfs://node-1:9000</span><br><span class="line">webhdfs_url=http://node-1:50070/webhdfs/v1</span><br><span class="line">hadoop_hdfs_home= /export/servers/hadoop-2.7.5</span><br><span class="line">hadoop_bin=/export/servers/hadoop-2.7.5/bin</span><br><span class="line">hadoop_conf_dir=/export/servers/hadoop-2.7.5/etc/hadoop</span><br></pre></td></tr></table></figure>

<p>重启hdfs与hue</p>
<h3 id="2-与yarn集成"><a href="#2-与yarn集成" class="headerlink" title="2 与yarn集成"></a>2 与yarn集成</h3><p>修改hue.ini 文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[yarn_clusters]]</span><br><span class="line">    [[[default]]]</span><br><span class="line">      resourcemanager_host=node-1</span><br><span class="line">      resourcemanager_port=8032</span><br><span class="line">      submit_to=True</span><br><span class="line">      resourcemanager_api_url=http://node-1:8088</span><br><span class="line">      history_server_api_url=http://node-1:19888</span><br></pre></td></tr></table></figure>

<p>开启yarn日志聚集服务</p>
<p>修改Hadoop中的 yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;  ##是否启用日志聚集功能。</span><br><span class="line">&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  ##设置日志保留时间，单位是秒。</span><br><span class="line">&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">&lt;value&gt;106800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>重启yarn 与hue</p>
<h2 id="2-hue集成hive"><a href="#2-hue集成hive" class="headerlink" title="2 hue集成hive"></a>2 hue集成hive</h2><p>如果需要配置hue与hive的集成，我们需要启动hive的metastore服务以及hiveserver2服务（impala需要hive的metastore服务，hue需要hvie的hiveserver2服务）。</p>
<p>1 修改hue.ini文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[beeswax]</span><br><span class="line">  hive_server_host=node-1</span><br><span class="line">  hive_server_port=10000</span><br><span class="line">  hive_conf_dir=/export/servers/hive/conf</span><br><span class="line">  server_conn_timeout=120</span><br><span class="line">  auth_username=root</span><br><span class="line">  auth_password=123456</span><br><span class="line"></span><br><span class="line">[metastore]</span><br><span class="line">  #允许使用hive创建数据库表等操作</span><br><span class="line">  enable_new_create_table=true</span><br></pre></td></tr></table></figure>

<p>2 启动hIive 和hue</p>
<p>去node-1机器上启动hive的metastore以及hiveserver2服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hive</span><br><span class="line">nohup bin/hive --service metastore &amp;</span><br><span class="line">nohup bin/hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>

<p>重新启动hue。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hue-3.9.0-cdh5.14.0/</span><br><span class="line">build/env/bin/supervisor</span><br></pre></td></tr></table></figure>

<h2 id="3-hue-集成MySQL"><a href="#3-hue-集成MySQL" class="headerlink" title="3 hue 集成MySQL"></a>3 hue 集成MySQL</h2><p>1 修改hue.ini文件</p>
<p>需要把mysql的注释给去掉。 大概位于1546行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[[mysql]]]</span><br><span class="line">      nice_name=&quot;My SQL DB&quot;</span><br><span class="line">      engine=mysql</span><br><span class="line">      host=node-1</span><br><span class="line">      port=3306</span><br><span class="line">      user=root</span><br><span class="line">      password=hadoop</span><br></pre></td></tr></table></figure>

<p>2重启hue</p>
<h2 id="4-hue集成Oozie"><a href="#4-hue集成Oozie" class="headerlink" title="4 hue集成Oozie"></a>4 hue集成Oozie</h2><p>1修改hue.ini 文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[liboozie]</span><br><span class="line">  # The URL where the Oozie service runs on. This is required in order for</span><br><span class="line">  # users to submit jobs. Empty value disables the config check.</span><br><span class="line">  oozie_url=http://node-1:11000/oozie</span><br><span class="line"></span><br><span class="line">  # Requires FQDN in oozie_url if enabled</span><br><span class="line">  ## security_enabled=false</span><br><span class="line"></span><br><span class="line">  # Location on HDFS where the workflows/coordinator are deployed when submitted.</span><br><span class="line">  remote_deployement_dir=/user/root/oozie_works</span><br><span class="line">[oozie]</span><br><span class="line">  # Location on local FS where the examples are stored.</span><br><span class="line">  # local_data_dir=/export/servers/oozie-4.1.0-cdh5.14.0/examples/apps</span><br><span class="line"></span><br><span class="line">  # Location on local FS where the data for the examples is stored.</span><br><span class="line">  # sample_data_dir=/export/servers/oozie-4.1.0-cdh5.14.0/examples/input-data</span><br><span class="line"></span><br><span class="line">  # Location on HDFS where the oozie examples and workflows are stored.</span><br><span class="line">  # Parameters are $TIME and $USER, e.g. /user/$USER/hue/workspaces/workflow-$TIME</span><br><span class="line">  # remote_data_dir=/user/root/oozie_works/examples/apps</span><br><span class="line"></span><br><span class="line">  # Maximum of Oozie workflows or coodinators to retrieve in one API call.</span><br><span class="line">  oozie_jobs_count=100</span><br><span class="line"></span><br><span class="line">  # Use Cron format for defining the frequency of a Coordinator instead of the old frequency number/unit.</span><br><span class="line">  enable_cron_scheduling=true</span><br><span class="line"></span><br><span class="line">  # Flag to enable the saved Editor queries to be dragged and dropped into a workflow.</span><br><span class="line">  enable_document_action=true</span><br><span class="line"></span><br><span class="line">  # Flag to enable Oozie backend filtering instead of doing it at the page level in Javascript. Requires Oozie 4.3+.</span><br><span class="line">  enable_oozie_backend_filtering=true</span><br><span class="line"></span><br><span class="line">  # Flag to enable the Impala action.</span><br><span class="line">  enable_impala_action=true</span><br><span class="line">[filebrowser]</span><br><span class="line">  # Location on local filesystem where the uploaded archives are temporary stored.</span><br><span class="line">  archive_upload_tempdir=/tmp</span><br><span class="line"></span><br><span class="line">  # Show Download Button for HDFS file browser.</span><br><span class="line">  show_download_button=true</span><br><span class="line"></span><br><span class="line">  # Show Upload Button for HDFS file browser.</span><br><span class="line">  show_upload_button=true</span><br><span class="line"></span><br><span class="line">  # Flag to enable the extraction of a uploaded archive in HDFS.</span><br><span class="line">  enable_extract_uploaded_archive=true</span><br></pre></td></tr></table></figure>

<p>启动hue和oozie</p>
<p>启动hue进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hue-3.9.0-cdh5.14.0</span><br><span class="line">build/env/bin/supervisor</span><br></pre></td></tr></table></figure>

<p>启动oozie进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/oozie-4.1.0-cdh5.14.0</span><br><span class="line">bin/oozied.sh start</span><br></pre></td></tr></table></figure>

<h2 id="5-hue集成impala"><a href="#5-hue集成impala" class="headerlink" title="5 hue集成impala"></a>5 hue集成impala</h2><p>修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[impala]</span><br><span class="line">  server_host=node-3</span><br><span class="line">  server_port=21050</span><br><span class="line">  impala_conf_dir=/etc/impala/conf</span><br></pre></td></tr></table></figure>

<p>启动impala 和hue</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Hue/" data-id="cjz2c0w8i000dagu51p45dvqt" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-kudu" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/kudu/" class="article-date">
  <time datetime="2019-08-08T03:40:26.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/kudu/">kudu</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="KuDu"><a href="#KuDu" class="headerlink" title="KuDu"></a>KuDu</h1><h2 id="一概述"><a href="#一概述" class="headerlink" title="一概述"></a>一概述</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>在KUDU之前，大数据主要以两种方式存储；</p>
<p>（1）静态数据：</p>
<p>以 HDFS 引擎作为存储引擎，适用于高吞吐量的离线大数据分析场景。</p>
<p>这类存储的局限性是数据无法进行随机的读写。</p>
<p>（2）动态数据：</p>
<p>以 HBase、Cassandra 作为存储引擎，适用于大数据随机读写场景。</p>
<p>局限性是批量读取吞吐量远不如 HDFS，不适用于批量数据分析的场景。</p>
<p>从上面分析可知，这两种数据在存储方式上完全不同，进而导致使用场景完全不同，但在真实的场景中，边界可能没有那么清晰，面对既需要随机读写，又需要批量分析的大数据场景，该如何选择呢？</p>
<p>这个场景中，单种存储引擎无法满足业务需求，我们需要通过多种大数据工具组合来满足这一需求</p>
<p><a href="https://manzhong.github.io/images/kudu/t.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/t.png" alt="img"></a></p>
<p>如上图所示，数据实时写入 HBase，实时的数据更新也在 HBase 完成，为了应对 OLAP 需求，我们定时将 HBase 数据写成静态的文件（如：Parquet）导入到 OLAP 引擎（如：Impala、hive）。这一架构能满足既需要随机读写，又可以支持 OLAP 分析的场景，但他有如下缺点：</p>
<p>(1)<strong>架构复杂</strong>。从架构上看，数据在HBase、消息队列、HDFS 间流转，涉及环节太多，运维成本很高。并且每个环节需要保证高可用，都需要维护多个副本，存储空间也有一定的浪费。最后数据在多个系统上，对数据安全策略、监控等都提出了挑战。</p>
<p>(2)<strong>时效性低</strong>。数据从HBase导出成静态文件是周期性的，一般这个周期是一天（或一小时），在时效性上不是很高。</p>
<p>(3)<strong>难以应对后续的更新</strong>。真实场景中，总会有数据是延迟到达的。如果这些数据之前已经从HBase导出到HDFS，新到的变更数据就难以处理了，一个方案是把原有数据应用上新的变更后重写一遍，但这代价又很高。</p>
<p>为了解决上述架构的这些问题，KUDU应运而生。<strong>KUDU</strong>的定位是Fast Analytics on Fast Data，<strong>是一个既支持随机读写、又支持 OLAP 分析的大数据存储引擎</strong>。</p>
<p>KUDU 是一个折中的产品，在 HDFS 和 HBase 这两个偏科生中平衡了随机读写和批量分析的性能。从 KUDU 的诞生可以说明一个观点：底层的技术发展很多时候都是上层的业务推动的，脱离业务的技术很可能是空中楼阁。</p>
<p>kudu是什么</p>
<ul>
<li>是一个大数据存储引擎 用于大数据的存储，结合其他软件开展数据分析。</li>
<li>汲取了hdfs中高吞吐数据的能力和hbase中高随机读写数据的能力</li>
<li>既满足有传统OLAP分析 又满足于随机读写访问数据</li>
<li>kudu来自于cloudera 后来贡献给了apache</li>
</ul>
<p>kudu应用场景</p>
<p>适用于那些既有随机访问，也有批量数据扫描的复合场景</p>
<p>高计算量的场景</p>
<p>使用了高性能的存储设备，包括使用更多的内存</p>
<p>支持数据更新，避免数据反复迁移</p>
<p>支持跨地域的实时数据备份和查询</p>
<h2 id="二架构"><a href="#二架构" class="headerlink" title="二架构"></a>二架构</h2><ul>
<li>kudu集群是主从架构<ul>
<li>主角色 master ：管理集群 管理元数据</li>
<li>从角色 tablet server：负责最终数据的存储 对外提供数据读写能力 里面存储的都是一个个tablet</li>
</ul>
</li>
<li>kudu tablet<ul>
<li>是kudu表中的数据水平分区 一个表可以划分成为多个tablet(类似于hbase region)</li>
<li>tablet中主键是不重复连续的 所有tablet加起来就是一个table的所有数据</li>
<li>tablet在存储的时候 会进行冗余存放 设置多个副本</li>
<li>在一个tablet所有冗余中 任意时刻 一个是leader 其他的冗余都是follower</li>
</ul>
</li>
</ul>
<p>与HDFS和HBase相似，Kudu使用单个的Master节点，用来管理集群的元数据，并且使用任意数量的Tablet Server（类似HBase中的RegionServer角色）节点用来存储实际数据。可以部署多个Master节点来提高容错性。</p>
<p><a href="https://manzhong.github.io/images/kudu/t2.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/t2.png" alt="img"></a></p>
<h2 id="1．-Table"><a href="#1．-Table" class="headerlink" title="1． Table"></a>1． Table</h2><p>表（Table）是数据库中用来存储数据的对象，是有结构的数据集合。kudu中的表具有schema（纲要）和全局有序的primary key（主键）。kudu中一个table会被水平分成多个被称之为tablet的片段。</p>
<h2 id="2．-Tablet"><a href="#2．-Tablet" class="headerlink" title="2． Tablet"></a>2． Tablet</h2><p>一个 tablet 是一张 table连续的片段，tablet是kudu表的水平分区，类似于HBase的region。每个tablet存储着一定连续range的数据（key），且tablet两两间的range不会重叠。一张表的所有tablet包含了这张表的所有key空间。</p>
<p>tablet 会冗余存储。放置到多个 tablet server上，并且在任何给定的时间点，其中一个副本被认为是leader tablet,其余的被认之为follower tablet。每个tablet都可以进行数据的读请求，但只有Leader tablet负责写数据请求。</p>
<h2 id="3．-Tablet-Server"><a href="#3．-Tablet-Server" class="headerlink" title="3． Tablet Server"></a>3． Tablet Server</h2><p>tablet server集群中的小弟，负责数据存储，并提供数据读写服务</p>
<p>一个 tablet server 存储了table表的tablet，向kudu client 提供读取数据服务。对于给定的 tablet，一个tablet server 充当 leader，其他 tablet server 充当该 tablet 的 follower 副本。</p>
<p>只有 leader服务写请求，然而 leader 或 followers 为每个服务提供读请求 。一个 tablet server 可以服务多个 tablets ，并且一个 tablet 可以被多个 tablet servers 服务着。</p>
<h2 id="4．-Master-Server"><a href="#4．-Master-Server" class="headerlink" title="4． Master Server"></a>4． Master Server</h2><p>集群中的老大，负责集群管理、元数据管理等功能。</p>
<h2 id="三-kudu安装"><a href="#三-kudu安装" class="headerlink" title="三 kudu安装"></a>三 kudu安装</h2><p>1节点规划</p>
<table>
<thead>
<tr>
<th align="left"><strong>节点</strong></th>
<th align="left"><strong>kudu-master</strong></th>
<th align="left"><strong>kudu-tserver</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">node01</td>
<td align="left">是</td>
<td align="left">是</td>
</tr>
<tr>
<td align="left">node02</td>
<td align="left">是</td>
<td align="left">是</td>
</tr>
<tr>
<td align="left">node03</td>
<td align="left">是</td>
<td align="left">是</td>
</tr>
</tbody></table>
<p>本次配置node01 和node02 不配置 kudu-master</p>
<p>2本地yum源配置</p>
<p>配过了在 node03上</p>
<p>3 安装KUDU</p>
<table>
<thead>
<tr>
<th align="left"><strong>服务器</strong></th>
<th align="left"><strong>安装命令</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">node01</td>
<td align="left">yum install -y kudu kudu-tserver kudu-client0 kudu-client-devel</td>
</tr>
<tr>
<td align="left">node02</td>
<td align="left">yum install -y kudu kudu-tserver kudu-client0 kudu-client-devel</td>
</tr>
<tr>
<td align="left">node03</td>
<td align="left">yum install -y kudu kudu-master kudu-tserver kudu-client0 kudu-client-devel</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum install kudu # Kudu的基本包</span><br><span class="line">yum install kudu-master # KuduMaster </span><br><span class="line">yum install kudu-tserver # KuduTserver </span><br><span class="line">yum install kudu-client0 #Kudu C ++客户端共享库</span><br><span class="line">yum install kudu-client-devel # Kudu C ++客户端共享库 SDK</span><br></pre></td></tr></table></figure>

<p>4 kudu节点配置</p>
<p>安装完成之后。 需要在所有节点的/etc/kudu/conf目录下有两个文件：master.gflagfile和tserver.gflagfile。</p>
<h3 id="1-1．-修改master-gflagfile"><a href="#1-1．-修改master-gflagfile" class="headerlink" title="1.1． 修改master.gflagfile"></a>1.1． 修改master.gflagfile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/kudu/conf/master.gflagfile</span><br><span class="line"># Do not modify these two lines. If you wish to change these variables,</span><br><span class="line"># modify them in /etc/default/kudu-master.</span><br><span class="line">--fromenv=rpc_bind_addresses</span><br><span class="line">--fromenv=log_dir</span><br><span class="line">--fs_wal_dir=/export/servers/kudu/master</span><br><span class="line">--fs_data_dirs=/export/servers/kudu/master</span><br><span class="line">--master_addresses=node03:7051     若为集node01:7051,node02:7051,node03:7051 若为单节点 则这句注释掉</span><br><span class="line">若为单节点 且没注释掉 则启动报错</span><br></pre></td></tr></table></figure>

<h3 id="1-2-修改tserver-gflagfile"><a href="#1-2-修改tserver-gflagfile" class="headerlink" title="1.2 修改tserver.gflagfile"></a>1.2 修改tserver.gflagfile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Do not modify these two lines. If you wish to change these variables,</span><br><span class="line"># modify them in /etc/default/kudu-tserver.</span><br><span class="line">--fromenv=rpc_bind_addresses</span><br><span class="line">--fromenv=log_dir</span><br><span class="line">--fs_wal_dir=/export/servers/kudu/tserver</span><br><span class="line">--fs_data_dirs=/export/servers/kudu/tserver</span><br><span class="line">--tserver_master_addrs=node03:7051  若为集node01:7051,node02:7051,node03:7051</span><br></pre></td></tr></table></figure>

<h3 id="1-3修改-etc-default-kudu-master"><a href="#1-3修改-etc-default-kudu-master" class="headerlink" title="1.3修改 /etc/default/kudu-master"></a>1.3修改 /etc/default/kudu-master</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export FLAGS_log_dir=/var/log/kudu</span><br><span class="line">#每台机器的master地址要与主机名一致,这里是在node03上</span><br><span class="line">export FLAGS_rpc_bind_addresses=node03:7051</span><br></pre></td></tr></table></figure>

<h3 id="1-4修改-etc-default-kudu-tserver"><a href="#1-4修改-etc-default-kudu-tserver" class="headerlink" title="1.4修改 /etc/default/kudu-tserver"></a>1.4修改 /etc/default/kudu-tserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export FLAGS_log_dir=/var/log/kudu</span><br><span class="line">#每台机器的tserver地址要与主机名一致，这里是在node03上</span><br><span class="line">export FLAGS_rpc_bind_addresses=node03:7050</span><br></pre></td></tr></table></figure>

<p>kudu默认用户就是KUDU，所以需要将/export/servers/kudu权限修改成kudu：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /export/servers/kudu</span><br><span class="line">chown -R kudu:kudu /export/servers/kudu</span><br></pre></td></tr></table></figure>

<p>(如果使用的是普通的用户，那么最好配置sudo权限)/etc/sudoers文件中添加：</p>
<p><strong>kudu集群的启动与关闭</strong></p>
<p>1 ntp服务的安装</p>
<p>启动的时候要注意时间同步</p>
<p>安装ntp服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install ntp</span><br></pre></td></tr></table></figure>

<p>设置开机启动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service ntpd start </span><br><span class="line">chkconfig ntpd on</span><br></pre></td></tr></table></figure>

<p>可以在每台服务器执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/ntpd restart</span><br><span class="line">启动</span><br><span class="line">service kudu-master start</span><br><span class="line">service kudu-tserver start</span><br><span class="line">关闭</span><br><span class="line">service kudu-master stop</span><br><span class="line">service kudu-tserver stop</span><br><span class="line">kudu的web管理界面。http://master主机名:8051</span><br><span class="line"></span><br><span class="line">可以查看每个机器上master相关信息。http://node03:8051/masters    一定为8051 不为7051</span><br><span class="line"></span><br><span class="line">tserver 的web地址  http://node03:8051/tablet-servers</span><br></pre></td></tr></table></figure>

<p>安装属于事项</p>
<h3 id="1-1给普通用户授予sudo出错"><a href="#1-1给普通用户授予sudo出错" class="headerlink" title="1.1给普通用户授予sudo出错"></a>1.1给普通用户授予sudo出错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo: /etc/sudoers is world writable</span><br><span class="line">解决方式：``pkexec chmod 555 /etc/sudoers</span><br></pre></td></tr></table></figure>

<h3 id="1-2-启动kudu的时候报错"><a href="#1-2-启动kudu的时候报错" class="headerlink" title="1.2 启动kudu的时候报错"></a>1.2 启动kudu的时候报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Failed to start Kudu Master Server. Return value: 1 [FAILED]</span><br><span class="line">去日志文件中查看：</span><br><span class="line">Service unavailable: Cannot initialize clock: Errorreading clock. Clock considered</span><br><span class="line">unsynchronized</span><br><span class="line">解决：</span><br><span class="line">第一步：首先检查是否有安装ntp：如果没有安装则使用以下命令安装：</span><br><span class="line">yum -y install ntp</span><br><span class="line">第二步：设置随机启动：</span><br><span class="line">service ntpd start</span><br><span class="line">chkconfig ntpd on</span><br></pre></td></tr></table></figure>

<h3 id="1-3-启动过程中报错"><a href="#1-3-启动过程中报错" class="headerlink" title="1.3 启动过程中报错"></a>1.3 启动过程中报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Invalid argument: Unable to initialize catalog manager: Failed to initialize sys</span><br><span class="line">tables</span><br><span class="line">async: on-disk master list</span><br><span class="line">解决：</span><br><span class="line">（1）：停掉master和tserver</span><br><span class="line">（2）：删除掉之前所有的/export/servers/kudu/master/*和/export/servers/kudu/tserver/*</span><br></pre></td></tr></table></figure>

<h3 id="1-4-启动过程中报错"><a href="#1-4-启动过程中报错" class="headerlink" title="1.4 启动过程中报错"></a>1.4 启动过程中报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">error: Could not create new FS layout: unable to create file system roots: unable to</span><br><span class="line">write instance metadata: Call to mkstemp() failed on name template</span><br><span class="line">/export/servers/kudu/master/instance.kudutmp.XXXXXX: Permission denied (error 13)</span><br><span class="line">这是因为kudu默认使用kudu权限进行执行，可能遇到文件夹的权限不一致情况，更改文件夹权限即可</span><br></pre></td></tr></table></figure>

<h2 id="四-Java操作kudu"><a href="#四-Java操作kudu" class="headerlink" title="四 Java操作kudu"></a>四 Java操作kudu</h2><p>###1 创建maven工程 导入依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;  </span><br><span class="line">   &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;kudu-client&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.6.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;4.12&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure>

<h3 id="2-初始化方法"><a href="#2-初始化方法" class="headerlink" title="2 初始化方法"></a>2 初始化方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public class TestKudu &#123;</span><br><span class="line"></span><br><span class="line">    //声明全局变量 KuduClient后期通过它来操作kudu表</span><br><span class="line">    private KuduClient kuduClient;</span><br><span class="line">    //指定kuduMaster地址</span><br><span class="line">    private String kuduMaster;</span><br><span class="line">    //指定表名</span><br><span class="line">    private String tableName;</span><br><span class="line"></span><br><span class="line">    @Before</span><br><span class="line">    public void init()&#123;</span><br><span class="line">        //初始化操作</span><br><span class="line">        kuduMaster=&quot;node03:7051&quot;;</span><br><span class="line">        //指定表名</span><br><span class="line">        tableName=&quot;student&quot;;</span><br><span class="line">        KuduClient.KuduClientBuilder kuduClientBuilder = new KuduClient.KuduClientBuilder(kuduMaster);</span><br><span class="line">		//设置客户端与kudu集群socket超时时间</span><br><span class="line">        kuduClientBuilder.defaultSocketReadTimeoutMs(10000);</span><br><span class="line">        kuduClient=kuduClientBuilder.build();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-创建表"><a href="#3-创建表" class="headerlink" title="3 创建表"></a>3 创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 创建表</span><br><span class="line"> */</span><br><span class="line">@Test</span><br><span class="line">public void createTable() throws KuduException &#123;</span><br><span class="line">    //判断表是否存在，不存在就构建</span><br><span class="line">    if(!kuduClient.tableExists(tableName))&#123;</span><br><span class="line"></span><br><span class="line">        //构建创建表的schema信息-----就是表的字段和类型</span><br><span class="line">        ArrayList&lt;ColumnSchema&gt; columnSchemas = new ArrayList&lt;ColumnSchema&gt;();</span><br><span class="line">        columnSchemas.add(new ColumnSchema.ColumnSchemaBuilder(&quot;id&quot;, Type.INT32).key(true).build());</span><br><span class="line">        columnSchemas.add(new ColumnSchema.ColumnSchemaBuilder(&quot;name&quot;, Type.STRING).build());</span><br><span class="line">        columnSchemas.add(new ColumnSchema.ColumnSchemaBuilder(&quot;age&quot;, Type.INT32).build());</span><br><span class="line">        columnSchemas.add(new ColumnSchema.ColumnSchemaBuilder(&quot;sex&quot;, Type.INT32).build());</span><br><span class="line">        Schema schema = new Schema(columnSchemas);</span><br><span class="line"></span><br><span class="line">        //指定创建表的相关属性</span><br><span class="line">        CreateTableOptions options = new CreateTableOptions();</span><br><span class="line">        ArrayList&lt;String&gt; partitionList = new ArrayList&lt;String&gt;();</span><br><span class="line">        //指定kudu表的分区字段是什么</span><br><span class="line">        partitionList.add(&quot;id&quot;);    //  按照 id.hashcode % 分区数 = 分区号</span><br><span class="line">        options.addHashPartitions(partitionList,6);</span><br><span class="line"></span><br><span class="line">        kuduClient.createTable(tableName,schema,options);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-插入数据"><a href="#4-插入数据" class="headerlink" title="4 插入数据"></a>4 插入数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">  /**</span><br><span class="line">   * 向表加载数据</span><br><span class="line">   */</span><br><span class="line">  @Test</span><br><span class="line">  public void insertTable() throws KuduException &#123;</span><br><span class="line">      //向表加载数据需要一个kuduSession对象</span><br><span class="line">      KuduSession kuduSession = kuduClient.newSession();</span><br><span class="line">      //设置提交数据为自动flush</span><br><span class="line">      kuduSession.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC);</span><br><span class="line"></span><br><span class="line">      //需要使用kuduTable来构建Operation的子类实例对象  就是打开本次操作的表名</span><br><span class="line">      KuduTable kuduTable = kuduClient.openTable(tableName);</span><br><span class="line">//此处需要kudutable 来构建operation的子类实例对象  此处 为insert</span><br><span class="line">      for(int i=1;i&lt;=10;i++)&#123;</span><br><span class="line">          Insert insert = kuduTable.newInsert();</span><br><span class="line">          PartialRow row = insert.getRow();</span><br><span class="line">          row.addInt(&quot;id&quot;,i);</span><br><span class="line">          row.addString(&quot;name&quot;,&quot;zhangsan-&quot;+i);</span><br><span class="line">          row.addInt(&quot;age&quot;,20+i);</span><br><span class="line">          row.addInt(&quot;sex&quot;,i%2);</span><br><span class="line"></span><br><span class="line">          kuduSession.apply(insert);//最后实现执行数据的加载操作</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-查询数据"><a href="#5-查询数据" class="headerlink" title="5 查询数据"></a>5 查询数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 查询表的数据结果</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void queryData() throws KuduException &#123;</span><br><span class="line"></span><br><span class="line">        //构建一个查询的扫描器 在扫描器中指定 表名</span><br><span class="line">        KuduScanner.KuduScannerBuilder kuduScannerBuilder = kuduClient.newScannerBuilder(kuduClient.openTable(tableName));</span><br><span class="line">        //创建集合 用于存储扫描的字段信息</span><br><span class="line">        ArrayList&lt;String&gt; columnsList = new ArrayList&lt;String&gt;();</span><br><span class="line">        columnsList.add(&quot;id&quot;);</span><br><span class="line">        columnsList.add(&quot;name&quot;);</span><br><span class="line">        columnsList.add(&quot;age&quot;);</span><br><span class="line">        columnsList.add(&quot;sex&quot;);</span><br><span class="line">        kuduScannerBuilder.setProjectedColumnNames(columnsList);</span><br><span class="line">        // 调用build方法执行扫描,返回结果集</span><br><span class="line">        KuduScanner kuduScanner = kuduScannerBuilder.build();</span><br><span class="line">        //遍历</span><br><span class="line">        while (kuduScanner.hasMoreRows())&#123;</span><br><span class="line">            RowResultIterator rowResults = kuduScanner.nextRows();</span><br><span class="line"></span><br><span class="line">             while (rowResults.hasNext())&#123;</span><br><span class="line">                 RowResult row = rowResults.next(); //拿到的是一行数据</span><br><span class="line">                 int id = row.getInt(&quot;id&quot;);</span><br><span class="line">                 String name = row.getString(&quot;name&quot;);</span><br><span class="line">                 int age = row.getInt(&quot;age&quot;);</span><br><span class="line">                 int sex = row.getInt(&quot;sex&quot;);</span><br><span class="line"></span><br><span class="line">                 System.out.println(&quot;id=&quot;+id+&quot;  name=&quot;+name+&quot;  age=&quot;+age+&quot;  sex=&quot;+sex);</span><br><span class="line">             &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-修改数据"><a href="#6-修改数据" class="headerlink" title="6 修改数据"></a>6 修改数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 修改表的数据</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void updateData() throws KuduException &#123;</span><br><span class="line">        //修改表的数据需要一个kuduSession对象</span><br><span class="line">        KuduSession kuduSession = kuduClient.newSession();</span><br><span class="line">        //设置提交数据为自动flush</span><br><span class="line">        kuduSession.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC);</span><br><span class="line"></span><br><span class="line">        //需要使用kuduTable来构建Operation的子类实例对象</span><br><span class="line">        KuduTable kuduTable = kuduClient.openTable(tableName);</span><br><span class="line"></span><br><span class="line">        //Update update = kuduTable.newUpdate(); //如果id不存在 ,就什么也不操作</span><br><span class="line">        Upsert upsert = kuduTable.newUpsert(); //如果id存在就表示修改，不存在就新增</span><br><span class="line">        PartialRow row = upsert.getRow();</span><br><span class="line">        row.addInt(&quot;id&quot;,100);</span><br><span class="line">        row.addString(&quot;name&quot;,&quot;zhangsan-100&quot;);</span><br><span class="line">        row.addInt(&quot;age&quot;,100);</span><br><span class="line">        row.addInt(&quot;sex&quot;,0);</span><br><span class="line"></span><br><span class="line">        kuduSession.apply(upsert);//最后实现执行数据的修改操作</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="7删除数据"><a href="#7删除数据" class="headerlink" title="7删除数据"></a>7删除数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 删除数据</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void deleteData() throws KuduException &#123;</span><br><span class="line">        //删除表的数据需要一个kuduSession对象</span><br><span class="line">        KuduSession kuduSession = kuduClient.newSession();</span><br><span class="line">        kuduSession.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC);</span><br><span class="line"></span><br><span class="line">        //需要使用kuduTable来构建Operation的子类实例对象</span><br><span class="line">        KuduTable kuduTable = kuduClient.openTable(tableName);</span><br><span class="line"></span><br><span class="line">        Delete delete = kuduTable.newDelete();</span><br><span class="line">        PartialRow row = delete.getRow();</span><br><span class="line">        row.addInt(&quot;id&quot;,100);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        kuduSession.apply(delete);//最后实现执行数据的删除操作</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="8-删除表"><a href="#8-删除表" class="headerlink" title="8 删除表"></a>8 删除表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void dropTable() throws KuduException &#123;</span><br><span class="line"></span><br><span class="line">    if(kuduClient.tableExists(tableName))&#123;</span><br><span class="line">        kuduClient.deleteTable(tableName);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="9-kudu的分区方式"><a href="#9-kudu的分区方式" class="headerlink" title="9 kudu的分区方式"></a>9 kudu的分区方式</h3><p>为了提供可扩展性，Kudu 表被划分为称为 tablet 的单元，并分布在许多 tablet servers 上。行总是属于单个tablet 。将行分配给 tablet 的方法由在表创建期间设置的表的分区决定。 kudu提供了3种分区方式。</p>
<h4 id="1-范围分区Range-Partitioning"><a href="#1-范围分区Range-Partitioning" class="headerlink" title="1 范围分区Range Partitioning"></a>1 范围分区Range Partitioning</h4><p>范围分区可以根据存入数据的数据量，均衡的存储到各个机器上，防止机器出现负载不均衡现象.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * 测试分区：</span><br><span class="line">  * RangePartition</span><br><span class="line">  */</span><br><span class="line"> @Test</span><br><span class="line"> public void testRangePartition() throws KuduException &#123;</span><br><span class="line">     //设置表的schema</span><br><span class="line">     LinkedList&lt;ColumnSchema&gt; columnSchemas = new LinkedList&lt;ColumnSchema&gt;();</span><br><span class="line">     columnSchemas.add(new Column(&quot;id&quot;, Type.INT32,true));</span><br><span class="line">     columnSchemas.add(new Column(&quot;WorkId&quot;, Type.INT32,false));</span><br><span class="line">     columnSchemas.add(new Column(&quot;Name&quot;, Type.STRING,false));</span><br><span class="line">     columnSchemas.add(new Column(&quot;Gender&quot;, Type.STRING,false));</span><br><span class="line">     columnSchemas.add(new Column(&quot;Photo&quot;, Type.STRING,false));</span><br><span class="line"></span><br><span class="line">     //创建schema</span><br><span class="line">     Schema schema = new Schema(columnSchemas);</span><br><span class="line"></span><br><span class="line">     //创建表时提供的所有选项</span><br><span class="line">     CreateTableOptions tableOptions = new CreateTableOptions();</span><br><span class="line">     //设置副本数</span><br><span class="line">     //tableOptions.setNumReplicas(1);</span><br><span class="line">     //设置范围分区的规则</span><br><span class="line">     LinkedList&lt;String&gt; parcols = new LinkedList&lt;String&gt;();</span><br><span class="line">     parcols.add(&quot;id&quot;);</span><br><span class="line">     //设置按照那个字段进行range分区</span><br><span class="line">     tableOptions.setRangePartitionColumns(parcols);</span><br><span class="line"></span><br><span class="line">     /**</span><br><span class="line">      * range</span><br><span class="line">      *  0 &lt; value &lt; 10</span><br><span class="line">      * 10 &lt;= value &lt; 20</span><br><span class="line">      * 20 &lt;= value &lt; 30</span><br><span class="line">      * ........</span><br><span class="line">      * 80 &lt;= value &lt; 90</span><br><span class="line">      * */</span><br><span class="line">     int count=0;</span><br><span class="line">     for(int i =0;i&lt;10;i++)&#123;</span><br><span class="line">         //范围开始</span><br><span class="line">         PartialRow lower = schema.newPartialRow();</span><br><span class="line">         lower.addInt(&quot;id&quot;,count);</span><br><span class="line"></span><br><span class="line">         //范围结束</span><br><span class="line">         PartialRow upper = schema.newPartialRow();</span><br><span class="line">         count +=10;</span><br><span class="line">         upper.addInt(&quot;id&quot;,count);</span><br><span class="line"></span><br><span class="line">         //设置每一个分区的范围</span><br><span class="line">         tableOptions.addRangePartition(lower,upper);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     try &#123;</span><br><span class="line">         kuduClient.createTable(&quot;t-range-partition&quot;,schema,tableOptions);</span><br><span class="line">     &#125; catch (KuduException e) &#123;</span><br><span class="line">         e.printStackTrace();</span><br><span class="line">     &#125;</span><br><span class="line">      kuduClient.close();</span><br><span class="line"></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-哈希分区Hash-Partitioning-为kudu的默认分区"><a href="#2-哈希分区Hash-Partitioning-为kudu的默认分区" class="headerlink" title="2 哈希分区Hash Partitioning 为kudu的默认分区"></a>2 哈希分区Hash Partitioning 为kudu的默认分区</h4><p>哈希分区通过哈希值将行分配到许多 buckets (存储桶 )之一； 哈希分区是一种有效的策略，当不需要对表进行有序访问时。哈希分区对于在 tablet 之间随机散布这些功能是有效的，这有助于减轻热点和 tablet 大小不均匀。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 测试分区：</span><br><span class="line">     * hash分区</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void testHashPartition() throws KuduException &#123;</span><br><span class="line">        //设置表的schema</span><br><span class="line">        LinkedList&lt;ColumnSchema&gt; columnSchemas = new LinkedList&lt;ColumnSchema&gt;();</span><br><span class="line">        columnSchemas.add(new Column(&quot;id&quot;, Type.INT32,true));</span><br><span class="line">        columnSchemas.add(new Column(&quot;WorkId&quot;, Type.INT32,false));</span><br><span class="line">        columnSchemas.add(new Column(&quot;Name&quot;, Type.STRING,false));</span><br><span class="line">        columnSchemas.add(new Column(&quot;Gender&quot;, Type.STRING,false));</span><br><span class="line">        columnSchemas.add(new Column(&quot;Photo&quot;, Type.STRING,false));</span><br><span class="line"></span><br><span class="line">        //创建schema</span><br><span class="line">        Schema schema = new Schema(columnSchemas);</span><br><span class="line"></span><br><span class="line">        //创建表时提供的所有选项</span><br><span class="line">        CreateTableOptions tableOptions = new CreateTableOptions();</span><br><span class="line">        //设置副本数</span><br><span class="line">        tableOptions.setNumReplicas(1);</span><br><span class="line">        //设置范围分区的规则</span><br><span class="line">        LinkedList&lt;String&gt; parcols = new LinkedList&lt;String&gt;();</span><br><span class="line">        parcols.add(&quot;id&quot;);</span><br><span class="line">        //设置按照那个字段进行range分区</span><br><span class="line">        tableOptions.addHashPartitions(parcols,6);</span><br><span class="line">        try &#123;</span><br><span class="line">            kuduClient.createTable(&quot;t-hash-partition&quot;,schema,tableOptions);</span><br><span class="line">        &#125; catch (KuduException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        kuduClient.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-多级分区Multilevel-Partitioning"><a href="#3-多级分区Multilevel-Partitioning" class="headerlink" title="3 多级分区Multilevel Partitioning"></a>3 多级分区Multilevel Partitioning</h4><p>Kudu 允许一个表在单个表上组合多级分区。 当正确使用时，多级分区可以保留各个分区类型的优点，同时减少每个分区的缺点</p>
<p>如 范围分区 为5个 hash分区为3个 则多级分区为15个 (即在范围分区里面有进行了hash分区)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 测试分区：</span><br><span class="line">     * 多级分区</span><br><span class="line">     * Multilevel Partition</span><br><span class="line">     * 混合使用hash分区和range分区</span><br><span class="line">     *</span><br><span class="line">     * 哈希分区有利于提高写入数据的吞吐量，而范围分区可以避免tablet无限增长问题，</span><br><span class="line">     * hash分区和range分区结合，可以极大的提升kudu的性能</span><br><span class="line">     */</span><br><span class="line">    @Test</span><br><span class="line">    public void testMultilevelPartition() throws KuduException &#123;</span><br><span class="line">        //设置表的schema</span><br><span class="line">        LinkedList&lt;ColumnSchema&gt; columnSchemas = new LinkedList&lt;ColumnSchema&gt;();</span><br><span class="line">        columnSchemas.add(new Column(&quot;id&quot;, Type.INT32,true));</span><br><span class="line">        columnSchemas.add(new Column(&quot;WorkId&quot;, Type.INT32,false));</span><br><span class="line">        columnSchemas.add(new Column(&quot;Name&quot;, Type.STRING,false));</span><br><span class="line">        columnSchemas.add(new Column(&quot;Gender&quot;, Type.STRING,false));</span><br><span class="line">        columnSchemas.add(new Column(&quot;Photo&quot;, Type.STRING,false));</span><br><span class="line"></span><br><span class="line">        //创建schema</span><br><span class="line">        Schema schema = new Schema(columnSchemas);</span><br><span class="line">        </span><br><span class="line">        //创建表时提供的所有选项</span><br><span class="line">        CreateTableOptions tableOptions = new CreateTableOptions();</span><br><span class="line">        //设置副本数</span><br><span class="line">        //tableOptions.setNumReplicas(1);</span><br><span class="line">        //设置范围分区的规则</span><br><span class="line">        LinkedList&lt;String&gt; parcols = new LinkedList&lt;String&gt;();</span><br><span class="line">        parcols.add(&quot;id&quot;);</span><br><span class="line"></span><br><span class="line">        //hash分区</span><br><span class="line">        tableOptions.addHashPartitions(parcols,5);</span><br><span class="line"></span><br><span class="line">        //range分区</span><br><span class="line">        int count=0;</span><br><span class="line">        for(int i=0;i&lt;10;i++)&#123;</span><br><span class="line">        //指定上界</span><br><span class="line">            PartialRow lower = schema.newPartialRow();</span><br><span class="line">            lower.addInt(&quot;id&quot;,count);</span><br><span class="line">            count+=10;</span><br><span class="line">			//指定下界</span><br><span class="line">            PartialRow upper = schema.newPartialRow();</span><br><span class="line">            upper.addInt(&quot;id&quot;,count);</span><br><span class="line">            tableOptions.addRangePartition(lower,upper);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        try &#123;</span><br><span class="line">            kuduClient.createTable(&quot;t-multilevel-partition&quot;,schema,tableOptions);</span><br><span class="line">        &#125; catch (KuduException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        kuduClient.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h2 id="五-kudu集成impala"><a href="#五-kudu集成impala" class="headerlink" title="五 kudu集成impala"></a>五 kudu集成impala</h2><h3 id="1-impala的配置修改"><a href="#1-impala的配置修改" class="headerlink" title="1 impala的配置修改"></a>1 impala的配置修改</h3><p>可选项 若配置以后写的时候指定 master地址即可</p>
<p>在每一个服务器的impala的配置文件中添加如下配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/default/impala</span><br></pre></td></tr></table></figure>

<p><code>在</code>IMPALA_SERVER_ARGS`下添加：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>-kudu_master_hosts=node-1:7051,node-2:7051,node-3:7051</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">### 2 创建kudu表</span><br><span class="line"></span><br><span class="line">需要先启动hdfs、hive、kudu、impala。使用impala的shell控制台。</span><br><span class="line"></span><br><span class="line">~~~</span><br><span class="line">impala-shell</span><br><span class="line">~~~</span><br><span class="line"></span><br><span class="line">内部表:impala中删除,kudu里也会删除  </span><br><span class="line"></span><br><span class="line">外部表: impala中删除,kudu中不会删除.</span><br><span class="line"></span><br><span class="line">2.1 内部表</span><br><span class="line"></span><br><span class="line">内部表由Impala管理，当您从Impala中删除时，数据和表确实被删除。当您使用Impala创建新表时，它通常是内部表。</span><br><span class="line"></span><br><span class="line">~~~~</span><br><span class="line">CREATE TABLE my_first_table</span><br><span class="line">(</span><br><span class="line">id BIGINT,</span><br><span class="line">name STRING,</span><br><span class="line">PRIMARY KEY(id)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 16 </span><br><span class="line">STORED AS KUDU</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">&apos;kudu.master_addresses&apos; = &apos;node1:7051,node2:7051,node3:7051&apos;,  //这种就是没有配置以上的 指定地址</span><br><span class="line">&apos;kudu.table_name&apos; = &apos;my_first_table&apos;</span><br><span class="line">);</span><br><span class="line">    </span><br><span class="line">在 CREATE TABLE 语句中，必须首先列出构成主键的列。</span><br><span class="line"></span><br><span class="line">~~~~</span><br><span class="line"></span><br><span class="line">2.2 外部表</span><br><span class="line"></span><br><span class="line">外部表（创建者CREATE EXTERNAL TABLE）不受Impala管理，并且删除此表不会将表从其源位置（此处为Kudu）丢弃。相反，它只会去除Impala和Kudu之间的映射。这是Kudu提供的用于将现有表映射到Impala的语法。</span><br><span class="line"></span><br><span class="line">首先使用java创建kudu表：</span><br><span class="line"></span><br><span class="line">~~~</span><br><span class="line">public class CreateTable &#123;</span><br><span class="line">        private static ColumnSchema newColumn(String name, Type type, boolean iskey) &#123;</span><br><span class="line">                ColumnSchema.ColumnSchemaBuilder column = new</span><br><span class="line">                    ColumnSchema.ColumnSchemaBuilder(name, type);</span><br><span class="line">                column.key(iskey);</span><br><span class="line">                return column.build();</span><br><span class="line">        &#125;</span><br><span class="line">    public static void main(String[] args) throws KuduException &#123;</span><br><span class="line">        // master地址</span><br><span class="line">        final String masteraddr = &quot;node1,node2,node3&quot;;</span><br><span class="line">        // 创建kudu的数据库链接</span><br><span class="line">        KuduClient client = new</span><br><span class="line">     KuduClient.KuduClientBuilder(masteraddr).defaultSocketReadTimeoutMs(6000).build();</span><br><span class="line">        </span><br><span class="line">        // 设置表的schema</span><br><span class="line">        List&lt;ColumnSchema&gt; columns = new LinkedList&lt;ColumnSchema&gt;();</span><br><span class="line">        columns.add(newColumn(&quot;CompanyId&quot;, Type.INT32, true));</span><br><span class="line">        columns.add(newColumn(&quot;WorkId&quot;, Type.INT32, false));</span><br><span class="line">        columns.add(newColumn(&quot;Name&quot;, Type.STRING, false));</span><br><span class="line">        columns.add(newColumn(&quot;Gender&quot;, Type.STRING, false));</span><br><span class="line">        columns.add(newColumn(&quot;Photo&quot;, Type.STRING, false));</span><br><span class="line">        Schema schema = new Schema(columns);</span><br><span class="line">    //创建表时提供的所有选项</span><br><span class="line">    CreateTableOptions options = new CreateTableOptions();</span><br><span class="line">        </span><br><span class="line">    // 设置表的replica备份和分区规则</span><br><span class="line">    List&lt;String&gt; parcols = new LinkedList&lt;String&gt;();</span><br><span class="line">        </span><br><span class="line">    parcols.add(&quot;CompanyId&quot;);</span><br><span class="line">    //设置表的备份数</span><br><span class="line">        options.setNumReplicas(1);</span><br><span class="line">    //设置range分区</span><br><span class="line">    options.setRangePartitionColumns(parcols);</span><br><span class="line">        </span><br><span class="line">    //设置hash分区和数量</span><br><span class="line">    options.addHashPartitions(parcols, 3);</span><br><span class="line">    try &#123;</span><br><span class="line">    client.createTable(&quot;person&quot;, schema, options);</span><br><span class="line">    &#125; catch (KuduException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    client.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">~~~</span><br><span class="line"></span><br><span class="line">使用impala创建外部表 ， 将kudu的表映射到impala上。</span><br><span class="line"></span><br><span class="line">~~~</span><br><span class="line">在impala-shell中执行</span><br><span class="line"></span><br><span class="line">CREATE EXTERNAL TABLE `person` STORED AS KUDU</span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">    &apos;kudu.table_name&apos; = &apos;person&apos;,</span><br><span class="line">    &apos;kudu.master_addresses&apos; = &apos;node1:7051,node2:7051,node3:7051&apos;)</span><br><span class="line"></span><br><span class="line">~~~</span><br><span class="line"></span><br><span class="line">### 3 使用impala对kudu进行DML</span><br><span class="line"></span><br><span class="line">1 插入数据</span><br><span class="line"></span><br><span class="line">impala 允许使用标准 SQL 语句将数据插入 Kudu 。</span><br><span class="line"></span><br><span class="line">首先建表：</span><br><span class="line"></span><br><span class="line">~~~~</span><br><span class="line">在impala-shell中</span><br><span class="line">CREATE TABLE my_first_table1</span><br><span class="line">(</span><br><span class="line">id BIGINT,</span><br><span class="line">name STRING,</span><br><span class="line">PRIMARY KEY(id)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 16</span><br><span class="line">STORED AS KUDU </span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">    &apos;kudu.table_name&apos; = &apos;person1&apos;,</span><br><span class="line">    &apos;kudu.master_addresses&apos; = &apos;node1:7051,node2:7051,node3:7051&apos;);</span><br><span class="line"></span><br><span class="line">~~~~</span><br><span class="line"></span><br><span class="line">此示例插入单个行：</span><br></pre></td></tr></table></figure>

<p>INSERT INTO my_first_table VALUES (50, “zhangsan”);</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此示例插入3行：</span><br></pre></td></tr></table></figure>

<p>INSERT INTO my_first_table VALUES (1, “john”), (2, “jane”), (3, “jim”);</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">批量导入数据：</span><br><span class="line"></span><br><span class="line">从 Impala 和 Kudu 的角度来看，通常表现最好的方法通常是使用 Impala 中的 SELECT FROM 语句导入数据。</span><br></pre></td></tr></table></figure>

<p>INSERT INTO my_first_table SELECT * FROM temp1;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2 更新数据</span><br></pre></td></tr></table></figure>

<p>UPDATE my_first_table SET name=”xiaowang” where id =1 ;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 删除数据</span><br></pre></td></tr></table></figure>

<p>delete from my_first_table where id =2;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4 更改表属性</span><br><span class="line"></span><br><span class="line">4.1重命名impala表</span><br></pre></td></tr></table></figure>

<p>ALTER TABLE PERSON RENAME TO person_temp;</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> 4.2重新命名内部表的基础kudu表</span><br><span class="line"></span><br><span class="line">  				4.1.1创建内部表</span><br><span class="line"></span><br><span class="line">~~~</span><br><span class="line">CREATE TABLE kudu_student</span><br><span class="line">(</span><br><span class="line">CompanyId INT,</span><br><span class="line">WorkId INT,</span><br><span class="line">Name STRING,</span><br><span class="line">Gender STRING,</span><br><span class="line">Photo STRING,</span><br><span class="line">PRIMARY KEY(CompanyId)</span><br><span class="line">)</span><br><span class="line">PARTITION BY HASH PARTITIONS 16</span><br><span class="line">STORED AS KUDU</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">&apos;kudu.master_addresses&apos; = &apos;node1:7051,node2:7051,node3:7051&apos;,</span><br><span class="line">&apos;kudu.table_name&apos; = &apos;student&apos;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">~~~</span><br><span class="line"></span><br><span class="line">				4.1.2如果表是内部表，则可以通过更改 kudu.table_name 属性重命名底层的 Kudu 表。</span><br></pre></td></tr></table></figure>

<p>ALTER TABLE kudu_student SET TBLPROPERTIES(‘kudu.table_name’ = ‘new_student’);<br>&lt;!–￼33–&gt;</p>
<p>CREATE EXTERNAL TABLE external_table<br>    STORED AS KUDU<br>    TBLPROPERTIES (<br>    ‘kudu.master_addresses’ = ‘node1:7051,node2:7051,node3:7051’,<br>    ‘kudu.table_name’ = ‘person’<br>);</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">重新映射外部表，指向不同的kudu表：</span><br></pre></td></tr></table></figure>

<p>ALTER TABLE external_table<br>SET TBLPROPERTIES(‘kudu.table_name’ = ‘hashTable’)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">上面的操作是：将external_table映射的PERSON表重新指向hashTable表。</span><br><span class="line"></span><br><span class="line">4.4 更改kudu master地址</span><br></pre></td></tr></table></figure>

<p>ALTER TABLE my_table<br>SET TBLPROPERTIES(‘kudu.master_addresses’ = ‘kudu-new-master.example.com:7051’);</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">4.5 将内部表改为外部表</span><br></pre></td></tr></table></figure>

<p>ALTER TABLE my_table SET TBLPROPERTIES(‘EXTERNAL’ = ‘TRUE’);</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">\##4 impala使用Java操作kudu</span><br><span class="line"></span><br><span class="line">对于impala而言，开发人员是可以通过JDBC连接impala的，有了JDBC，开发人员可以通过impala来间接操作 kudu。</span><br><span class="line"></span><br><span class="line">### 1 引入依赖</span><br></pre></td></tr></table></figure>

   <!--impala的jdbc操作--> 
<dependency>
        <groupid>com.cloudera</groupid>
        <artifactid>ImpalaJDBC41</artifactid>
        <version>2.5.42</version>
    </dependency>

<pre><code>&lt;!--Caused by : ClassNotFound : thrift.protocol.TPro--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt;
    &lt;artifactId&gt;libfb303&lt;/artifactId&gt;
    &lt;version&gt;0.9.3&lt;/version&gt;
    &lt;type&gt;pom&lt;/type&gt;
&lt;/dependency&gt;

&lt;!--Caused by : ClassNotFound : thrift.protocol.TPro--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.thrift&lt;/groupId&gt;
    &lt;artifactId&gt;libthrift&lt;/artifactId&gt;
    &lt;version&gt;0.9.3&lt;/version&gt;
    &lt;type&gt;pom&lt;/type&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;
    &lt;exclusions&gt;
        &lt;exclusion&gt;
            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
            &lt;artifactId&gt;hive-service-rpc&lt;/artifactId&gt;
        &lt;/exclusion&gt;
        &lt;exclusion&gt;
            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
            &lt;artifactId&gt;hive-service&lt;/artifactId&gt;
        &lt;/exclusion&gt;
    &lt;/exclusions&gt;
    &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;

&lt;!--导入hive--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;
    &lt;artifactId&gt;hive-service&lt;/artifactId&gt;
    &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 2 jdbc连接impala操作kudu</span><br><span class="line"></span><br><span class="line">使用JDBC连接impala操作kudu，与JDBC连接mysql做更重增删改查基本一样。</span><br><span class="line"></span><br><span class="line">创建实体类</span><br></pre></td></tr></table></figure>

<p>package cn.itcast.impala.impala;</p>
<p>public class Person {<br>    private int companyId;<br>    private int workId;<br>    private  String name;<br>    private  String gender;<br>    private  String photo;</p>
<pre><code>public Person(int companyId, int workId, String name, String gender, String photo) {
    this.companyId = companyId;
    this.workId = workId;
    this.name = name;
    this.gender = gender;
    this.photo = photo;
}

public int getCompanyId() {
    return companyId;
}

public void setCompanyId(int companyId) {
    this.companyId = companyId;
}

public int getWorkId() {
    return workId;
}

public void setWorkId(int workId) {
    this.workId = workId;
}

public String getName() {
    return name;
}

public void setName(String name) {
    this.name = name;
}

public String getGender() {
    return gender;
}

public void setGender(String gender) {
    this.gender = gender;
}

public String getPhoto() {
    return photo;
}

public void setPhoto(String photo) {
    this.photo = photo;
}</code></pre><p>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### JDBC连接impala对kudu进行增删改查</span><br></pre></td></tr></table></figure>

<p>package cn.itcast.impala.impala;</p>
<p>import java.sql.*;</p>
<p>public class Contants {<br>    private static String JDBC_DRIVER=”com.cloudera.impala.jdbc41.Driver”;<br>    private static  String CONNECTION_URL=”jdbc:impala://node1:21050/default;auth=noSasl”;<br>     //定义数据库连接<br>    static Connection conn=null;<br>    //定义PreparedStatement对象<br>    static PreparedStatement ps=null;<br>    //定义查询的结果集<br>    static ResultSet rs= null;</p>
<pre><code>//数据库连接
public static Connection getConn(){
    try {
        Class.forName(JDBC_DRIVER);
        conn=DriverManager.getConnection(CONNECTION_URL);
    } catch (Exception e) {
        e.printStackTrace();
    }

    return  conn;

}

//创建一个表
public static void createTable(){
    conn=getConn();
    String sql=&quot;CREATE TABLE impala_kudu_test&quot; +
            &quot;(&quot; +
            &quot;companyId BIGINT,&quot; +
            &quot;workId BIGINT,&quot; +
            &quot;name STRING,&quot; +
            &quot;gender STRING,&quot; +
            &quot;photo STRING,&quot; +
            &quot;PRIMARY KEY(companyId)&quot; +
            &quot;)&quot; +
            &quot;PARTITION BY HASH PARTITIONS 16 &quot; +
            &quot;STORED AS KUDU &quot; +
            &quot;TBLPROPERTIES (&quot; +
            &quot;&apos;kudu.master_addresses&apos; = &apos;node1:7051,node2:7051,node3:7051&apos;,&quot; +
            &quot;&apos;kudu.table_name&apos; = &apos;impala_kudu_test&apos;&quot; +
            &quot;);&quot;;

    try {
        ps = conn.prepareStatement(sql);
        ps.execute();
    } catch (SQLException e) {
        e.printStackTrace();
    }
}


//查询数据
public static ResultSet queryRows(){
    try {
        //定义执行的sql语句
        String sql=&quot;select * from impala_kudu_test&quot;;
        ps = getConn().prepareStatement(sql);
        rs= ps.executeQuery();
    } catch (SQLException e) {
        e.printStackTrace();
    }

    return  rs;
}

//打印结果
public  static void printRows(ResultSet rs){
    /**
     private int companyId;
     private int workId;
     private  String name;
     private  String gender;
     private  String photo;
     */

    try {
        while (rs.next()){
            //获取表的每一行字段信息
            int companyId = rs.getInt(&quot;companyId&quot;);
            int workId = rs.getInt(&quot;workId&quot;);
            String name = rs.getString(&quot;name&quot;);
            String gender = rs.getString(&quot;gender&quot;);
            String photo = rs.getString(&quot;photo&quot;);
            System.out.print(&quot;companyId:&quot;+companyId+&quot; &quot;);
            System.out.print(&quot;workId:&quot;+workId+&quot; &quot;);
            System.out.print(&quot;name:&quot;+name+&quot; &quot;);
            System.out.print(&quot;gender:&quot;+gender+&quot; &quot;);
            System.out.println(&quot;photo:&quot;+photo);

        }
    } catch (SQLException e) {
        e.printStackTrace();
    }finally {
        if(ps!=null){
            try {
                ps.close();
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }

        if(conn !=null){
            try {
                conn.close();
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
    }
}


//插入数据
public static void insertRows(Person person){
    conn=getConn();
    String sql=&quot;insert into table impala_kudu_test(companyId,workId,name,gender,photo) values(?,?,?,?,?)&quot;;

    try {
        ps=conn.prepareStatement(sql);
        //给占位符？赋值
        ps.setInt(1,person.getCompanyId());
        ps.setInt(2,person.getWorkId());
        ps.setString(3,person.getName());
        ps.setString(4,person.getGender());
        ps.setString(5,person.getPhoto());
        ps.execute();

    } catch (SQLException e) {
        e.printStackTrace();
    }finally {
        if(ps !=null){
            try {
                //关闭
                ps.close();
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }

        if(conn !=null){
            try {
                  //关闭
                conn.close();
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
    }

}

//更新数据
public static void updateRows(Person person){
   //定义执行的sql语句
    String sql=&quot;update impala_kudu_test set workId=&quot;+person.getWorkId()+
            &quot;,name=&apos;&quot;+person.getName()+&quot;&apos; ,&quot;+&quot;gender=&apos;&quot;+person.getGender()+&quot;&apos; ,&quot;+
            &quot;photo=&apos;&quot;+person.getPhoto()+&quot;&apos; where companyId=&quot;+person.getCompanyId();

    try {
        ps= getConn().prepareStatement(sql);
        ps.execute();
    } catch (SQLException e) {
        e.printStackTrace();
    }finally {
        if(ps !=null){
            try {
                  //关闭
                ps.close();
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }

        if(conn !=null){
            try {
                  //关闭
                conn.close();
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
    }
}

//删除数据
public   static void deleteRows(int companyId){

    //定义sql语句
    String sql=&quot;delete from impala_kudu_test where companyId=&quot;+companyId;
    try {
        ps =getConn().prepareStatement(sql);
        ps.execute();
    } catch (SQLException e) {
        e.printStackTrace();

    }
}</code></pre><p>   //删除表<br>    public static void dropTable() {<br>        String sql=”drop table if exists impala_kudu_test”;<br>        try {<br>            ps =getConn().prepareStatement(sql);<br>            ps.execute();<br>        } catch (SQLException e) {<br>            e.printStackTrace();<br>        }<br>    }<br>}</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 代码测试运行</span><br></pre></td></tr></table></figure>

<p>package cn.itcast.impala.impala;</p>
<p>import java.sql.Connection;</p>
<p>public class ImpalaJdbcClient {<br>    public static void main(String[] args) {<br>        Connection conn = Contants.getConn();</p>
<pre><code>    //创建一个表
   Contants.createTable();

    //插入数据
   Contants.insertRows(new Person(1,100,&quot;lisi&quot;,&quot;male&quot;,&quot;lisi-photo&quot;));

    //查询表的数据
    ResultSet rs = Contants.queryRows();
    Contants.printRows(rs);

    //更新数据
    Contants.updateRows(new Person(1,200,&quot;zhangsan&quot;,&quot;male&quot;,&quot;zhangsan-photo&quot;));

    //删除数据
    Contants.deleteRows(1);

    //删除表
    Contants.dropTable();

}</code></pre><p>}<br>```</p>
<h1 id="六-Apache-KUDU的原理"><a href="#六-Apache-KUDU的原理" class="headerlink" title="六 Apache KUDU的原理"></a>六 Apache KUDU的原理</h1><h2 id="1．-table与schema"><a href="#1．-table与schema" class="headerlink" title="1． table与schema"></a>1． table与schema</h2><p>Kudu设计是面向结构化存储的，因此，Kudu的表需要用户在建表时定义它的Schema信息，这些Schema信息包含：列定义（含类型），Primary Key定义（用户指定的若干个列的有序组合）。数据的唯一性，依赖于用户所提供的Primary Key中的Column组合的值的唯一性。Kudu提供了Alter命令来增删列，但位于Primary Key中的列是不允许删除的。</p>
<p>从用户角度来看，Kudu是一种存储结构化数据表的存储系统。在一个Kudu集群中可以定义任意数量的table，每个table都需要预先定义好schema。每个table的列数是确定的，每一列都需要有名字和类型，每个表中可以把其中一列或多列定义为主键。这么看来，Kudu更像关系型数据库，而不是像HBase、Cassandra和MongoDB这些NoSQL数据库。不过Kudu目前还不能像关系型数据一样支持二级索引。</p>
<p>Kudu使用确定的列类型，而不是类似于NoSQL的“everything is byte”。带来好处：确定的列类型使Kudu可以进行类型特有的编码,可以提供元数据给其他上层查询工具。</p>
<h2 id="2-kudu底层数据模型"><a href="#2-kudu底层数据模型" class="headerlink" title="2 kudu底层数据模型"></a>2 kudu底层数据模型</h2><p>Kudu的底层数据文件的存储，未采用HDFS这样的较高抽象层次的分布式文件系统，而是自行开发了一套可基于Table/Tablet/Replica视图级别的底层存储系统。</p>
<p>这套实现基于如下的几个设计目标：</p>
<p>• 可提供快速的列式查询</p>
<p>• 可支持快速的随机更新</p>
<p>• 可提供更为稳定的查询性能保障</p>
<p><a href="https://manzhong.github.io/images/kudu/dcs.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/dcs.png" alt="img"></a></p>
<p>一张table会分成若干个tablet，每个tablet包括MetaData元信息及若干个RowSet。</p>
<p>RowSet包含一个MemRowSet及若干个DiskRowSet，DiskRowSet中包含一个BloomFile、Ad_hoc Index、BaseData、DeltaMem及若干个RedoFile和UndoFile。</p>
<p>MemRowSet：用于新数据insert及已在MemRowSet中的数据的更新，一个MemRowSet写满后会将数据刷到磁盘形成若干个DiskRowSet。默认是1G或者或者120S。</p>
<p>DiskRowSet：用于老数据的变更，后台定期对DiskRowSet做compaction，以删除没用的数据及合并历史数据，减少查询过程中的IO开销。</p>
<p>BloomFile：根据一个DiskRowSet中的key生成一个bloom filter，用于快速模糊定位某个key是否在DiskRowSet中。</p>
<p>Ad_hocIndex：是主键的索引，用于定位key在DiskRowSet中的具体哪个偏移位置。</p>
<p>BaseData是MemRowSet flush下来的数据，按列存储，按主键有序。</p>
<p>UndoFile是基于BaseData之前时间的历史数据，通过在BaseData上apply UndoFile中的记录，可以获得历史数据。</p>
<p>RedoFile是基于BaseData之后时间的变更记录，通过在BaseData上apply RedoFile中的记录，可获得较新的数据。</p>
<p>DeltaMem用于DiskRowSet中数据的变更，先写到内存中，写满后flush到磁盘形成RedoFile。</p>
<p>REDO与UNDO与关系型数据库中的REDO与UNDO日志类似（在关系型数据库中，REDO日志记录了更新后的数据，可以用来恢复尚未写入Data File的已成功事务更新的数据。而UNDO日志用来记录事务更新之前的数据，可以用来在事务失败时进行回滚）</p>
<p><a href="https://manzhong.github.io/images/kudu/dcs2.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/dcs2.png" alt="img"></a></p>
<p>MemRowSets可以对比理解成HBase中的MemStore, 而DiskRowSets可理解成HBase中的HFile。</p>
<p>MemRowSets中的数据被Flush到磁盘之后，形成DiskRowSets。 DisRowSets中的数据，按照32MB大小为单位，按序划分为一个个的DiskRowSet。 DiskRowSet中的数据按照Column进行组织，与Parquet类似。</p>
<p>这是Kudu可支持一些分析性查询的基础。每一个Column的数据被存储在一个相邻的数据区域，而这个数据区域进一步被细分成一个个的小的Page单元，与HBase File中的Block类似，对每一个Column Page可采用一些Encoding算法，以及一些通用的Compression算法。 既然可对Column Page可采用Encoding以及Compression算法，那么，对单条记录的更改就会比较困难了。</p>
<p>前面提到了Kudu可支持单条记录级别的更新/删除，是如何做到的？</p>
<p>与HBase类似，也是通过增加一条新的记录来描述这次更新/删除操作的。DiskRowSet是不可修改了，那么 KUDU 要如何应对数据的更新呢？在KUDU中，把DiskRowSet分为了两部分：base data、delta stores。base data 负责存储基础数据，delta stores负责存储 base data 中的变更数据.</p>
<p><a href="https://manzhong.github.io/images/kudu/dcs3.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/dcs3.png" alt="img"></a></p>
<p>如上图所示，数据从MemRowSet 刷到磁盘后就形成了一份 DiskRowSet（只包含 base data），每份 DiskRowSet 在内存中都会有一个对应的 DeltaMemStore，负责记录此 DiskRowSet 后续的数据变更（更新、删除）。DeltaMemStore 内部维护一个 B-树索引，映射到每个 row_offset 对应的数据变更。DeltaMemStore 数据增长到一定程度后转化成二进制文件存储到磁盘，形成一个 DeltaFile，随着 base data 对应数据的不断变更，DeltaFile 逐渐增长。</p>
<h2 id="3-tablet发现过程"><a href="#3-tablet发现过程" class="headerlink" title="3 tablet发现过程"></a>3 tablet发现过程</h2><p>当创建Kudu客户端时，其会从主服务器上获取tablet位置信息，然后直接与服务于该tablet的服务器进行交谈。</p>
<p>为了优化读取和写入路径，客户端将保留该信息的本地缓存，以防止他们在每个请求时需要查询主机的tablet位置信息。随着时间的推移，客户端的缓存可能会变得过时，并且当写入被发送到不再是tablet领导者的tablet服务器时，则将被拒绝。然后客户端将通过查询主服务器发现新领导者的位置来更新其缓存。</p>
<p><a href="https://manzhong.github.io/images/kudu/f.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/f.png" alt="img"></a></p>
<h2 id="4-kudu写流程"><a href="#4-kudu写流程" class="headerlink" title="4 kudu写流程"></a>4 kudu写流程</h2><p>当 Client 请求写数据时，先根据主键从Master Server中获取要访问的目标 Tablets，然后到依次对应的Tablet获取数据。</p>
<p>因为KUDU表存在主键约束，所以需要进行主键是否已经存在的判断，这里就涉及到之前说的索引结构对读写的优化了。一个Tablet中存在很多个RowSets，为了提升性能，我们要尽可能地减少要扫描的RowSets数量。</p>
<p>首先，我们先通过每个 RowSet 中记录的主键的（最大最小）范围，过滤掉一批不存在目标主键的RowSets，然后在根据RowSet中的布隆过滤器，过滤掉确定不存在目标主键的 RowSets，最后再通过RowSets中的 B-树索引，精确定位目标主键是否存在。</p>
<p>如果主键已经存在，则报错（主键重复），否则就进行写数据（写 MemRowSet）。</p>
<p><a href="https://manzhong.github.io/images/kudu/w.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/w.png" alt="img"></a></p>
<h2 id="5kudu读流程"><a href="#5kudu读流程" class="headerlink" title="5kudu读流程"></a>5kudu读流程</h2><p>数据读取过程大致如下：先根据要扫描数据的主键范围，定位到目标的Tablets，然后读取Tablets 中的RowSets。</p>
<p>在读取每个RowSet时，先根据主键过滤要scan范围，然后加载范围内的base data，再找到对应的delta stores，应用所有变更，最后union上MemRowSet中的内容，返回数据给Client。</p>
<p><a href="https://manzhong.github.io/images/kudu/r.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/r.png" alt="img"></a></p>
<h2 id="6kudu更新流程"><a href="#6kudu更新流程" class="headerlink" title="6kudu更新流程"></a>6kudu更新流程</h2><p>数据更新的核心是定位到待更新数据的位置，这块与写入的时候类似，就不展开了，等定位到具体位置后，然后将变更写到对应的delta store 中。</p>
<p><a href="https://manzhong.github.io/images/kudu/updata.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kudu/updata.png" alt="img"></a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/kudu/" data-id="cjz2c0w9d000sagu5kubxz9dg" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-ClouderaManager" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/ClouderaManager/" class="article-date">
  <time datetime="2019-08-08T03:40:10.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/ClouderaManager/">ClouderaManager</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="ClouderaManager"><a href="#ClouderaManager" class="headerlink" title="ClouderaManager"></a>ClouderaManager</h1><p>cm 管理工具,用来安装cdh&lt;hadoop,zookeeper,habse…….&gt;</p>
<p>cdh (cloudera distribute hadoop&lt;hadoop,zookeeper,hbase……&gt;)</p>
<p>版本:5.14.0</p>
<p>Cloudera Manager是cloudera公司提供的一种大数据的解决方案，可以通过ClouderaManager管理界面来对我们的集群进行安装和操作，提供了良好的UI界面交互，使得我们管理集群不用熟悉任何的linux技术，只需要通过网页浏览器就可以实现我们的集群的操作和管理，让我们使用和管理集群更加的方便。</p>
<h1 id="1、ClouderaManager整体架构"><a href="#1、ClouderaManager整体架构" class="headerlink" title="1、ClouderaManager整体架构"></a>1、ClouderaManager整体架构</h1><p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image001.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image001.png" alt="img"></a></p>
<p>Cloudera Manager的核心是Cloudera Manager Server。Server托管Admin Console Web Server和应用程序逻辑。它负责安装软件、配置、启动和停止服务以及管理运行服务的群集。</p>
<p>解释：</p>
<p>· Agent：安装在每台主机上。它负责启动和停止进程，解压缩配置，触发安装和监控主机</p>
<p>· Management Service：执行各种监控、报警和报告功能的一组角色的服务。</p>
<p>· Database：存储配置和监控信息</p>
<p>· Cloudera Repository：可供Cloudera Manager分配的软件的存储库（repo库）</p>
<p>· Client：用于与服务器进行交互的接口：</p>
<p>· Admin Console：管理员控制台</p>
<p>· API：开发人员使用 API可以创建自定义的Cloudera Manager应用程序</p>
<h2 id="Cloudera-Management-Service"><a href="#Cloudera-Management-Service" class="headerlink" title="Cloudera Management Service"></a><strong>Cloudera Management Service</strong></h2><p>Cloudera Management Service 可作为一组角色实施各种管理功能</p>
<p>· Activity Monitor：收集有关服务运行的活动的信息</p>
<p>· Host Monitor：收集有关主机的运行状况和指标信息</p>
<p>· Service Monitor：收集有关服务的运行状况和指标信息</p>
<p>· Event Server：聚合组件的事件并将其用于警报和搜索</p>
<p>· Alert Publisher ：为特定类型的事件生成和提供警报</p>
<p>· Reports Manager：生成图表报告，它提供用户、用户组的目录的磁盘使用率、磁盘、io等历史视图</p>
<h2 id="信号检测"><a href="#信号检测" class="headerlink" title="信号检测"></a><strong>信号检测</strong></h2><p>默认情况下，Agent 每隔 15 秒向 Cloudera Manager Server 发送一次检测信号。但是，为了减少用户延迟，在状态变化时会提高频率。</p>
<h2 id="状态管理"><a href="#状态管理" class="headerlink" title="状态管理"></a><strong>状态管理</strong></h2><p>· 模型状态捕获什么进程应在何处运行以及具有什么配置</p>
<p>· 运行时状态是哪些进程正在何处运行以及正在执行哪些命令（例如，重新平衡 HDFS 或执行备份/灾难恢复计划或滚动升级或停止）</p>
<p>· 当您更新配置（例如Hue Server Web 端口）时，您即更新了模型状态。但是，如果 Hue 在更新时正在运行，则它仍将使用旧端口。当出现这种不匹配情况时，角色会标记为具有”过时的配置”。要重新同步，您需重启角色（这会触发重新生成配置和重启进程）</p>
<p>· 特殊情况如果要加入一些clouder manager控制台没有的属性时候都在高级里面嵌入</p>
<h2 id="服务器和客户端配置"><a href="#服务器和客户端配置" class="headerlink" title="服务器和客户端配置"></a><strong>服务器和客户端配置</strong></h2><p>· 如使用HDFS，文件 /etc/hadoop/conf/hdfs-site.xml 仅包含与 HDFS 客户端相关的配置</p>
<p>· 而 HDFS 角色实例（例如，NameNode 和 DataNode）会从/var/run/cloudera-scm-agent/process/unique-process-name下的每个进程专用目录获取它们的配置</p>
<h2 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a><strong>进程管理</strong></h2><p>· 在 Cloudera Manager 管理的群集中，只能通过 Cloudera Manager 启动或停止服务。ClouderaManager 使用一种名为 supervisord的开源进程管理工具，它会重定向日志文件，通知进程失败，为合适用户设置调用进程的有效用户 ID 等等</p>
<p>· Cloudera Manager 支持自动重启崩溃进程。如果一个角色实例在启动后反复失败，Cloudera Manager还会用不良状态标记该实例</p>
<p>· 特别需要注意的是，停止 Cloudera Manager 和 Cloudera Manager Agent 不会停止群集；所有正在运行的实例都将保持运行</p>
<p>· Agent 的一项主要职责是启动和停止进程。当 Agent 从检测信号检测到新进程时，Agent 会在/var/run/cloudera-scm-agent 中为它创建一个目录，并解压缩配置</p>
<p>· Agent 受到监控，属于 Cloudera Manager 的主机监控的一部分：如果 Agent 停止检测信号，主机将被标记为运行状况不良</p>
<h2 id="主机管理"><a href="#主机管理" class="headerlink" title="主机管理"></a><strong>主机管理</strong></h2><p>· Cloudera Manager 自动将作为群集中的托管主机身份：JDK、Cloudera Manager Agent、CDH、Impala、Solr 等参与所需的所有软件部署到主机</p>
<p>· Cloudera Manager 提供用于管理参与主机生命周期的操作以及添加和删除主机的操作</p>
<p>· Cloudera Management Service Host Monitor 角色执行运行状况检查并收集主机度量，以使您可以监控主机的运行状况和性能</p>
<h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a><strong>安全</strong></h2><p>· <strong>身份验证</strong></p>
<p>· Hadoop中身份验证的目的仅仅是证明用户或服务确实是他或她所声称的用户或服务，通常，企业中的身份验证通过单个分布式系统（例如，轻型目录访问协议 (LDAP) 目录）进行管理。LDAP身份验证包含由各种存储系统提供支持的简单用户名/密码服务</p>
<p>· Hadoop 生态系统的许多组件会汇总到一起来使用 Kerberos 身份验证并提供用于在 LDAP 或 AD 中管理和存储凭据的选项</p>
<p>· <strong>授权</strong><br>CDH 当前提供以下形式的访问控制：</p>
<p>· 适用于目录和文件的传统 POSIX 样式的权限</p>
<p>· 适用于 HDFS 的扩展的访问控制列表 (ACL)</p>
<p>· Apache HBase 使用 ACL 来按列、列族和列族限定符授权各种操作 (READ, WRITE, CREATE, ADMIN)</p>
<p>· 使用 Apache Sentry 基于角色进行访问控制</p>
<p>· <strong>加密</strong></p>
<p>· 需要获得企业版的Cloudera（Cloudera Navigator 许可）</p>
<h1 id="2、clouderaManager环境安装前准备"><a href="#2、clouderaManager环境安装前准备" class="headerlink" title="2、clouderaManager环境安装前准备"></a>2、clouderaManager环境安装前准备</h1><p>准备两台虚拟机，其中一台作为我们的主节点，安装我们的ClouderaManager Server与ClouderaManager agent，另外一台作为我们的从节点只安装我们的clouderaManager agent</p>
<p>机器规划如下</p>
<table>
<thead>
<tr>
<th align="left">服务器IP</th>
<th align="left">192.168.52.100</th>
<th align="left">192.168.52.110</th>
</tr>
</thead>
<tbody><tr>
<td align="left">主机名</td>
<td align="left">node01.hadoop.com</td>
<td align="left">node02.hadoop.com</td>
</tr>
<tr>
<td align="left">主机名与IP地址映射</td>
<td align="left">是</td>
<td align="left">是</td>
</tr>
<tr>
<td align="left">防火墙</td>
<td align="left">关闭</td>
<td align="left">关闭</td>
</tr>
<tr>
<td align="left">selinux</td>
<td align="left">关闭</td>
<td align="left">关闭</td>
</tr>
<tr>
<td align="left">jdk</td>
<td align="left">安装</td>
<td align="left">安装</td>
</tr>
<tr>
<td align="left">ssh免密码登录</td>
<td align="left">是</td>
<td align="left">是</td>
</tr>
<tr>
<td align="left">mysql数据库</td>
<td align="left">否</td>
<td align="left">是</td>
</tr>
<tr>
<td align="left">服务器内存</td>
<td align="left">16G</td>
<td align="left">8G</td>
</tr>
</tbody></table>
<p>所有机器统一两个路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/softwares/</span><br><span class="line">mkdir -p /export/servers/</span><br></pre></td></tr></table></figure>

<h2 id="2-1、两台机器更改主机名"><a href="#2-1、两台机器更改主机名" class="headerlink" title="2.1、两台机器更改主机名"></a>2.1、两台机器更改主机名</h2><p>第一台机器更改主机名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=node01.hadoop.com</span><br></pre></td></tr></table></figure>

<p>第二台机器更改主机名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network</span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=node02.hadoop.com</span><br></pre></td></tr></table></figure>

<h2 id="2-2、更改主机名与IP地址的映射"><a href="#2-2、更改主机名与IP地址的映射" class="headerlink" title="2.2、更改主机名与IP地址的映射"></a>2.2、更改主机名与IP地址的映射</h2><p>两台机器更改hosts文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line">192.168.52.100 node01.hadoop.com  node01</span><br><span class="line">192.168.52.110 node02.hadoop.com  node02</span><br></pre></td></tr></table></figure>

<h2 id="2-3、两台机器关闭防火墙"><a href="#2-3、两台机器关闭防火墙" class="headerlink" title="2.3、两台机器关闭防火墙"></a>2.3、两台机器关闭防火墙</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service iptables stop</span><br><span class="line">chkconfig iptables off</span><br></pre></td></tr></table></figure>

<h2 id="2-4、两台机器关闭selinux"><a href="#2-4、两台机器关闭selinux" class="headerlink" title="2.4、两台机器关闭selinux"></a>2.4、两台机器关闭selinux</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/selinux/config</span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image003.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image003.jpg" alt="img"></a></p>
<h2 id="2-5、两台机器安装jdk"><a href="#2-5、两台机器安装jdk" class="headerlink" title="2.5、两台机器安装jdk"></a>2.5、两台机器安装jdk</h2><p>将我们的jdk的压缩包上传到node01.hadoop.com的/export/softwares路径下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares/</span><br><span class="line">tar -zxvf jdk-8u141-linux-x64.tar.gz  -C /export/servers/</span><br></pre></td></tr></table></figure>

<p>配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/export/servers/jdk1.8.0_141</span><br><span class="line">export PATH=:$JAVA_HOME/bin:PATH</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<p>第二台机器同样安装jdk即可</p>
<h2 id="2-6、两台机器实现SSH免密码登录"><a href="#2-6、两台机器实现SSH免密码登录" class="headerlink" title="2.6、两台机器实现SSH免密码登录"></a>2.6、两台机器实现SSH免密码登录</h2><h3 id="第一步：两台器生成公钥与私钥"><a href="#第一步：两台器生成公钥与私钥" class="headerlink" title="第一步：两台器生成公钥与私钥"></a>第一步：两台器生成公钥与私钥</h3><p>两台机器上面执行以下命令，然后按下三个回车键即可生成公钥与私钥</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image005.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image005.jpg" alt="img"></a></p>
<h3 id="第二步：两台机器将公钥拷贝到同一个文件当中去"><a href="#第二步：两台机器将公钥拷贝到同一个文件当中去" class="headerlink" title="第二步：两台机器将公钥拷贝到同一个文件当中去"></a>第二步：两台机器将公钥拷贝到同一个文件当中去</h3><p>两台机器执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id node01.hadoop.com</span><br></pre></td></tr></table></figure>

<h3 id="第三步：拷贝authorized-keys到其他机器"><a href="#第三步：拷贝authorized-keys到其他机器" class="headerlink" title="第三步：拷贝authorized_keys到其他机器"></a>第三步：拷贝authorized_keys到其他机器</h3><p>第一台机器上将authorized_keys拷贝到第二台机器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp /root/.ssh/authorized_keys node02.hadoop.com:/root/.ssh/</span><br></pre></td></tr></table></figure>

<h2 id="2-7、第二台机器安装mysql数据库"><a href="#2-7、第二台机器安装mysql数据库" class="headerlink" title="2.7、第二台机器安装mysql数据库"></a>2.7、第二台机器安装mysql数据库</h2><p>通过yum源，在线安装mysql</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum  install  mysql  mysql-server  mysql-devel</span><br><span class="line">/etc/init.d/mysqld start</span><br><span class="line">/usr/bin/mysql_secure_installation</span><br><span class="line"> grant all privileges on . to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;</span><br><span class="line"> flush privileges;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image007.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image007.jpg" alt="img"></a></p>
<h2 id="2-8、解除linux系统打开文件最大数量的限制"><a href="#2-8、解除linux系统打开文件最大数量的限制" class="headerlink" title="2.8、解除linux系统打开文件最大数量的限制"></a>2.8、解除linux系统打开文件最大数量的限制</h2><p>两台机器都需要执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/security/limits.conf</span><br></pre></td></tr></table></figure>

<p>添加以下内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">*   soft noproc 11000</span><br><span class="line"></span><br><span class="line">*   hard noproc 11000</span><br><span class="line"></span><br><span class="line">*   soft nofile 65535</span><br><span class="line"></span><br><span class="line">*   hard nofile 65535</span><br></pre></td></tr></table></figure>

<h2 id="2-9、设置linux交换区内存"><a href="#2-9、设置linux交换区内存" class="headerlink" title="2.9、设置linux交换区内存"></a>2.9、设置linux交换区内存</h2><p>两台机器都要执行</p>
<p>执行命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 10 &gt; /proc/sys/vm/swappiness</span><br></pre></td></tr></table></figure>

<p>并编辑文件sysctl.conf：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br><span class="line"></span><br><span class="line">添加或修改</span><br><span class="line"></span><br><span class="line">vm.swappiness = 0</span><br></pre></td></tr></table></figure>

<p>两台机器都要执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br></pre></td></tr></table></figure>

<p>并编辑文件rc.local ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/rc.local</span><br><span class="line"></span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image009.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image009.jpg" alt="img"></a></p>
<h2 id="2-10、两台机器时钟同步"><a href="#2-10、两台机器时钟同步" class="headerlink" title="2.10、两台机器时钟同步"></a>2.10、两台机器时钟同步</h2><p>两台机器需要进行时钟同步操作，保证两台机器时间相同</p>
<h1 id="3、clouderaManager安装资源下载"><a href="#3、clouderaManager安装资源下载" class="headerlink" title="3、clouderaManager安装资源下载"></a>3、clouderaManager安装资源下载</h1><h2 id="第一步：下载安装资源并上传到服务器"><a href="#第一步：下载安装资源并上传到服务器" class="headerlink" title="第一步：下载安装资源并上传到服务器"></a>第一步：下载安装资源并上传到服务器</h2><p>我们这里安装CM5.14.0这个版本，需要下载以下这些资源，一共是四个文件即可</p>
<p>下载cm5的压缩包</p>
<p>下载地址：<a href="http://archive.cloudera.com/cm5/cm/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cm5/cm/5/</a></p>
<p>具体文件地址：</p>
<p><a href="http://archive.cloudera.com/cm5/cm/5/cloudera-manager-el6-cm5.14.0_x86_64.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cm5/cm/5/cloudera-manager-el6-cm5.14.0_x86_64.tar.gz</a></p>
<p>下载cm5的parcel包</p>
<p>下载地址：</p>
<p><a href="http://archive.cloudera.com/cdh5/parcels/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/parcels/</a></p>
<p>第一个文件具体下载地址：</p>
<p><a href="http://archive.cloudera.com/cdh5/parcels/5.14.0/CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/parcels/5.14.0/CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel</a></p>
<p>第二个文件具体下载地址：</p>
<p><a href="http://archive.cloudera.com/cdh5/parcels/5.14.0/CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha1" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/parcels/5.14.0/CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha1</a></p>
<p>第三个文件具体下载地址：</p>
<p><a href="http://archive.cloudera.com/cdh5/parcels/5.14.0/manifest.json" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/parcels/5.14.0/manifest.json</a></p>
<p>将这四个安装包都上传到第一台机器的/opt/softwares路径下</p>
<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image011.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image011.jpg" alt="img"></a></p>
<h2 id="第二步：解压压缩包到指定路径"><a href="#第二步：解压压缩包到指定路径" class="headerlink" title="第二步：解压压缩包到指定路径"></a>第二步：解压压缩包到指定路径</h2><p>解压CM安装包到/opt路径下去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares</span><br><span class="line"></span><br><span class="line">tar -zxvf cloudera-manager-el6-cm5.14.0_x86_64.tar.gz -C /opt/</span><br></pre></td></tr></table></figure>

<h2 id="第三步：将我们的parcel包的三个文件拷贝到对应路径"><a href="#第三步：将我们的parcel包的三个文件拷贝到对应路径" class="headerlink" title="第三步：将我们的parcel包的三个文件拷贝到对应路径"></a>第三步：将我们的parcel包的三个文件拷贝到对应路径</h2><p>将我们的parcel包含三个文件，拷贝到/opt/cloudera/parcel-repo路径下面去，并记得有个文件需要重命名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares/</span><br><span class="line"></span><br><span class="line">cp CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha1 manifest.json  /opt/cloudera/parcel-repo/</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image013.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image013.jpg" alt="img"></a></p>
<p>重命名标黄的这个文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/cloudera/parcel-repo/</span><br><span class="line"></span><br><span class="line">mv CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha1 CDH-5.14.0-1.cdh5.14.0.p0.24-el6.parcel.sha</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image015.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image015.jpg" alt="img"></a></p>
<h2 id="第四步：所有节点添加普通用户并给与sudo权限"><a href="#第四步：所有节点添加普通用户并给与sudo权限" class="headerlink" title="第四步：所有节点添加普通用户并给与sudo权限"></a>第四步：所有节点添加普通用户并给与sudo权限</h2><p>在node01机器上面添加普通用户并赋予sudo权限</p>
<p>执行以下命令创建普通用户cloudera-scm</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd --system --home=/opt/cm-5.14.0/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scm</span><br></pre></td></tr></table></figure>

<p>赋予cloudera-scm普通用户的sudo权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sudoers</span><br><span class="line"></span><br><span class="line">cloudera-scm ALL=(ALL) NOPASSWD: ALL</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image017.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image017.jpg" alt="img"></a></p>
<h2 id="第五步：更改主节点的配置文件"><a href="#第五步：更改主节点的配置文件" class="headerlink" title="第五步：更改主节点的配置文件"></a>第五步：更改主节点的配置文件</h2><p>node01机器上面更改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /opt/cm-5.14.0/etc/cloudera-scm-agent/config.ini</span><br><span class="line"></span><br><span class="line">server_host=node01.hadoop.com</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image019.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image019.jpg" alt="img"></a></p>
<h2 id="第六步：将-opt目录下的安装包发放到其他机器"><a href="#第六步：将-opt目录下的安装包发放到其他机器" class="headerlink" title="第六步：将/opt目录下的安装包发放到其他机器"></a>第六步：将/opt目录下的安装包发放到其他机器</h2><p>将第一台机器的安装包发放到其他机器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line"></span><br><span class="line">scp -r cloudera/ cm-5.14.0/ node02:/opt</span><br></pre></td></tr></table></figure>

<h2 id="第七步：创建一些数据库备用"><a href="#第七步：创建一些数据库备用" class="headerlink" title="第七步：创建一些数据库备用"></a>第七步：创建一些数据库备用</h2><p>node02机器上面创建数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive 数据库</span><br><span class="line"></span><br><span class="line">create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">集群监控数据库</span><br><span class="line"></span><br><span class="line">create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">hue 数据库</span><br><span class="line"></span><br><span class="line">create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line"></span><br><span class="line">oozie 数据库</span><br><span class="line"></span><br><span class="line">create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br></pre></td></tr></table></figure>

<h2 id="第八步：准备数据库连接的驱动包"><a href="#第八步：准备数据库连接的驱动包" class="headerlink" title="第八步：准备数据库连接的驱动包"></a>第八步：准备数据库连接的驱动包</h2><p>在所有机器上面都准备一份数据库的连接驱动jar包放到/usr/share/java路径下</p>
<p>准备一份mysql的驱动连接包，放到/usr/share/java路径下去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares/</span><br><span class="line"></span><br><span class="line">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf mysql-connector-java-5.1.45.tar.gz</span><br><span class="line"></span><br><span class="line">cd /export/softwares/mysql-connector-java-5.1.45</span><br><span class="line"></span><br><span class="line">cp mysql-connector-java-5.1.45-bin.jar /usr/share/java/mysql-connector-java.jar</span><br></pre></td></tr></table></figure>

<p>拷贝驱动包到第二台机器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/share/java</span><br><span class="line"></span><br><span class="line">scp mysql-connector-java.jar node02:$PWD</span><br></pre></td></tr></table></figure>

<h2 id="第九步：为clouderaManager创建数据库"><a href="#第九步：为clouderaManager创建数据库" class="headerlink" title="第九步：为clouderaManager创建数据库"></a>第九步：为clouderaManager创建数据库</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/cm-5.14.0/share/cmf/schema/scm_prepare_database.sh mysql -hnode02  -uroot -p123456 --scm-host node01 scm root 123456</span><br></pre></td></tr></table></figure>

<p>命令说明：/<strong>opt</strong>/<strong>cm</strong>-5.14.0/share/cmf/schema/scm_prepare_database.<strong>sh</strong> 数据库类型 -h数据库主机 –u数据库用户名 –p数据库密码 –scm-host <strong>cm</strong>主机 数据库名称 用户名 密码</p>
<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image021.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image021.jpg" alt="img"></a></p>
<h2 id="第十步：启动服务"><a href="#第十步：启动服务" class="headerlink" title="第十步：启动服务"></a>第十步：启动服务</h2><p>主节点启动clouderaManager Server与ClouderaManager agent服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/cm-5.14.0/etc/init.d/cloudera-scm-server start</span><br><span class="line"></span><br><span class="line">/opt/cm-5.14.0/etc/init.d/cloudera-scm-agent start</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/ClouderaManager/clip_image023.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/ClouderaManager/clip_image023.jpg" alt="img"></a></p>
<p>从节点node02启动ClouderaManager agent服务</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/ClouderaManager/" data-id="cjz2c0w8d000aagu50vtyfoqk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-KafKa" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/KafKa/" class="article-date">
  <time datetime="2019-08-08T03:38:55.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/KafKa/">KafKa</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Kafka消息队列"><a href="#Kafka消息队列" class="headerlink" title="Kafka消息队列"></a>Kafka消息队列</h1><h2 id="一消息队列概述"><a href="#一消息队列概述" class="headerlink" title="一消息队列概述"></a>一消息队列概述</h2><h3 id="1-kafka企业级消息系统kafka企业级消息系统"><a href="#1-kafka企业级消息系统kafka企业级消息系统" class="headerlink" title="1 kafka企业级消息系统kafka企业级消息系统"></a>1 kafka企业级消息系统kafka企业级消息系统</h3><p><strong>为何使用消息系统</strong></p>
<p>在没有使用消息系统以前，我们对于传统许多业务，以及跨服务器传递消息的时候，会采用串行方式或者并行方法；</p>
<p>与串行的差别是并行的方式可以缩短程序整体处理的时间。</p>
<p><strong>消息系统:</strong></p>
<p>消息系统负责将数据从一个应用程序传送到另一个应用程序，因此应用程序可以专注于数据，但是不必担心 如何共享它。分布式消息系统基于可靠的消息队列的概念。消息在客户端应用程序和消息传递系统之间的异步排队。</p>
<p>有两种类型的消息模式可用 点对点；发布-订阅消息系统</p>
<p>点对点消息系统中，消息被保留在队列中，一个或者多个消费者可以消费队列中的消息，但是特定的消 息只能有最多的一个消费者消费。一旦消费者读取队列中的消息，他就从该队列中消失。该系统的典型应用就是订单处理系统，其中每个订单将有一个订单处理器处理，但多个订单处理器可以同时工作。</p>
<p>大多数的消息系统是基于发布-订阅消息系统</p>
<p><strong>分类</strong></p>
<h2 id="2-1、点对点"><a href="#2-1、点对点" class="headerlink" title="2.1、点对点"></a>2.1、点对点</h2><p>主要采用的队列的方式，如A-&gt;B 当B消费的队列中的数据，那么队列的数据就会被删除掉【如果B不消费那么就会存在队列中有很多的脏数据】</p>
<h2 id="2-2、发布-订阅"><a href="#2-2、发布-订阅" class="headerlink" title="2.2、发布-订阅"></a>2.2、发布-订阅</h2><p>发布与订阅主要三大组件</p>
<p>主题：一个消息的分类</p>
<p>发布者：将消息通过主动推送的方式推送给消息系统；</p>
<p>订阅者：可以采用拉、推的方式从消息系统中获取数据</p>
<p><strong>应用场景</strong></p>
<h2 id="3-1、应用解耦"><a href="#3-1、应用解耦" class="headerlink" title="3.1、应用解耦"></a>3.1、应用解耦</h2><p>将一个大型的任务系统分成若干个小模块，将所有的消息进行统一的管理和存储，因此为了解耦，就会涉及到kafka企业级消息平台</p>
<h2 id="3-2、流量控制"><a href="#3-2、流量控制" class="headerlink" title="3.2、流量控制"></a>3.2、流量控制</h2><p>秒杀活动当中，一般会因为流量过大，应用服务器挂掉，为了解决这个问题，一般需要在应用前端加上消息队列以控制访问流量。</p>
<p>1、 可以控制活动的人数 可以缓解短时间内流量大使得服务器崩掉</p>
<p>2、 可以通过队列进行数据缓存，后续再进行消费处理</p>
<h2 id="3-3、日志处理"><a href="#3-3、日志处理" class="headerlink" title="3.3、日志处理"></a>3.3、日志处理</h2><p>日志处理指将消息队列用在日志处理中，比如kafka的应用中，解决大量的日志传输问题；</p>
<p>日志采集工具采集 数据写入kafka中；kafka消息队列负责日志数据的接收，存储，转发功能；</p>
<p>日志处理应用程序：订阅并消费 kafka队列中的数据，进行数据分析。</p>
<h2 id="3-4、消息通讯"><a href="#3-4、消息通讯" class="headerlink" title="3.4、消息通讯"></a>3.4、消息通讯</h2><p>消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯，比如点对点的消息队列，或者聊天室等。</p>
<h1 id="二-kafka概述"><a href="#二-kafka概述" class="headerlink" title="二 kafka概述"></a>二 kafka概述</h1><p>kafka是最初由linkedin公司开发的，使用scala语言编写，kafka是一个分布式，分区的，多副本的，多订阅者的日志系统（分布式MQ系统），可以用于搜索日志，监控日志，访问日志等。</p>
<p>kafka目前支持多种客户端的语言：java、python、c++、php等</p>
<p>apache kafka是一个分布式发布-订阅消息系统和一个强大的队列，可以处理大量的数据，并使能够将消息从一个端点传递到另一个端点，kafka适合离线和在线消息消费。kafka消息保留在磁盘上，并在集群内复制以防止数据丢失。kafka构建在zookeeper同步服务之上。它与apache和spark非常好的集成，应用于实时流式数据分析。</p>
<p>其他消息队列:</p>
<p>RabbitMQ</p>
<p>Redis</p>
<p>ZeroMQ</p>
<p>ActiveMQ</p>
<p><strong>kafka好处:</strong></p>
<p>可靠性：分布式的，分区，复制和容错的。</p>
<p>可扩展性：kafka消息传递系统轻松缩放，无需停机。</p>
<p>耐用性：kafka使用分布式提交日志，这意味着消息会尽可能快速的保存在磁盘上，因此它是持久的。</p>
<p>性能：kafka对于发布和定于消息都具有高吞吐量。即使存储了许多TB的消息，他也爆出稳定的性能。</p>
<p>kafka非常快：保证零停机和零数据丢失。</p>
<p><strong>应用场景</strong></p>
<h2 id="5-1、指标分析"><a href="#5-1、指标分析" class="headerlink" title="5.1、指标分析"></a>5.1、指标分析</h2><p>kafka 通常用于操作监控数据。这设计聚合来自分布式应用程序的统计信息， 以产生操作的数据集中反馈</p>
<h2 id="5-2、日志聚合解决方法"><a href="#5-2、日志聚合解决方法" class="headerlink" title="5.2、日志聚合解决方法"></a>5.2、日志聚合解决方法</h2><p>kafka可用于跨组织从多个服务器收集日志，并使他们以标准的合适提供给多个服务器。</p>
<h2 id="5-3、流式处理"><a href="#5-3、流式处理" class="headerlink" title="5.3、流式处理"></a>5.3、流式处理</h2><p>流式处理框架（spark，storm，ﬂink）重主题中读取数据，对齐进行处理，并将处理后的数据写入新的主题，供 用户和应用程序使用，kafka的强耐久性在流处理的上下文中也非常的有用。</p>
<h1 id="三架构"><a href="#三架构" class="headerlink" title="三架构"></a>三架构</h1><p><a href="https://manzhong.github.io/images/kafka/jg.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kafka/jg.jpg" alt="img"></a></p>
<p>四大核心:</p>
<h5 id="生产者API"><a href="#生产者API" class="headerlink" title="生产者API"></a>生产者API</h5><p>允许应用程序发布记录流至一个或者多个kafka的主题（topics）。</p>
<h5 id="消费者API"><a href="#消费者API" class="headerlink" title="消费者API"></a>消费者API</h5><p>允许应用程序订阅一个或者多个主题，并处理这些主题接收到的记录流。</p>
<h5 id="StreamsAPI"><a href="#StreamsAPI" class="headerlink" title="StreamsAPI"></a>StreamsAPI</h5><p>允许应用程序充当流处理器（stream processor），从一个或者多个主题获取输入流，并生产一个输出流到一个或 者多个主题，能够有效的变化输入流为输出流。</p>
<h5 id="ConnectorAPI"><a href="#ConnectorAPI" class="headerlink" title="ConnectorAPI"></a>ConnectorAPI</h5><p>允许构建和运行可重用的生产者或者消费者，能够把kafka主题连接到现有的应用程序或数据系统。例如：一个连 接到关系数据库的连接器可能会获取每个表的变化。</p>
<p><strong>架构关系图</strong></p>
<p><a href="https://manzhong.github.io/images/kafka/jgg.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kafka/jgg.jpg" alt="img"></a></p>
<p>说明：kafka支持消息持久化，消费端为拉模型来拉取数据，消费状态和订阅关系有客户端负责维护，消息消费完 后，不会立即删除，会保留历史消息。因此支持多订阅时，消息只会存储一份就可以了</p>
<p><strong>整体架构</strong></p>
<p><a href="https://manzhong.github.io/images/kafka/ztjg.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kafka/ztjg.jpg" alt="img"></a></p>
<p>一个典型的kafka集群中包含若干个Producer，若干个Broker，若干个Consumer，以及一个zookeeper集群； kafka通过zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行Rebalance（负载均 衡）；Producer使用push模式将消息发布到Broker；Consumer使用pull模式从Broker中订阅并消费消息。</p>
<p><strong>kafka术语介绍</strong></p>
<p><strong>Broker</strong>：kafka集群中包含一个或者多个服务实例，这种服务实例被称为Broker</p>
<p><strong>Topic</strong>：每条发布到kafka集群的消息都有一个类别，这个类别就叫做Topic</p>
<p><strong>Partition</strong>：Partition是一个物理上的概念，每个Topic包含一个或者多个Partition</p>
<p><strong>Producer</strong>：负责发布消息到kafka的Broker中。</p>
<p><strong>Consumer</strong>：消息消费者,向kafka的broker中读取消息的客户端</p>
<p><strong>Consumer Group</strong>：每一个Consumer属于一个特定的Consumer Group（可以为每个Consumer指定 groupName）</p>
<p><strong>kafka中topic说明</strong></p>
<p>1,kafka将消息以topic为单位进行归类</p>
<p>2,topic特指kafka处理的消息源（feeds of messages）的不同分类。</p>
<p>3.topic是一种分类或者发布的一些列记录的名义上的名字。kafka主题始终是支持多用户订阅的；也就是说，一 个主题可以有零个，一个或者多个消费者订阅写入的数据。</p>
<p>4.在kafka集群中，可以有无数的主题。</p>
<p>5.生产者和消费者消费数据一般以主题为单位。更细粒度可以到分区级别。</p>
<p><strong>kafka中分区数</strong></p>
<p>Partitions：分区数：控制topic将分片成多少个log，可以显示指定，如果不指定则会使用 broker（server.properties）中的num.partitions配置的数量。</p>
<p>一个broker服务下，是否可以创建多个分区？</p>
<p>可以的，broker数与分区数没有关系； 在kafka中，每一个分区会有一个编号：编号从0开始</p>
<p>某一个分区的数据是有序的</p>
<p>如何保证一个主题是有序的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一个主题下面只有一个分区即可</span><br></pre></td></tr></table></figure>

<p>topic的Partition数量在创建topic时配置。</p>
<p>Partition数量决定了每个Consumer group中并发消费者的最大数量。</p>
<p>Consumer group A 有两个消费者来读取4个partition中数据；Consumer group B有四个消费者来读取4个 partition中的数据</p>
<p><a href="https://manzhong.github.io/images/kafka/fq.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kafka/fq.jpg" alt="img"></a></p>
<p><strong>kafka中的副本数</strong></p>
<p><a href="https://manzhong.github.io/images/kafka/fb.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/kafka/fb.jpg" alt="img"></a></p>
<p>kafka分区副本数（kafka Partition Replicas)</p>
<p>副本数（replication-factor）：控制消息保存在几个broker（服务器）上，一般情况下等于broker的个数</p>
<p>一个broker服务下，是否可以创建多个副本因子？</p>
<p> 不可以；创建主题时，副本因子应该小于等于可用的broker数。</p>
<p>副本因子操作以分区为单位的。每个分区都有各自的主副本和从副本；主副本叫做leader，从副本叫做 follower（在有多个副本的情况下，kafka会为同一个分区下的分区，设定角色关系：一个leader和N个 follower），处于同步状态的副本叫做<strong>in-sync-replicas</strong>(ISR);follower通过拉的方式从leader同步数据。消费 者和生产者都是从leader读写数据，不与follower交互。</p>
<p><strong>副本因子的作用</strong>：让kafka读取数据和写入数据时的可靠性.</p>
<p>副本因子是包含本身|同一个副本因子不能放在同一个Broker中。</p>
<p>如果某一个分区有三个副本因子，就算其中一个挂掉，那么只会剩下的两个钟，选择一个leader，但不会在其 他的broker中，另启动一个副本（因为在另一台启动的话，存在数据传递，只要在机器之间有数据传递，就 会长时间占用网络IO，kafka是一个高吞吐量的消息系统，这个情况不允许发生）所以不会在零个broker中启 动。</p>
<p>如果所有的副本都挂了，生产者如果生产数据到指定分区的话，将写入不成功。</p>
<p>lsr表示：当前可用的副本</p>
<p><strong>kafka的partition offset</strong></p>
<p>任何发布到此partition的消息都会被直接追加到log文件的尾部，每条消息在文件中的位置称为oﬀset（偏移量），</p>
<p>oﬀset是一个long类型数字，它唯一标识了一条消息，消费者通过（oﬀset，partition，topic）跟踪记录。</p>
<p><strong>kafka分区与消费组之间的关系</strong></p>
<p>消费组： 由一个或者多个消费者组成，同一个组中的消费者对于同一条消息只消费一次。</p>
<p>某一个主题下的分区数，对于消费组来说，应该小于等于该主题下的分区数。如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如：某一个主题有4个分区，那么消费组中的消费者应该小于4，而且最好与分区数成整数倍</span><br><span class="line">1	2	4</span><br><span class="line">同一个分区下的数据，在同一时刻，不能同一个消费组的不同消费者消费</span><br></pre></td></tr></table></figure>

<p>总结：分区数越多，同一时间可以有越多的消费者来进行消费，消费数据的速度就会越快，提高消费的性能</p>
<h1 id="四-集群搭建"><a href="#四-集群搭建" class="headerlink" title="四 集群搭建"></a>四 集群搭建</h1><h2 id="1-jdk-与zookeeper必须安装"><a href="#1-jdk-与zookeeper必须安装" class="headerlink" title="1 jdk 与zookeeper必须安装"></a>1 jdk 与zookeeper必须安装</h2><h2 id="2-安装用户"><a href="#2-安装用户" class="headerlink" title="2 安装用户"></a>2 安装用户</h2><p>如默认用户安装则跳过这一步骤</p>
<p>安装hadoop，会创建一个hadoop用户</p>
<p>安装kafka，创建一个kafka用户</p>
<p>或者 创建bigdata用户，用来安装所有的大数据软件</p>
<p>本例：使用root用户来进行安装</p>
<h2 id="3验证环境"><a href="#3验证环境" class="headerlink" title="3验证环境"></a>3验证环境</h2><p>保证三台机器的zk服务都正常启动，且正常运行</p>
<p>查看zk的运行装填，保证有一台zk的服务状态为leader，且两台为follower即可</p>
<h2 id="4下载安装包"><a href="#4下载安装包" class="headerlink" title="4下载安装包"></a>4下载安装包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://archive.apache.org/dist/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz</span><br></pre></td></tr></table></figure>

<h2 id="5上传解压"><a href="#5上传解压" class="headerlink" title="5上传解压"></a>5上传解压</h2><h2 id="6修改配置文件"><a href="#6修改配置文件" class="headerlink" title="6修改配置文件"></a>6修改配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka_2.11-0.10.0.0/config</span><br><span class="line">vim server.properties</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">broker.id=0</span><br><span class="line">num.network.threads=3</span><br><span class="line">num.io.threads=8</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dirs=/export/servers/kafka_2.11-0.10.0.0/logs</span><br><span class="line">num.partitions=2</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line">transaction.state.log.replication.factor=1</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line">log.flush.interval.messages=10000</span><br><span class="line">log.flush.interval.ms=1000</span><br><span class="line">log.retention.hours=168</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">zookeeper.connect=node01:2181,node02:2181,node03:2181</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br><span class="line">delete.topic.enable=true</span><br><span class="line">host.name=node01            //每台主机不一样</span><br></pre></td></tr></table></figure>

<p>创建数据文件存放目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p  /export/servers/kafka_2.11-0.10.0.0/logs</span><br></pre></td></tr></table></figure>

<p>分发安装包:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/</span><br><span class="line">scp -r kafka_2.11-0.10.0.0/ node02:$PWD</span><br><span class="line">scp -r kafka_2.11-0.10.0.0/ node03:$PWD</span><br></pre></td></tr></table></figure>

<p>node02修改:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka_2.11-0.10.0.0/config</span><br><span class="line">vim server.properties</span><br><span class="line"></span><br><span class="line">broker.id=1</span><br><span class="line">num.network.threads=3</span><br><span class="line">num.io.threads=8</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dirs=/export/servers/kafka_2.11-0.10.0.0/logs</span><br><span class="line">num.partitions=2</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line">transaction.state.log.replication.factor=1</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line">log.flush.interval.messages=10000</span><br><span class="line">log.flush.interval.ms=1000</span><br><span class="line">log.retention.hours=168</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">zookeeper.connect=node01:2181,node02:2181,node03:2181</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br><span class="line">delete.topic.enable=true</span><br><span class="line">host.name=node02</span><br></pre></td></tr></table></figure>

<p>node03修改</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka_2.11-0.10.0.0/config</span><br><span class="line">vim server.properties</span><br><span class="line"></span><br><span class="line">broker.id=2</span><br><span class="line">num.network.threads=3</span><br><span class="line">num.io.threads=8</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dirs=/export/servers/kafka_2.11-0.10.0.0/logs</span><br><span class="line">num.partitions=2</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line">transaction.state.log.replication.factor=1</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line">log.flush.interval.messages=10000</span><br><span class="line">log.flush.interval.ms=1000</span><br><span class="line">log.retention.hours=168</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">zookeeper.connect=node01:2181,node02:2181,node03:2181</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br><span class="line">delete.topic.enable=true</span><br><span class="line">host.name=node03</span><br></pre></td></tr></table></figure>

<p>启动集群:</p>
<p><strong>注意事项</strong>：在kafka启动前，一定要让zookeeper启动起来。</p>
<p>前台启动:</p>
<p>node01服务器执行以下命令来启动kafka集群</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka_2.11-0.10.0.0</span><br><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>

<p>后台启动:</p>
<p><strong>node01**</strong>执行以下命令将kafka进程启动在后台**</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka_2.11-0.10.0.0</span><br><span class="line">nohup bin/kafka-server-start.sh config/server.properties &gt;/export/log/kafka.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>停止命令:</p>
<p>node01执行以下命令便可以停止kakfa进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka_2.11-0.10.0.0</span><br><span class="line">bin/kafka-server-stop.sh</span><br></pre></td></tr></table></figure>

<p>查看启动进程:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<h1 id="五-集群操作"><a href="#五-集群操作" class="headerlink" title="五 集群操作"></a>五 集群操作</h1><p>nohup bin/kafka-server-start.sh config/server.properties 2&gt;&amp;1 &amp;</p>
<h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><p>三分区 两副本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create  --partitions 3 --replication-factor 2 --topic test --zookeeper node01:2181,node02:2181,node03:2181</span><br></pre></td></tr></table></figure>

<h3 id="查看topic"><a href="#查看topic" class="headerlink" title="查看topic"></a>查看topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper node01:2181,node02:2181,node03:2181</span><br></pre></td></tr></table></figure>

<h3 id="生产数据"><a href="#生产数据" class="headerlink" title="生产数据"></a>生产数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test</span><br></pre></td></tr></table></figure>

<h3 id="消费数据"><a href="#消费数据" class="headerlink" title="消费数据"></a>消费数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --from-beginning  --topic test --zookeeper node01:2181,node02:2181,node03:2181</span><br></pre></td></tr></table></figure>

<h3 id="查看topic的一些信息"><a href="#查看topic的一些信息" class="headerlink" title="查看topic的一些信息"></a>查看topic的一些信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe  --topic test --zookeeper node01:2181</span><br></pre></td></tr></table></figure>

<h3 id="修改topic的配置属性"><a href="#修改topic的配置属性" class="headerlink" title="修改topic的配置属性"></a>修改topic的配置属性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper node01:2181 --alter --topic test --config flush.messages=1</span><br></pre></td></tr></table></figure>

<h3 id="删除topic"><a href="#删除topic" class="headerlink" title="删除topic"></a>删除topic</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper node01:2181 --delete --topic test</span><br></pre></td></tr></table></figure>

<h2 id="kafka集群当中JavaAPI操作"><a href="#kafka集群当中JavaAPI操作" class="headerlink" title="kafka集群当中JavaAPI操作"></a>kafka集群当中JavaAPI操作</h2><p>依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt; 0.10.0.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;kafka-streams&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;0.10.0.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">&lt;build&gt;</span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">        &lt;!-- java编译插件 --&gt;</span><br><span class="line">        &lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;3.2&lt;/version&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/plugin&gt;</span><br><span class="line">    &lt;/plugins&gt;</span><br><span class="line">&lt;/build&gt;</span><br></pre></td></tr></table></figure>

<h3 id="kafka集群当中ProducerAPI"><a href="#kafka集群当中ProducerAPI" class="headerlink" title="kafka集群当中ProducerAPI"></a>kafka集群当中ProducerAPI</h3><p>生产者代码:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public class MyProducer &#123;</span><br><span class="line">    /**</span><br><span class="line">     * 实现生产数据到kafka test这个topic里面去</span><br><span class="line">     * @param args</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;node01:9092&quot;);</span><br><span class="line">        props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">        props.put(&quot;retries&quot;, 0);</span><br><span class="line">        props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">        props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">        props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        //获取kafakProducer这个类</span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line">        //使用循环发送消息</span><br><span class="line">        for(int i =0;i&lt;100;i++)&#123;</span><br><span class="line">           // kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(&quot;test&quot;,&quot;mymessage&quot; + i));</span><br><span class="line">            kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(&quot;mypartition&quot;,&quot;mymessage&quot; + i));</span><br><span class="line">        &#125;</span><br><span class="line">        //关闭生产者</span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="kafka集群当中的consumerAPI"><a href="#kafka集群当中的consumerAPI" class="headerlink" title="kafka集群当中的consumerAPI"></a>kafka集群当中的consumerAPI</h3><p>消费者:</p>
<p>自动提交offset：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">public class AutomaticConsumer &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 自动提交offset</span><br><span class="line">     * @param args</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;node01:9092&quot;);</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test_group&quot;);  //消费组</span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);//允许自动提交offset</span><br><span class="line">        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);//每隔多久自动提交offset</span><br><span class="line">        props.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);</span><br><span class="line">        //指定key，value的反序列化类</span><br><span class="line">        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line"></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        //指定消费哪个topic里面的数据</span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(&quot;test&quot;));</span><br><span class="line">        //使用死循环来消费test这个topic里面的数据</span><br><span class="line">        while (true)&#123;</span><br><span class="line">            //这里面是我们所有拉取到的数据</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(1000);</span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                long offset = consumerRecord.offset();</span><br><span class="line">                String value = consumerRecord.value();</span><br><span class="line">                System.out.println(&quot;消息的offset值为&quot;+offset +&quot;消息的value值为&quot;+ value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>手动提交offset：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">public class MannualConsumer &#123;</span><br><span class="line">    /**</span><br><span class="line">     * 实现手动的提交offset</span><br><span class="line">     * @param args</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;node01:9092&quot;);</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test_group&quot;);</span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); //禁用自动提交offset，后期我们手动提交offset</span><br><span class="line">        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span><br><span class="line">        props.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);</span><br><span class="line">        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(&quot;test&quot;));  //订阅test这个topic</span><br><span class="line">        int minBatchSize = 200;  //达到200条进行批次的处理，处理完了之后，提交offset</span><br><span class="line">        List&lt;ConsumerRecord&lt;String, String&gt;&gt; consumerRecordList = new ArrayList&lt;&gt;();//定义一个集合，用于存储我们的ConsumerRecorder</span><br><span class="line">        while (true)&#123;</span><br><span class="line"></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(1000);</span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                consumerRecordList.add(consumerRecord);</span><br><span class="line">            &#125;</span><br><span class="line">            if(consumerRecordList.size() &gt;=  minBatchSize)&#123;</span><br><span class="line">                //如果集合当中的数据大于等于200条的时候，我们批量进行处理</span><br><span class="line">                //将这一批次的数据保存到数据库里面去</span><br><span class="line">                //insertToDb(consumerRecordList);</span><br><span class="line">                System.out.println(&quot;手动提交offset的值&quot;);</span><br><span class="line">                //提交offset，表示这一批次的数据全部都处理完了</span><br><span class="line">               // kafkaConsumer.commitAsync();  //异步提交offset值</span><br><span class="line">                kafkaConsumer.commitSync();//同步提交offset的值</span><br><span class="line">                consumerRecordList.clear();//清空集合当中的数据</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>offset：offset记录了每个分区里面的消息消费到了哪一条，下一次来的时候，我们继续从上一次的记录接着消费</p>
<h3 id="kafka的streamAPI"><a href="#kafka的streamAPI" class="headerlink" title="kafka的streamAPI"></a>kafka的streamAPI</h3><p>需求：使用StreamAPI获取test这个topic当中的数据，然后将数据全部转为大写，写入到test2这个topic当中去</p>
<h4 id="第一步：创建一个topic"><a href="#第一步：创建一个topic" class="headerlink" title="第一步：创建一个topic"></a>第一步：创建一个topic</h4><p>node01服务器使用以下命令来常见一个topic 名称为test2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka_2.11-0.10.0.0/</span><br><span class="line">bin/kafka-topics.sh --create  --partitions 3 --replication-factor 2 --topic test2 --zookeeper node01:2181,node02:2181,node03:2181</span><br></pre></td></tr></table></figure>

<h4 id="第二步：开发StreamAPI"><a href="#第二步：开发StreamAPI" class="headerlink" title="第二步：开发StreamAPI"></a>第二步：开发StreamAPI</h4><p>注意：如果程序启动的时候抛出异常，找不到文件夹的路径，需要我们手动的去创建文件夹的路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public class Stream &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 通过streamAPI实现将数据从test里面读取出来，写入到test2里面去</span><br><span class="line">     * @param args</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        Properties properties = new Properties();</span><br><span class="line">        properties.put(StreamsConfig.APPLICATION_ID_CONFIG,&quot;bigger&quot;);</span><br><span class="line">        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;node01:9092&quot;);</span><br><span class="line">        properties.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());//key的序列化和反序列化的类</span><br><span class="line">        properties.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG,Serdes.String().getClass());</span><br><span class="line">        //获取核心类 KStreamBuilder</span><br><span class="line">        KStreamBuilder kStreamBuilder = new KStreamBuilder();</span><br><span class="line">        //通过KStreamBuilder调用stream方法表示从哪个topic当中获取数据</span><br><span class="line">        //调用mapValues方法，表示将每一行value都给取出来</span><br><span class="line">        //line表示我们取出来的一行行的数据</span><br><span class="line">        //将转成大写的数据，写入到test2这个topic里面去</span><br><span class="line">        kStreamBuilder.stream(&quot;test&quot;).mapValues(line -&gt; line.toString().toUpperCase()).to(&quot;test2&quot;);</span><br><span class="line">        //通过kStreamBuilder可以用于创建KafkaStream  通过kafkaStream来实现流失的编程启动</span><br><span class="line">        KafkaStreams kafkaStreams = new KafkaStreams(kStreamBuilder, properties);</span><br><span class="line">        kafkaStreams.start();  //调用start启动kafka的流 API</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第三步：生产数据"><a href="#第三步：生产数据" class="headerlink" title="第三步：生产数据"></a>第三步：生产数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node01执行以下命令，向test这个topic当中生产数据</span><br><span class="line">cd /export/servers/kafka_2.11-0.10.0.0</span><br><span class="line">bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic test</span><br></pre></td></tr></table></figure>

<p><strong>第四步：消费数据</strong></p>
<p>node02执行一下命令消费test2这个topic当中的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka_2.11-0.10.0.0</span><br><span class="line">bin/kafka-console-consumer.sh --from-beginning  --topic test2 --zookeeper node01:2181,node02:2181,node03:2181</span><br></pre></td></tr></table></figure>

<h2 id="六-kafka原理"><a href="#六-kafka原理" class="headerlink" title="六 kafka原理"></a>六 kafka原理</h2><h3 id="一-生产者"><a href="#一-生产者" class="headerlink" title="一 生产者"></a>一 生产者</h3><p>生产者是一个向kafka Cluster发布记录的客户端；<strong>生产者是线程安全的</strong>，跨线程共享单个生产者实例通常比具有多个实例更快。</p>
<p><strong>必要条件</strong></p>
<p>生产者要进行生产数据到kafka Cluster中，必要条件有以下三个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#1、地址</span><br><span class="line">bootstrap.servers=node01:9092</span><br><span class="line">#2、序列化 key.serializer=org.apache.kafka.common.serialization.StringSerializer value.serializer=org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line">#3、主题（topic） 需要制定具体的某个topic（order）即可。</span><br></pre></td></tr></table></figure>

<p><strong>生产者写数据</strong></p>
<p>流程:</p>
<p>1、总体流程</p>
<p>Producer连接任意活着的Broker，请求指定Topic，Partion的Leader元数据信息，然后直接与对应的Broker直接连接，发布数据</p>
<p>2、开放分区接口(生产者数据分发策略)</p>
<p>2.1、用户可以指定分区函数，使得消息可以根据key，发送到指定的Partition中。</p>
<p>2.2、kafka在数据生产的时候，有一个数据分发策略。默认的情况使用DefaultPartitioner.class类。 这个类中就定义数据分发的策略。</p>
<p>2.3、如果是用户指定了partition，生产就不会调用DefaultPartitioner.partition()方法</p>
<p>2.4、当用户指定key，使用hash算法。如果key一直不变，同一个key算出来的hash值是个固定值。如果是固定 值，这种hash取模就没有意义。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions</span><br></pre></td></tr></table></figure>

<p>2.5、 当用既没有指定partition也没有key。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">The default partitioning strategy:</span><br><span class="line">&lt;ul&gt;</span><br><span class="line">&lt;li&gt;If a partition is specified in the record, use it</span><br><span class="line">&lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key</span><br><span class="line">&lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion</span><br><span class="line">*/</span><br></pre></td></tr></table></figure>

<p>2.6、数据分发策略的时候，可以指定数据发往哪个partition。当ProducerRecord 的构造参数中有partition的时 候，就可以发送到对应partition上。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">public class PartitionProducer &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * kafka生产数据 通过不同的方式，将数据写入到不同的分区里面去</span><br><span class="line">     *</span><br><span class="line">     * @param args</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;node01:9092&quot;);</span><br><span class="line">        props.put(&quot;acks&quot;, &quot;all&quot;);</span><br><span class="line">        props.put(&quot;retries&quot;, 0);</span><br><span class="line">        props.put(&quot;batch.size&quot;, 16384);</span><br><span class="line">        props.put(&quot;linger.ms&quot;, 1);</span><br><span class="line">        props.put(&quot;buffer.memory&quot;, 33554432);</span><br><span class="line">        props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line">        props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);</span><br><span class="line"></span><br><span class="line">        //配置我们自定义分区类</span><br><span class="line">        props.put(&quot;partitioner.class&quot;,&quot;cn.itcast.kafka.partition.MyPartitioner&quot;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //获取kafakProducer这个类</span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;&gt;(props);</span><br><span class="line">        //使用循环发送消息</span><br><span class="line">        for(int i =0;i&lt;100;i++)&#123;</span><br><span class="line">            //分区策略第一种，如果既没有指定分区号，也没有指定数据key，那么就会使用轮询的方式将数据均匀的发送到不同的分区里面去</span><br><span class="line">            //ProducerRecord&lt;String, String&gt; producerRecord1 = new ProducerRecord&lt;&gt;(&quot;mypartition&quot;, &quot;mymessage&quot; + i);</span><br><span class="line">            //kafkaProducer.send(producerRecord1);</span><br><span class="line">            //第二种分区策略 如果没有指定分区号，指定了数据key，通过key.hashCode  % numPartitions来计算数据究竟会保存在哪一个分区里面</span><br><span class="line">            //注意：如果数据key，没有变化   key.hashCode % numPartitions  =  固定值  所有的数据都会写入到同一个分区里面去</span><br><span class="line">            //ProducerRecord&lt;String, String&gt; producerRecord2 = new ProducerRecord&lt;&gt;(&quot;mypartition&quot;, &quot;mykey&quot;, &quot;mymessage&quot; + i);</span><br><span class="line">            //kafkaProducer.send(producerRecord2);</span><br><span class="line">            //第三种分区策略：如果指定了分区号，那么就会将数据直接写入到对应的分区里面去</span><br><span class="line">          //  ProducerRecord&lt;String, String&gt; producerRecord3 = new ProducerRecord&lt;&gt;(&quot;mypartition&quot;, 0, &quot;mykey&quot;, &quot;mymessage&quot; + i);</span><br><span class="line">           // kafkaProducer.send(producerRecord3);</span><br><span class="line">            //第四种分区策略：自定义分区策略。如果不自定义分区规则，那么会将数据使用轮询的方式均匀的发送到各个分区里面去</span><br><span class="line">            kafkaProducer.send(new ProducerRecord&lt;String, String&gt;(&quot;mypartition&quot;,&quot;mymessage&quot;+i));</span><br><span class="line">        &#125;</span><br><span class="line">        //关闭生产者</span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>自定义分区策略:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">//对应第四种分区</span><br><span class="line">public class MyPartitioner implements Partitioner &#123;</span><br><span class="line">    /*</span><br><span class="line">    这个方法就是确定数据到哪一个分区里面去</span><br><span class="line">    直接return  2 表示将数据写入到2号分区里面去</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123;</span><br><span class="line">        return 2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void configure(Map&lt;String, ?&gt; configs) &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>总体四种:</p>
<p>a、可根据主题和内容发送</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">//可根据主题和内容发送</span><br><span class="line"></span><br><span class="line">producer.send(new ProducerRecord&lt;String, String&gt;(&quot;my-topic&quot;,&quot;具体的数据&quot;));</span><br></pre></td></tr></table></figure>

<p>b、根据主题，key、内容发送</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">//可根据主题、key、内容发送</span><br><span class="line"></span><br><span class="line">producer.send(new  ProducerRecord&lt;String,  String&gt;(&quot;my-topic&quot;,&quot;key&quot;,&quot;具体的数据&quot;));</span><br></pre></td></tr></table></figure>

<p>c、根据主题、分区、key、内容发送</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">//可根据主题、分区、key、内容发送</span><br><span class="line"></span><br><span class="line">producer.send(new  ProducerRecord&lt;String,  String&gt;(&quot;my-topic&quot;,1,&quot;key&quot;,&quot;具体的数据&quot;));</span><br></pre></td></tr></table></figure>

<p>d、根据主题、分区、时间戳、key，内容发送</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Producer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">//可根据主题、分区、时间戳、key、内容发送</span><br><span class="line">producer.send(new  ProducerRecord&lt;String,  String&gt;(&quot;my-topic&quot;,1,12L,&quot;key&quot;,&quot;具体的数据&quot;));</span><br></pre></td></tr></table></figure>

<p>总结：1如果指定了数据的分区号，那么数据直接生产到对应的分区里面去</p>
<p>2如果没有指定分区好，出现了数据key，通过key取hashCode来计算数据究竟该落在哪一个分区里面</p>
<p>3如果既没有指定分区号，也没有指定数据的key，使用round-robin轮询 的这种机制来是实现</p>
<h3 id="二-消费者"><a href="#二-消费者" class="headerlink" title="二 消费者"></a>二 消费者</h3><p>消费者是一个从kafka Cluster中消费数据的一个客户端；该客户端可以处理kafka brokers中的故障问题，并且可以适应在集群内的迁移的topic分区；该客户端还允许消费者组使用消费者组来进行负载均衡。</p>
<p>消费者维持一个TCP的长连接来获取数据，使用后未能正常关闭这些消费者问题会出现，因此<strong>消费者不是线程安全 的。</strong></p>
<p><strong>必要条件</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#1、地址</span><br><span class="line">bootstrap.servers=node01:9092</span><br><span class="line">#2、序列化 key.serializer=org.apache.kafka.common.serialization.StringSerializer value.serializer=org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line">#3、主题（topic） 需要制定具体的某个topic（order）即可。</span><br><span class="line">#4、消费者组 group.id=test</span><br></pre></td></tr></table></figure>

<p>一 自动提交offset的值(参考上面)</p>
<p>二 手动提交offset的值(参考上面)</p>
<p>三处理完每一个分区里面的数据，就马上提交这个分区里面的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public class ConmsumerPartition &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 处理完每一个分区里面的数据，就马上提交这个分区里面的数据</span><br><span class="line">     * @param args</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;node01:9092&quot;);</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test_group&quot;);</span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;false&quot;); //禁用自动提交offset，后期我们手动提交offset</span><br><span class="line">        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span><br><span class="line">        props.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);</span><br><span class="line">        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(&quot;mypartition&quot;));</span><br><span class="line">        while (true)&#123;</span><br><span class="line">            //通过while ture进行消费数据</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(1000);</span><br><span class="line">            //获取mypartition这个topic里面所有的分区</span><br><span class="line">            Set&lt;TopicPartition&gt; partitions = records.partitions();</span><br><span class="line">            //循环遍历每一个分区里面的数据，然后将每一个分区里面的数据进行处理，处理完了之后再提交每一个分区里面的offset</span><br><span class="line">            for (TopicPartition partition : partitions) &#123;</span><br><span class="line">                //获取每一个分区里面的数据</span><br><span class="line">                List&lt;ConsumerRecord&lt;String, String&gt;&gt; records1 = records.records(partition);</span><br><span class="line">                for (ConsumerRecord&lt;String, String&gt; record : records1) &#123;</span><br><span class="line">                    System.out.println(record.value()+&quot;====&quot;+ record.offset());</span><br><span class="line">                &#125;</span><br><span class="line">                //获取我们分区里面最后一条数据的offset，表示我们已经消费到了这个offset了</span><br><span class="line">                long offset = records1.get(records1.size() - 1).offset();</span><br><span class="line">                //提交offset</span><br><span class="line">                //提交我们的offset，并且给offset加1  表示我们下次从没有消费的那一条数据开始消费</span><br><span class="line">                kafkaConsumer.commitSync(Collections.singletonMap(partition,new OffsetAndMetadata(offset + 1)));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>四 实现消费一个topic里面某些分区里面的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public class ConsumerSomePartition &#123;</span><br><span class="line">    //实现消费一个topic里面某些分区里面的数据</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        Properties props = new Properties();</span><br><span class="line">        props.put(&quot;bootstrap.servers&quot;, &quot;node01:9092,node02:9092,node03:9092&quot;);</span><br><span class="line">        props.put(&quot;group.id&quot;, &quot;test_group&quot;);  //消费组</span><br><span class="line">        props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);//允许自动提交offset</span><br><span class="line">        props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);//每隔多久自动提交offset</span><br><span class="line">        props.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);</span><br><span class="line">        //指定key，value的反序列化类</span><br><span class="line">        props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line">        props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span><br><span class="line"></span><br><span class="line">        //获取kafkaConsumer</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        //通过consumer订阅某一个topic，进行消费.会消费topic里面所有分区的数据</span><br><span class="line">       // consumer.subscribe();</span><br><span class="line"></span><br><span class="line">        //通过调用assign方法实现消费mypartition这个topic里面的0号和1号分区里面的数据</span><br><span class="line"></span><br><span class="line">        TopicPartition topicPartition0 = new TopicPartition(&quot;mypartition&quot;, 0);</span><br><span class="line">        TopicPartition topicPartition1 = new TopicPartition(&quot;mypartition&quot;, 1);</span><br><span class="line">        //订阅我们某个topic里面指定分区的数据进行消费</span><br><span class="line">        consumer.assign(Arrays.asList(topicPartition0,topicPartition1));</span><br><span class="line"></span><br><span class="line">        int i =0;</span><br><span class="line">        while(true)&#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000);</span><br><span class="line">            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                i++;</span><br><span class="line">                System.out.println(&quot;数据值为&quot;+ record.value()+&quot;数据的offset为&quot;+ record.offset());</span><br><span class="line">                System.out.println(&quot;消费第&quot;+i+&quot;条数据&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>五 消费者数据丢失-数据重复</p>
<p>说明：</p>
<p>1、已经消费的数据对于kafka来说，会将消费组里面的oﬀset值进行修改，那什么时候进行修改了？是在数据消费 完成之后，比如在控制台打印完后自动提交；</p>
<p>2、提交过程：是通过kafka将oﬀset进行移动到下个message所处的oﬀset的位置。</p>
<p>3、拿到数据后，存储到hbase中或者mysql中，如果hbase或者mysql在这个时候连接不上，就会抛出异常，如果在处理数据的时候已经进行了提交，那么kafka伤的oﬀset值已经进行了修改了，但是hbase或者mysql中没有数据，这个时候就会出现<strong>数据丢失</strong>。</p>
<p>4、什么时候提交oﬀset值？在Consumer将数据处理完成之后，再来进行oﬀset的修改提交。默认情况下oﬀset是 自动提交，需要修改为手动提交oﬀset值。</p>
<p>5、如果在处理代码中正常处理了，但是在提交oﬀset请求的时候，没有连接到kafka或者出现了故障，那么该次修 改oﬀset的请求是失败的，那么下次在进行读取同一个分区中的数据时，会从已经处理掉的oﬀset值再进行处理一 次，那么在hbase中或者mysql中就会产生两条一样的数据，也就是<strong>数据重复</strong></p>
<p><strong>kafka当中数据消费模型:</strong></p>
<p>eactly once：消费且仅仅消费一次，可以在事务里面执行kafka的操作</p>
<p>at most once：至多消费一次，数据丢失的问题</p>
<p>at least once ：至少消费一次，数据重复消费的问题</p>
<p><strong>kafka的消费模式：决定了offset值保存在哪里:</strong></p>
<p>kafka的highLevel API进行消费：将offset保存在zk当中，每次更新offset的时候，都需要连接zk</p>
<p>以及kafka的lowLevelAP进行消费：保存了消费的状态，其实就是保存了offset，将offset保存在kafka的一个默认的topic里面。kafka会自动的创建一个topic，保存所有其他topic里面的offset在哪里</p>
<p>kafka将数据全部都以文件的方式保存到了文件里面去了。</p>
<p>###三 kafka的log-存储机制</p>
<p>kafka里面一个topic有多个partition组成，每一个partition里面有多个segment组成，每个segment都由两部分组成，分别是.log文件和.index文件 。一旦.log文件达到1GB的时候，就会生成一个新的segment</p>
<p>.log文件：顺序的保存了我们的写入的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">00000000000000000000.log</span><br></pre></td></tr></table></figure>

<p>.index文件：索引文件，使用索引文件，加快kafka数据的查找速度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">00000000000000000000.index</span><br></pre></td></tr></table></figure>

<p><strong>kafka日志的组成:</strong></p>
<p>1 segment ﬁle组成：由两个部分组成，分别为index ﬁle和data ﬁle，此两个文件一一对应且成对出现； 后缀.index和.log分别表示为segment的索引文件、数据文件。</p>
<p>2 segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个全局 partion的最大oﬀset（偏移message数）。数值最大为64位long大小，19位数字字符长度，没有数字就用0 填充</p>
<p>3 通过索引信息可以快速定位到message。通过index元数据全部映射到memory，可以避免segment ﬁle的IO磁盘操作；</p>
<p>4 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 稀疏索引：为了数据创建索引，但范围并不是为每一条创建，而是为某一个区间创建；</p>
<p><strong>好处</strong>：就是可以减少索引值的数量。</p>
<p><strong>不好的地方</strong>：找到索引区间之后，要得进行第二次处理。</p>
<p><strong>kafka日志清理</strong></p>
<p>kafka中清理日志的方式有两种：delete和compact。</p>
<p>删除的阈值有两种：过期的时间和分区内总日志大小。</p>
<p>在kafka中，因为数据是存储在本地磁盘中，并没有像hdfs的那样的分布式存储，就会产生磁盘空间不足的情 况，可以采用删除或者合并的方式来进行处理</p>
<p>可以通过时间来删除、合并：默认7天 还可以通过字节大小、合并</p>
<p><strong>总结：查找数据的过程</strong></p>
<p>第一步：通过offset确定数据保存在哪一个segment里面了，</p>
<p>第二部：查找对应的segment里面的index文件 。index文件都是key/value对的。key表示数据在log文件里面的顺序是第几条。value记录了这一条数据在全局的标号。如果能够直接找到对应的offset直接去获取对应的数据即可</p>
<p>如果index文件里面没有存储offset，就会查找offset最近的那一个offset，例如查找offset为7的数据找不到，那么就会去查找offset为6对应的数据，找到之后，再取下一条数据就是offset为7的数据</p>
<h3 id="四-kafka的消息不丢失机制"><a href="#四-kafka的消息不丢失机制" class="headerlink" title="四 kafka的消息不丢失机制"></a>四 kafka的消息不丢失机制</h3><p>生产者：使用ack机制 有多少个分区，就启动多少个线程来进行同步数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1发送数据的方式:     可以采用同步或者异步的方式      同步：发送一批数据给kafka后，等待kafka返回结果         1、生产者等待10s，如果broker没有给出ack相应，就认为失败。         2、生产者重试3次，如果还没有相应，就报错      异步: 发送一批数据给kafka，只是提供一个回调函数         1、先将数据保存在生产者端的buffer中。buffer大小是2万条          2、满足数据阈值或者数量阈值其中的一个条件就可以发送数据。         3、发送一批数据的大小是500条         说明：如果broker迟迟不给ack，而buﬀer又满了，开发者可以设置是否直接清空buﬀer中的数据。</span><br><span class="line">ask机制   确认机制</span><br><span class="line">服务端返回一个确认码，即ack响应码；ack的响应有三个状态值</span><br><span class="line">   0：生产者只负责发送数据，不关心数据是否丢失，响应的状态码为0（丢失的数据，需要再次发送      ）</span><br><span class="line">   1：partition的leader收到数据，响应的状态码为1</span><br><span class="line">   -1：所有的从节点都收到数据，响应的状态码为-1</span><br><span class="line">   说明：如果broker端一直不给ack状态，producer永远不知道是否成功；producer可以设置一个超时时间10s，超 过时间认为失败。</span><br></pre></td></tr></table></figure>

<p>broker：使用partition的副本机制</p>
<p>消费者：使用offset来进行记录 在消费者消费数据的时候，只要每个消费者记录好oﬀset值即可，就能保证数据不丢失。</p>
<h3 id="五-CAP-理论-与kafka中的CAP"><a href="#五-CAP-理论-与kafka中的CAP" class="headerlink" title="五 CAP 理论 与kafka中的CAP"></a>五 CAP 理论 与kafka中的CAP</h3><p>分布式系统（distributed system）正变得越来越重要，大型网站几乎都是分布式的。</p>
<p>分布式系统的最大难点，就是各个节点的状态如何同步。</p>
<p>为了解决各个节点之间的状态同步问题，在1998年，由加州大学的计算机科学家 Eric Brewer 提出分布式系统的三个指标，分别是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> Consistency：一致性</span><br><span class="line">Availability：可用性</span><br><span class="line">Partition tolerance：分区容错性</span><br><span class="line"></span><br><span class="line">这三个指标不可能同时做到。这个结论就叫做 CAP 定理</span><br></pre></td></tr></table></figure>

<p>1 Partition tolerance</p>
<p>大多数分布式系统都分布在多个子网络。每个子网络就叫做一个区（partition）。分区容错的意思是，区间通信可能失败。比如，一台服务器放在中国，另一台服务器放在美国，这就是两个区，它们之间可能无法通信</p>
<p>一般来说，分区容错无法避免，因此可以认为 CAP 的 P 总是存在的。即永远可能存在分区容错这个问题</p>
<p>2 Consistency</p>
<p>Consistency 中文叫做”一致性”。意思是，写操作之后的读操作，必须返回该值。</p>
<p>如 v0这个数据存在 s1和s2中 用户向s1中 把v0这个值改为v1 则s2中的值也应该改为 v1</p>
<p>3 Availability</p>
<p>Availability 中文叫做”可用性”，意思是只要收到用户的请求，服务器就必须给出回应。</p>
<p>用户可以选择向服务器 G1 或 G2 发起读操作。不管是哪台服务器，只要收到请求，就必须告诉用户，到底是 v0 还是 v1，否则就不满足可用性。</p>
<p><strong>kafka中的CAP 应用</strong></p>
<p>kafka为一个分布式消息队列系统,一定满足CAP 定理</p>
<p> kafka满足的是CAP定律当中的CA，其中Partition tolerance通过的是一定的机制尽量的保证分区容错性。</p>
<p>其中C表示的是数据一致性。A表示数据可用性。</p>
<p>kafka首先将数据写入到不同的分区里面去，每个分区又可能有好多个副本，数据首先写入到leader分区里面去，读写的操作都是与leader分区进行通信，保证了数据的一致性原则，也就是满足了Consistency原则。然后kafka通过分区副本机制，来保证了kafka当中数据的可用性。但是也存在另外一个问题，就是副本分区当中的数据与leader当中的数据存在差别的问题如何解决，这个就是Partition tolerance的问题。</p>
<p><strong>kafka为了解决Partition tolerance的问题，使用了ISR的同步策略，来尽最大可能减少Partition tolerance的问题</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">每个leader会维护一个ISR（a set of in-sync replicas，基本同步）列表</span><br><span class="line">ISR列表主要的作用就是决定哪些副本分区是可用的，也就是说可以将leader分区里面的数据同步到副本分区里面去，决定一个副本分区是否可用的条件有两个</span><br><span class="line">•	replica.lag.time.max.ms=10000     副本分区与主分区心跳时间延迟</span><br><span class="line">•	replica.lag.max.messages=4000    副本分区与主分区消息同步最大差</span><br></pre></td></tr></table></figure>

<p>总结 主分区与副本分区之间的数据同步：</p>
<p>两个指标，一个是副本分区与主分区之间的心跳间隔，超过10S就认为副本分区已经宕机，会将副本分区从ISR当中移除</p>
<p>主分区与副本分区之间的数据同步延迟，默认数据差值是4000条</p>
<p>例如主分区有10000条数据，副本分区同步了3000条，差值是7000 &gt; 4000条，也会将这个副本分区从ISR列表里面移除掉</p>
<p><strong>kafka in zookeeper</strong></p>
<p>kafka集群中：包含了很多的broker，但是在这么的broker中也会有一个老大存在；是在kafka节点中的一个临时节 点，去创建相应的数据，这个老大就是 <strong>Controller</strong> <strong>Broker</strong>。</p>
<p><strong>Controller Broker职责</strong>：管理所有的</p>
<h2 id="kafka-监控及运维"><a href="#kafka-监控及运维" class="headerlink" title="kafka 监控及运维"></a>kafka 监控及运维</h2><p>在开发工作中，消费在Kafka集群中消息，数据变化是我们关注的问题，当业务前提不复杂时，我们可以使用Kafka 命令提供带有Zookeeper客户端工具的工具，可以轻松完成我们的工作。随着业务的复杂性，增加Group和 Topic，那么我们使用Kafka提供命令工具，已经感到无能为力，那么Kafka监控系统目前尤为重要，我们需要观察 消费者应用的细节。</p>
<h2 id="1、kafka-eagle概述"><a href="#1、kafka-eagle概述" class="headerlink" title="1、kafka-eagle概述"></a>1、kafka-eagle概述</h2><p>为了简化开发者和服务工程师维护Kafka集群的工作有一个监控管理工具，叫做 Kafka-eagle。这个管理工具可以很容易地发现分布在集群中的哪些topic分布不均匀，或者是分区在整个集群分布不均匀的的情况。它支持管理多个集群、选择副本、副本重新分配以及创建Topic。同时，这个管理工具也是一个非常好的可以快速浏览这个集群的工具，</p>
<h2 id="2、环境和安装"><a href="#2、环境和安装" class="headerlink" title="2、环境和安装"></a>2、环境和安装</h2><h3 id="1、环境要求"><a href="#1、环境要求" class="headerlink" title="1、环境要求"></a>1、环境要求</h3><p>需要安装jdk，启动zk以及kafka的服务</p>
<h3 id="2、安装步骤"><a href="#2、安装步骤" class="headerlink" title="2、安装步骤"></a>2、安装步骤</h3><h4 id="1、下载源码包"><a href="#1、下载源码包" class="headerlink" title="1、下载源码包"></a>1、下载源码包</h4><p>kafka-eagle官网：</p>
<p><a href="http://download.kafka-eagle.org/" target="_blank" rel="noopener">http://download.kafka-eagle.org/</a></p>
<p>我们可以从官网上面直接下载最细的安装包即可kafka-eagle-bin-1.3.2.tar.gz这个版本即可</p>
<p>代码托管地址：</p>
<p><a href="https://github.com/smartloli/kafka-eagle/releases" target="_blank" rel="noopener">https://github.com/smartloli/kafka-eagle/releases</a></p>
<h4 id="2、解压"><a href="#2、解压" class="headerlink" title="2、解压"></a>2、解压</h4><p>这里我们选择将kafak-eagle安装在第三台</p>
<p>直接将kafka-eagle安装包上传到node03服务器的/export/softwares路径下，然后进行解压</p>
<p>node03服务器执行一下命令进行解压</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/softwares/</span><br><span class="line">tar -zxf kafka-eagle-bin-1.3.2.tar.gz -C /export/servers/</span><br><span class="line">cd /export/servers/kafka-eagle-bin-1.3.2</span><br><span class="line">tar -zxf kafka-eagle-web-1.3.2-bin.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="3、准备数据库"><a href="#3、准备数据库" class="headerlink" title="3、准备数据库"></a>3、准备数据库</h4><p>kafka-eagle需要使用一个数据库来保存一些元数据信息，我们这里直接使用msyql数据库来保存即可，在node03服务器执行以下命令创建一个mysql数据库即可</p>
<p>进入mysql客户端</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line">create database eagle;</span><br><span class="line"></span><br><span class="line">在这个库中创建表:</span><br><span class="line">/*</span><br><span class="line"> Navicat MySQL Data Transfer</span><br><span class="line"></span><br><span class="line"> Source Server         : local</span><br><span class="line"> Source Server Version : 50616</span><br><span class="line"> Source Host           : localhost</span><br><span class="line"> Source Database       : ke</span><br><span class="line"></span><br><span class="line"> Target Server Version : 50616</span><br><span class="line"> File Encoding         : utf-8</span><br><span class="line"></span><br><span class="line"> Date: 06/23/2017 17:02:12 PM</span><br><span class="line">*/</span><br><span class="line">use eagle;</span><br><span class="line">SET NAMES utf8;</span><br><span class="line">SET FOREIGN_KEY_CHECKS = 0;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Table structure for `ke_p_role`</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `ke_p_role`;</span><br><span class="line">CREATE TABLE `ke_p_role` (</span><br><span class="line">  `id` tinyint(4) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(64) CHARACTER SET utf8 NOT NULL COMMENT &apos;role name&apos;,</span><br><span class="line">  `seq` tinyint(4) NOT NULL COMMENT &apos;rank&apos;,</span><br><span class="line">  `description` varchar(128) CHARACTER SET utf8 NOT NULL COMMENT &apos;role describe&apos;,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Records of `ke_p_role`</span><br><span class="line">-- ----------------------------</span><br><span class="line">BEGIN;</span><br><span class="line">INSERT INTO `ke_p_role` VALUES (&apos;1&apos;, &apos;Administrator&apos;, &apos;1&apos;, &apos;Have all permissions&apos;), (&apos;2&apos;, &apos;Devs&apos;, &apos;2&apos;, &apos;Own add or delete&apos;), (&apos;3&apos;, &apos;Tourist&apos;, &apos;3&apos;, &apos;Only viewer&apos;);</span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Table structure for `ke_resources`</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `ke_resources`;</span><br><span class="line">CREATE TABLE `ke_resources` (</span><br><span class="line">  `resource_id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `name` varchar(255) CHARACTER SET utf8 NOT NULL COMMENT &apos;resource name&apos;,</span><br><span class="line">  `url` varchar(255) NOT NULL,</span><br><span class="line">  `parent_id` int(11) NOT NULL,</span><br><span class="line">  PRIMARY KEY (`resource_id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=17 DEFAULT CHARSET=utf8;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Records of `ke_resources`</span><br><span class="line">-- ----------------------------</span><br><span class="line">BEGIN;</span><br><span class="line">INSERT INTO `ke_resources` VALUES (&apos;1&apos;, &apos;System&apos;, &apos;/system&apos;, &apos;-1&apos;), (&apos;2&apos;, &apos;User&apos;, &apos;/system/user&apos;, &apos;1&apos;), (&apos;3&apos;, &apos;Role&apos;, &apos;/system/role&apos;, &apos;1&apos;), (&apos;4&apos;, &apos;Resource&apos;, &apos;/system/resource&apos;, &apos;1&apos;), (&apos;5&apos;, &apos;Notice&apos;, &apos;/system/notice&apos;, &apos;1&apos;), (&apos;6&apos;, &apos;Topic&apos;, &apos;/topic&apos;, &apos;-1&apos;), (&apos;7&apos;, &apos;Message&apos;, &apos;/topic/message&apos;, &apos;6&apos;), (&apos;8&apos;, &apos;Create&apos;, &apos;/topic/create&apos;, &apos;6&apos;), (&apos;9&apos;, &apos;Alarm&apos;, &apos;/alarm&apos;, &apos;-1&apos;), (&apos;10&apos;, &apos;Add&apos;, &apos;/alarm/add&apos;, &apos;9&apos;), (&apos;11&apos;, &apos;Modify&apos;, &apos;/alarm/modify&apos;, &apos;9&apos;), (&apos;12&apos;, &apos;Cluster&apos;, &apos;/cluster&apos;, &apos;-1&apos;), (&apos;13&apos;, &apos;ZkCli&apos;, &apos;/cluster/zkcli&apos;, &apos;12&apos;), (&apos;14&apos;, &apos;UserDelete&apos;, &apos;/system/user/delete&apos;, &apos;1&apos;), (&apos;15&apos;, &apos;UserModify&apos;, &apos;/system/user/modify&apos;, &apos;1&apos;), (&apos;16&apos;, &apos;Mock&apos;, &apos;/topic/mock&apos;, &apos;6&apos;);</span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Table structure for `ke_role_resource`</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `ke_role_resource`;</span><br><span class="line">CREATE TABLE `ke_role_resource` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `role_id` int(11) NOT NULL,</span><br><span class="line">  `resource_id` int(11) NOT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=19 DEFAULT CHARSET=utf8;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Records of `ke_role_resource`</span><br><span class="line">-- ----------------------------</span><br><span class="line">BEGIN;</span><br><span class="line">INSERT INTO `ke_role_resource` VALUES (&apos;1&apos;, &apos;1&apos;, &apos;1&apos;), (&apos;2&apos;, &apos;1&apos;, &apos;2&apos;), (&apos;3&apos;, &apos;1&apos;, &apos;3&apos;), (&apos;4&apos;, &apos;1&apos;, &apos;4&apos;), (&apos;5&apos;, &apos;1&apos;, &apos;5&apos;), (&apos;6&apos;, &apos;1&apos;, &apos;7&apos;), (&apos;7&apos;, &apos;1&apos;, &apos;8&apos;), (&apos;8&apos;, &apos;1&apos;, &apos;10&apos;), (&apos;9&apos;, &apos;1&apos;, &apos;11&apos;), (&apos;10&apos;, &apos;1&apos;, &apos;13&apos;), (&apos;11&apos;, &apos;2&apos;, &apos;7&apos;), (&apos;12&apos;, &apos;2&apos;, &apos;8&apos;), (&apos;13&apos;, &apos;2&apos;, &apos;13&apos;), (&apos;14&apos;, &apos;2&apos;, &apos;10&apos;), (&apos;15&apos;, &apos;2&apos;, &apos;11&apos;), (&apos;16&apos;, &apos;1&apos;, &apos;14&apos;), (&apos;17&apos;, &apos;1&apos;, &apos;15&apos;), (&apos;18&apos;, &apos;1&apos;, &apos;16&apos;);</span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Table structure for `ke_trend`</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `ke_trend`;</span><br><span class="line">CREATE TABLE `ke_trend` (</span><br><span class="line">  `cluster` varchar(64) NOT NULL,</span><br><span class="line">  `key` varchar(64) NOT NULL,</span><br><span class="line">  `value` varchar(64) NOT NULL,</span><br><span class="line">  `hour` varchar(2) NOT NULL,</span><br><span class="line">  `tm` varchar(16) NOT NULL</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Table structure for `ke_user_role`</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `ke_user_role`;</span><br><span class="line">CREATE TABLE `ke_user_role` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `user_id` int(11) NOT NULL,</span><br><span class="line">  `role_id` tinyint(4) NOT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Records of `ke_user_role`</span><br><span class="line">-- ----------------------------</span><br><span class="line">BEGIN;</span><br><span class="line">INSERT INTO `ke_user_role` VALUES (&apos;1&apos;, &apos;1&apos;, &apos;1&apos;);</span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Table structure for `ke_users`</span><br><span class="line">-- ----------------------------</span><br><span class="line">DROP TABLE IF EXISTS `ke_users`;</span><br><span class="line">CREATE TABLE `ke_users` (</span><br><span class="line">  `id` int(11) NOT NULL AUTO_INCREMENT,</span><br><span class="line">  `rtxno` int(11) NOT NULL,</span><br><span class="line">  `username` varchar(64) NOT NULL,</span><br><span class="line">  `password` varchar(128) NOT NULL,</span><br><span class="line">  `email` varchar(64) NOT NULL,</span><br><span class="line">  `realname` varchar(128) NOT NULL,</span><br><span class="line">  PRIMARY KEY (`id`)</span><br><span class="line">) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;</span><br><span class="line"></span><br><span class="line">-- ----------------------------</span><br><span class="line">--  Records of `ke_users`</span><br><span class="line">-- ----------------------------</span><br><span class="line">BEGIN;</span><br><span class="line">INSERT INTO `ke_users` VALUES (&apos;1&apos;, &apos;1000&apos;, &apos;admin&apos;, &apos;123456&apos;, &apos;admin@email.com&apos;, &apos;Administrator&apos;);</span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line">SET FOREIGN_KEY_CHECKS = 1;</span><br></pre></td></tr></table></figure>

<h4 id="4、修改kafak-eagle配置文件"><a href="#4、修改kafak-eagle配置文件" class="headerlink" title="4、修改kafak-eagle配置文件"></a>4、修改kafak-eagle配置文件</h4><p>node03执行以下命令修改kafak-eagle配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka-eagle-bin-1.3.2/kafka-eagle-web-1.3.2/conf</span><br><span class="line">vim system-config.properties</span><br><span class="line"></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1,cluster2</span><br><span class="line">cluster1.zk.list=node01:2181,node02:2181,node03:2181</span><br><span class="line">cluster2.zk.list=node01:2181,node02:2181,node03:2181</span><br><span class="line"></span><br><span class="line">kafka.eagle.driver=com.mysql.jdbc.Driver</span><br><span class="line">kafka.eagle.url=jdbc:mysql://node03:3306/eagle</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=123456</span><br></pre></td></tr></table></figure>

<h4 id="5、配置环境变量"><a href="#5、配置环境变量" class="headerlink" title="5、配置环境变量"></a>5、配置环境变量</h4><p>kafka-eagle必须配置环境变量，node03服务器执行以下命令来进行配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">export KE_HOME=/export/servers/kafka-eagle-bin-1.3.2/kafka-eagle-web-1.3.2</span><br><span class="line">export PATH=:$KE_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="6、启动kafka-eagle"><a href="#6、启动kafka-eagle" class="headerlink" title="6、启动kafka-eagle"></a>6、启动kafka-eagle</h4><p>node03执行以下界面启动kafka-eagle</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/kafka-eagle-bin-1.3.2/kafka-eagle-web-1.3.2/bin</span><br><span class="line">chmod u+x ke.sh</span><br><span class="line">./ke.sh start  stop  status</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/KafKa/" data-id="cjz2c0w9f000tagu5ytyft9kw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/08/Hbase/">Hbase</a>
          </li>
        
          <li>
            <a href="/2019/08/08/Hbase增强/">Hbase增强</a>
          </li>
        
          <li>
            <a href="/2019/08/08/Storm/">Storm</a>
          </li>
        
          <li>
            <a href="/2019/08/08/Scala入门/">Scala入门</a>
          </li>
        
          <li>
            <a href="/2019/08/08/Scala进阶1/">Scala进阶1</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>