<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Linux根目录权限修复方法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Linux根目录权限修复方法/" class="article-date">
  <time datetime="2019-08-08T03:33:23.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Linux根目录权限修复方法/">Linux根目录权限修复方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>今天由于权限问题一般把/这个目录用chmod -R 777执行了一遍，结果各种问题出现，su root就报su:鉴定故障的错误。然后上网找教程很多都要求在root权限下操作来修复，真是悔不当初，哭都哭不出来，只想剁手。幸好最好予以解决了，不然就真得重装系统了，在此把解决方案记录下来，希望能给踩到坑的朋友抢救一下。</p>
<h2 id="step1"><a href="#step1" class="headerlink" title="step1"></a>step1</h2><p>新建一个.c文件，在这里我命名为chmodfix.c，把如下内容写到这个.c文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;                                                                   #include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line">#include &lt;sys/stat.h&gt;</span><br><span class="line">#include &lt;ftw.h&gt;</span><br><span class="line">int list(const char *name, const struct stat *status, int type)</span><br><span class="line">&#123;</span><br><span class="line">    if(type == FTW_NS)</span><br><span class="line">        return 0;</span><br><span class="line">    printf(&quot;%s 0%3o\n&quot;, name, status-&gt;st_mode &amp; 07777);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line">int main(int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">    if(argc == 1)</span><br><span class="line">        ftw(&quot;.&quot;, list, 1); </span><br><span class="line">    else</span><br><span class="line">    ftw(argv[1], list, 2); </span><br><span class="line">    exit(0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后在终端命令行下使用gcc编译得到可执行文件chmodfix.com</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc chmodfix.c -o chmodfix.com</span><br></pre></td></tr></table></figure>

<h2 id="step2"><a href="#step2" class="headerlink" title="step2"></a>step2</h2><p>新建一个.sh文件，在这里我命名为chmodfix.sh，把如下内容写到这个.sh文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh                                                                            if [ $# != 1 ] </span><br><span class="line">then</span><br><span class="line">    echo Usage : $0 \&lt;filename\&gt;</span><br><span class="line">    exit</span><br><span class="line">fi</span><br><span class="line">PERMFILE=$1</span><br><span class="line">cat $PERMFILE | while read LINE</span><br><span class="line">do</span><br><span class="line">    FILE=`echo $LINE | awk &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">    PERM=`echo $LINE | awk &apos;&#123;print $2&#125;&apos;`    </span><br><span class="line">    chmod $PERM $FILE</span><br><span class="line">    #echo &quot;chmod $PERM $FILE&quot;</span><br><span class="line">done</span><br><span class="line">echo &quot;change perm finished! &quot;</span><br></pre></td></tr></table></figure>

<h2 id="step3"><a href="#step3" class="headerlink" title="step3"></a>step3</h2><p>找到另一台装有centos7并且系统权限正常的电脑，利用step1中得到的chmodfix.com从这台电脑上获取被你损坏的目录下所有文件的正常权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 假设原电脑上权限损坏的目录为/usr/bin</span><br><span class="line">./chmodfix.com /usr/bin &gt;&gt; chmodfix.txt</span><br></pre></td></tr></table></figure>

<p>输出文件chmodfix.txt的内容形式如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin 0755 /usr/bin/cp 0755</span><br><span class="line">/usr/bin/lua 0755</span><br><span class="line">/usr/bin/captoinfo 0755</span><br><span class="line">/usr/bin/csplit 0755</span><br><span class="line">/usr/bin/clear 0755</span><br><span class="line">/usr/bin/cut 0755</span><br></pre></td></tr></table></figure>

<p>将得到的权限文件chmodfix.txt复制到权限受损的电脑上</p>
<h2 id="step4"><a href="#step4" class="headerlink" title="step4"></a>step4</h2><p>权限受损电脑进入单用户模式:</p>
<p>CentOS6.x版本<br>单用户模式，就是你现在站在这台机器面前能干的活，再通俗点就是你能够接触到这个物理设备。</p>
<p>一般干这个活的话，基本上是系统出现严重故障或者其他的root密码忘记等等，单用户模式就非常有用了；</p>
<p>1、在开机启动的时候能看到引导目录，用上下方向键选择你忘记密码的那个系统，然后按“e”；</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517153944.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517153944.png" alt="CentOS6/CentOS7进入单用户模式 - 第1张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>2、接下来你可以看到如下图所示的画面，然后你再用上下键选择最新的内核，然后在按“e”；</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154016.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154016.png" alt="CentOS6/CentOS7进入单用户模式 - 第2张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154040.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154040.png" alt="CentOS6/CentOS7进入单用户模式 - 第3张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>3、执行完上步操作后 在rhgb quiet最后加“空格”，然后键入“single”，或者直接输入数字的“1”并回车确定；</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154102.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154102.png" alt="CentOS6/CentOS7进入单用户模式 - 第4张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154155.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154155.png" alt="CentOS6/CentOS7进入单用户模式 - 第5张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>4、按“b”键，重新引导系统；</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154216.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154216.png" alt="CentOS6/CentOS7进入单用户模式 - 第6张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>5、然后就进入了单用户模式下，你就可以使用root功能的东西了，改完你要改的文件后reboot即可。</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154249.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154249.png" alt="CentOS6/CentOS7进入单用户模式 - 第7张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154615.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517154615.png" alt="CentOS6/CentOS7进入单用户模式 - 第8张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>centos7版本采用的是grub2，和centos6.x进入单用户的方法不同。</p>
<p>init方法</p>
<p>1、centos7的grub2界面会有两个入口，正常系统入口和救援模式；</p>
<p>2、修改grub2引导</p>
<p>在正常系统入口上按下”e”，会进入edit模式，搜寻ro那一行，以linux16开头的；</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517145321.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517145321.png" alt="CentOS6/CentOS7进入单用户模式 - 第9张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517145440.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517145440.png" alt="CentOS6/CentOS7进入单用户模式 - 第10张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>把只读更改成可写<br>把ro更改成rw</p>
<p>指定shell环境<br>增加init=/sysroot/bin/sh<br>或init=/sysroot/bin/bash</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517145703.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517145703.png" alt="CentOS6/CentOS7进入单用户模式 - 第11张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517145737.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517145737.png" alt="CentOS6/CentOS7进入单用户模式 - 第12张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>按下ctrl+x来启动系统。</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517150057.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517150057.png" alt="CentOS6/CentOS7进入单用户模式 - 第13张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>3、进入系统以后将/sysroot/设置为根</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chroot /sysroot/</span><br></pre></td></tr></table></figure>

<p>4、做相应的系统维护工作</p>
<p>如：修改密码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">passwd</span><br></pre></td></tr></table></figure>

<p>5、系统启用了selinux，必须运行以下命令，否则将无法正常启动系统：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch /.autorelabel</span><br></pre></td></tr></table></figure>

<p>6、退出并重启系统<br>退出之前设置的根</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br></pre></td></tr></table></figure>

<p>重启系统</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>

<p>另外还有一种rd.break方法</p>
<p>1、启动的时候，在启动界面，相应启动项，内核名称上按“e”；</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517150940.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517150940.png" alt="CentOS6/CentOS7进入单用户模式 - 第14张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>2、进入后，找到linux16开头的地方，按“end”键到最后，输入rd.break，按ctrl+x进入；</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151010.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151010.png" alt="CentOS6/CentOS7进入单用户模式 - 第15张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151045.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151045.png" alt="CentOS6/CentOS7进入单用户模式 - 第16张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>3、进去后输入命令mount，发现根为/sysroot/，并且不能写，只有ro=readonly权限；</p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151129.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151129.png" alt="CentOS6/CentOS7进入单用户模式 - 第17张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151232.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151232.png" alt="CentOS6/CentOS7进入单用户模式 - 第18张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>4、重新挂载，之后mount，发现有了r,w权限；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount -o remount,rw /sysroot/</span><br></pre></td></tr></table></figure>

<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151527.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151527.png" alt="CentOS6/CentOS7进入单用户模式 - 第19张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>5、改变根</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chroot /sysroot/</span><br></pre></td></tr></table></figure>

<p>在/tmp/下创建一个aaa的目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /tmp/aaa</span><br></pre></td></tr></table></figure>

<p>6、系统启用了selinux，必须运行以下命令，否则将无法正常启动系统：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch /.autorelabel</span><br></pre></td></tr></table></figure>

<p>7、退出之前设置的根</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br></pre></td></tr></table></figure>

<p><a href="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151625.png" target="_blank" rel="noopener"><img src="http://laofuxi.com/wp-content/uploads/2016/05/QQ%E5%9B%BE%E7%89%8720160517151625.png" alt="CentOS6/CentOS7进入单用户模式 - 第20张  | 劳福喜博客-专注Linux服务器运维技术"></a></p>
<p>8、重启系统</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>

<h1 id="以上结束后"><a href="#以上结束后" class="headerlink" title="以上结束后"></a>以上结束后</h1><p>然后进入到chmodfix.sh和chmodfix.txt所存放的文件夹下，执行chmodfix.sh以根据chmodfix.txt恢复受损文件的正确权限</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash chmodfix.sh chmodfix.txt</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Linux根目录权限修复方法/" data-id="cjz24s2x00007u8u5z1vpmm34" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Linux常用命令" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Linux常用命令/" class="article-date">
  <time datetime="2019-08-08T03:32:34.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Linux常用命令/">Linux常用命令</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一-目录结构"><a href="#一-目录结构" class="headerlink" title="一 目录结构"></a>一 目录结构</h1><p><a href="https://manzhong.github.io/images/linux/linux%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/linux/linux%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84.png" alt="img"></a></p>
<p>Linux系统它是文件系统。</p>
<p>它的根目录 是”/”,是以树型结构来管理。</p>
<p>Root用户登录后，显示时有一个~,它其实代表的就是root目录</p>
<p>root目录 管理员的</p>
<p>其他用户的在home目录中</p>
<h1 id="二常用命令"><a href="#二常用命令" class="headerlink" title="二常用命令"></a>二常用命令</h1><h2 id="1．-切换目录命令cd："><a href="#1．-切换目录命令cd：" class="headerlink" title="1． 切换目录命令cd："></a>1． 切换目录命令cd：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">使用cd app  切换到app目录</span><br><span class="line">cd ..       切换到上一层目录</span><br><span class="line">cd /        切换到系统根目录</span><br><span class="line">cd~         切换到root用户主目录</span><br><span class="line">cd -        切换到上一个所在目录</span><br></pre></td></tr></table></figure>

<h2 id="2．-列出文件列表：ls-ll-dir"><a href="#2．-列出文件列表：ls-ll-dir" class="headerlink" title="2． 列出文件列表：ls ll dir(*****)"></a>2． 列出文件列表：ls ll dir(*****)</h2><p>ls(list)是一个非常有用的命令，用来显示当前目录下的内容。配合参数的使用，能以不同的方式显示目录内容。 格式：ls[参数] [路径或文件名]</p>
<p>常用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在linux中以 . 开头的文件都是隐藏的文件</span><br><span class="line">* ls</span><br><span class="line">* ls -a  显示所有文件或目录（包含隐藏的文件）</span><br><span class="line">* ls -l  缩写成ll</span><br></pre></td></tr></table></figure>

<h2 id="3．-创建目录和移除目录：mkdir-rmdir"><a href="#3．-创建目录和移除目录：mkdir-rmdir" class="headerlink" title="3． 创建目录和移除目录：mkdir rmdir"></a>3． 创建目录和移除目录：mkdir rmdir</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir(make directory)命令可用来创建子目录。</span><br><span class="line">mkdir app   在当前目录下创建app目录</span><br><span class="line">mkdir –p app2/test  级联创建aap2以及test目  递归创建</span><br><span class="line"></span><br><span class="line">rmdir(remove directory)命令可用来删除“空”的子目录：</span><br><span class="line">rmdir app  删除app目录</span><br></pre></td></tr></table></figure>

<h2 id="4．-浏览文件"><a href="#4．-浏览文件" class="headerlink" title="4． 浏览文件"></a>4． 浏览文件</h2><p>【cat、more、less】</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cat操作：展示所有内容—适合查看小文件</span><br><span class="line">cat a.txt</span><br><span class="line">more与less用法类似 ： 适合查看大文件—翻页查看</span><br><span class="line">more a.log</span><br><span class="line">less b.log</span><br><span class="line">空格 ：查看下一页</span><br><span class="line">Enter ：查看下一行</span><br><span class="line">Q    ：退出</span><br><span class="line">Less 可以使用上下箭头查看行。</span><br><span class="line">tail命令是在实际使用过程中使用非常多的一个命令，它的功能是：用于显示文件后几行的内容。</span><br><span class="line">用法:</span><br><span class="line">tail -10 /etc/passwd  查看后10行数据</span><br><span class="line">tail -f catalina.log  动态查看日志(*****)</span><br><span class="line"></span><br><span class="line">ctrl+c</span><br><span class="line">Ctrl+c 与ctrl+z 的区别</span><br><span class="line"></span><br><span class="line">两者都为中断进程</span><br><span class="line">Ctrl+c  是强制中断程序的执行</span><br><span class="line">ctrl+z  是将任务中断,但任务并未结束,他任然在进程中只是挂起了,用户可以使用fg/bg操作继续前台或后台的任务,fg命令重新启动前台被中断的任务,bg命令把被中断的任务放在后台执行.</span><br></pre></td></tr></table></figure>

<h2 id="5．-文件操作："><a href="#5．-文件操作：" class="headerlink" title="5． 文件操作："></a>5． 文件操作：</h2><p>cp是copy操作</p>
<p>mv它是move相当于剪切</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cp(copy)命令可以将文件从一处复制到另一处。一般在使用cp命令时将一个文件复制成另一个文件或复制到某目录时，需要指定源文件名与目标文件名或目录。</span><br><span class="line">cp a.txt b.txt    将a.txt复制为b.txt文件</span><br><span class="line">cp a.txt ../    将a.txt文件复制到上一层目录中</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mv 移动或者重命名</span><br><span class="line">mv a.txt ../    将a.txt文件移动到上一层目录中</span><br><span class="line">mv a.txt b.txt    将a.txt文件重命名为b.txt</span><br><span class="line">mv a.txt /b/b.txt  将a.txt移动到根目录下的b目录下的b.txt</span><br></pre></td></tr></table></figure>

<p>tar 命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tar命令位于/bin目录下，它能够将用户所指定的文件或目录打包成一个文件，但不做压缩。一般Linux上常用的压缩方式是选用tar将许多文件打包成一个文件，再以gzip压缩命令压缩成xxx.tar.gz(或称为xxx.tgz)的文件。常用参数：-c：创建一个新tar文件-v：显示运行过程的信息-f：指定文件名-z：调用gzip压缩命令进行压缩-t：查看压缩文件的内容-x：解开tar文件</span><br><span class="line"></span><br><span class="line">打包：</span><br><span class="line">tar –cvf xxx.tar ./*</span><br><span class="line">打包并且压缩：</span><br><span class="line">tar –zcvf xxx.tar.gz ./* </span><br><span class="line"></span><br><span class="line">解压 </span><br><span class="line">     tar –xvf xxx.tar</span><br><span class="line">tar -xvf xxx.tar.gz -C /usr/aaa</span><br></pre></td></tr></table></figure>

<p>find 命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">find指令用于查找符合条件的文件</span><br><span class="line">示例：</span><br><span class="line">find / -name “ins*” 查找文件名称是以ins开头的文件</span><br><span class="line">find / -name “ins*” –ls </span><br><span class="line">find / –user itcast –ls 查找用户itcast的文件</span><br><span class="line">find / –user itcast –type d –ls 查找用户itcast的目录</span><br><span class="line">find /-perm -777 –type d-ls 查找权限是777的文件</span><br></pre></td></tr></table></figure>

<p>grep 命令 常与|命令一起使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">查找文件里符合条件的字符串。</span><br><span class="line">用法: grep [选项]... PATTERN [FILE]...示例：</span><br><span class="line">grep lang anaconda-ks.cfg  在文件中查找lang</span><br><span class="line">grep lang anaconda-ks.cfg –color 高亮显示</span><br></pre></td></tr></table></figure>

<h2 id="6．-其他常用命令"><a href="#6．-其他常用命令" class="headerlink" title="6． 其他常用命令"></a>6． 其他常用命令</h2><p>【pwd】</p>
<p>显示当前所在目录</p>
<p>【touch】</p>
<p>创建一个空文件</p>
<p>* touch a.txt</p>
<p>【ll -h】</p>
<p>友好显示文件大小</p>
<p>【wget】</p>
<p>下载资料</p>
<p>* wget <a href="http://nginx.org/download/nginx-1.9.12.tar.gz" target="_blank" rel="noopener">http://nginx.org/download/nginx-1.9.12.tar.gz</a></p>
<h2 id="7vi-和vim-编辑器"><a href="#7vi-和vim-编辑器" class="headerlink" title="7vi 和vim 编辑器"></a>7vi 和vim 编辑器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">在Linux下一般使用vi编辑器来编辑文件。vi既可以查看文件也可以编辑文件。三种模式：命令行、插入、底行模式。</span><br><span class="line">切换到命令行模式：按Esc键；</span><br><span class="line">切换到插入模式：按 i 、o、a键；</span><br><span class="line">    i 在当前位置前插入</span><br><span class="line">    I 在当前行首插入</span><br><span class="line">    a 在当前位置后插入</span><br><span class="line">    A 在当前行尾插入</span><br><span class="line">    o 在当前行之后插入一行</span><br><span class="line">    O 在当前行之前插入一行</span><br><span class="line"></span><br><span class="line">切换到底行模式：按 :（冒号）；更多详细用法，查询文档《Vim命令合集.docx》和《vi使用方法详细介绍.docx》</span><br><span class="line"></span><br><span class="line">打开文件：vim file</span><br><span class="line">退出：esc  :q</span><br><span class="line">修改文件：输入i进入插入模式</span><br><span class="line">保存并退出：esc:wq</span><br><span class="line"></span><br><span class="line">不保存退出：esc:q!</span><br><span class="line"></span><br><span class="line">三种进入插入模式：</span><br><span class="line">i:在当前的光标所在处插入</span><br><span class="line">o:在当前光标所在的行的下一行插入</span><br><span class="line">a:在光标所在的下一个字符插入</span><br><span class="line"></span><br><span class="line">快捷键：</span><br><span class="line">dd – 快速删除一行</span><br><span class="line">yy - 复制当前行</span><br><span class="line">nyy - 从当前行向后复制几行</span><br><span class="line">p - 粘贴</span><br><span class="line">R – 替换</span><br></pre></td></tr></table></figure>

<h2 id="8-重定向输出-gt-gt-gt"><a href="#8-重定向输出-gt-gt-gt" class="headerlink" title="8 重定向输出&gt; &gt;&gt;"></a>8 重定向输出&gt; &gt;&gt;</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;  重定向输出，覆盖原有内容；&gt;&gt; 重定向输出，又追加功能；示例：</span><br><span class="line">cat /etc/passwd &gt; a.txt  将输出定向到a.txt中</span><br><span class="line">cat /etc/passwd &gt;&gt; a.txt  输出并且追加</span><br><span class="line"></span><br><span class="line">ifconfig &gt; ifconfig.txt</span><br></pre></td></tr></table></figure>

<h2 id="9-管道"><a href="#9-管道" class="headerlink" title="9 管道"></a>9 管道</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">管道是Linux命令中重要的一个概念，其作用是将一个命令的输出用作另一个命令的输入。示例</span><br><span class="line">ls --help | more  分页查询帮助信息</span><br><span class="line">ps –ef | grep java  查询名称中包含java的进程</span><br><span class="line"></span><br><span class="line">ifconfig | more</span><br><span class="line">cat index.html | more</span><br><span class="line">ps –ef | grep aio</span><br></pre></td></tr></table></figure>

<h2 id="10-amp-amp-命令执行控制"><a href="#10-amp-amp-命令执行控制" class="headerlink" title="10 &amp;&amp;命令执行控制"></a>10 &amp;&amp;命令执行控制</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">命令之间使用 &amp;&amp; 连接，实现逻辑与的功能。 </span><br><span class="line">只有在 &amp;&amp; 左边的命令返回真（命令返回值 $? == 0），&amp;&amp; 右边的命令才会被执行。 </span><br><span class="line">只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。</span><br><span class="line"></span><br><span class="line">mkdir test &amp;&amp; cd test</span><br></pre></td></tr></table></figure>

<h2 id="11-系统管理命令"><a href="#11-系统管理命令" class="headerlink" title="11 系统管理命令"></a>11 系统管理命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">date 显示或设置系统时间</span><br><span class="line">date  显示当前系统时间</span><br><span class="line">date -s “2014-01-01 10:10:10“  设置系统时间df 显示磁盘信息</span><br><span class="line">df –h  友好显示大小free 显示内存状态</span><br><span class="line">free –m 以mb单位显示内存组昂头top 显示，管理执行中的程序</span><br><span class="line"></span><br><span class="line">clear 清屏幕</span><br><span class="line">ps 正在运行的某个进程的状态</span><br><span class="line">ps –ef  查看所有进程</span><br><span class="line">ps –ef | grep ssh 查找某一进程</span><br><span class="line">kill 杀掉某一进程</span><br><span class="line">kill 2868  杀掉2868编号的进程</span><br><span class="line">kill -9 2868  强制杀死进程</span><br><span class="line"></span><br><span class="line">du 显示目录或文件的大小。</span><br><span class="line">du –h 显示当前目录的大小</span><br><span class="line">who 显示目前登入系统的用户信息。</span><br><span class="line">uname 显示系统信息。</span><br><span class="line">uname -a 显示本机详细信息。依次为：内核名称(类别)，主机名，内核版本号，内核版本，内核编译日期，硬件名，处理器类型，硬件平台类型，操作系统名称</span><br></pre></td></tr></table></figure>

<h2 id="12-用户和组"><a href="#12-用户和组" class="headerlink" title="12 用户和组"></a>12 用户和组</h2><p>1用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">useradd 添加一个用户</span><br><span class="line">useradd test 添加test用户</span><br><span class="line">useradd test -d /home/t1  指定用户home目录</span><br><span class="line">passwd  设置、修改密码</span><br><span class="line">passwd test  为test用户设置密码</span><br><span class="line"></span><br><span class="line">切换登录：</span><br><span class="line">ssh -l test -p 22 192.168.19.128</span><br><span class="line"></span><br><span class="line">su – 用户名</span><br><span class="line">userdel 删除一个用户</span><br><span class="line">userdel test 删除test用户(不会删除home目录)</span><br><span class="line">userdel –r test  删除用户以及home目录</span><br></pre></td></tr></table></figure>

<p>2 组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当在创建一个新用户user时，若没有指定他所属于的组，就建立一个和该用户同名的私有组</span><br><span class="line">创建用户时也可以指定所在组</span><br><span class="line">groupadd  创建组</span><br><span class="line">groupadd public  创建一个名为public的组</span><br><span class="line">useradd u1 –g public  创建用户指定组groupdel 删除组，如果该组有用户成员，必须先删除用户才能删除组。</span><br><span class="line">groupdel public</span><br></pre></td></tr></table></figure>

<p>3 ID su 命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">功能：查看一个用户的UID和GID用法：id [选项]... [用户名]</span><br><span class="line">直接使用id</span><br><span class="line">直接使用id 用户名</span><br><span class="line">功能：切换用户。用法：su [选项]... [-] [用户 [参数]... ]示例：</span><br><span class="line">su u1  切换到u1用户</span><br><span class="line">su - u1 切换到u1用户，并且将环境也切换到u1用户的环境（推荐使用）</span><br><span class="line">/etc/passwd  用户文件/etc/shadow  密码文件/etc/group  组信息文件</span><br><span class="line"></span><br><span class="line">用户文件</span><br><span class="line"></span><br><span class="line">root:x:0:0:root:/root:/bin/bash账号名称：		在系统中是唯一的用户密码：		此字段存放加密口令用户标识码(User ID)：  系统内部用它来标示用户组标识码(Group ID)：   系统内部用它来标识用户属性用户相关信息：		例如用户全名等用户目录：		用户登录系统后所进入的目录用户环境:		用户工作的环境</span><br><span class="line"></span><br><span class="line">密码文件</span><br><span class="line"></span><br><span class="line">shadow文件中每条记录用冒号间隔的9个字段组成.用户名：用户登录到系统时使用的名字，而且是惟一的口令：  存放加密的口令最后一次修改时间:  标识从某一时刻起到用户最后一次修改时间最大时间间隔:  口令保持有效的最大天数，即多少天后必须修改口令最小时间间隔：	再次修改口令之间的最小天数警告时间：从系统开始警告到口令正式失效的天数不活动时间：	口令过期少天后，该账号被禁用失效时间：指示口令失效的绝对天数(从1970年1月1日开始计算)标志：未使用</span><br><span class="line"></span><br><span class="line">组文件</span><br><span class="line"></span><br><span class="line">root:x:0:组名：用户所属组组口令：一般不用GID：组ID用户列表：属于该组的所有用户</span><br></pre></td></tr></table></figure>

<h2 id="13-权限命令"><a href="#13-权限命令" class="headerlink" title="13 权限命令"></a>13 权限命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">drwxr-xr-x.</span><br><span class="line">d 文件类型   rwx </span><br><span class="line">r:对文件是指可读取内容 对目录是可以ls</span><br><span class="line">w:对文件是指可修改文件内容，对目录 是指可以在其中创建或删除子节点(目录或文件)</span><br><span class="line">x:对文件是指是否可以运行这个文件，对目录是指是否可以cd进入这个目录</span><br><span class="line"></span><br><span class="line">rwx 属主</span><br><span class="line">r-x  属组</span><br><span class="line">r-x 其他用户</span><br><span class="line">普通文件： 包括文本文件、数据文件、可执行的二进制程序文件等。 </span><br><span class="line">目录文件： Linux系统把目录看成是一种特殊的文件，利用它构成文件系统的树型结构。  </span><br><span class="line">设备文件： Linux系统把每一个设备都看成是一个文件</span><br><span class="line">普通文件（-）目录（d）符号链接（l）</span><br><span class="line">权限管理</span><br><span class="line">chmod 变更文件或目录的权限。</span><br><span class="line">chmod 755 a.txt </span><br><span class="line">chmod u=rwx,g=rx,o=rx a.txt</span><br><span class="line">chmod 000 a.txt  / chmod 777 a.txtchown 变更文件或目录改文件所属用户和组</span><br><span class="line">chown u1:public a.txt	：变更当前的目录或文件的所属用户和组</span><br><span class="line">chown -R u1:public dir	：变更目录中的所有的子目录及文件的所属用户和组</span><br></pre></td></tr></table></figure>

<h2 id="14-网络操作"><a href="#14-网络操作" class="headerlink" title="14 网络操作"></a>14 网络操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">主机名</span><br><span class="line">hostname 查看主机名</span><br><span class="line">hostname xxx 修改主机名 重启后无效</span><br><span class="line">如果想要永久生效，可以修改/etc/sysconfig/network文件</span><br><span class="line">ip地址</span><br><span class="line"></span><br><span class="line">setup设置ip地址</span><br><span class="line">ifconfig 查看(修改)ip地址(重启后无效)</span><br><span class="line">ifconfig eth0 192.168.12.22 修改ip地址</span><br><span class="line">如果想要永久生效</span><br><span class="line">修改 /etc/sysconfig/network-scripts/ifcfg-eth0文件</span><br><span class="line">域名映射</span><br><span class="line">/etc/hosts文件用于在通过主机名进行访问时做ip地址解析之用</span><br><span class="line">网络服务管理</span><br><span class="line"></span><br><span class="line">service network status 查看网络的状态</span><br><span class="line">service network stop 停止网络服务</span><br><span class="line">service network start 启动网络服务</span><br><span class="line">service network restart 重启网络服务</span><br><span class="line"></span><br><span class="line">service --status–all 查看系统中所有后台服务</span><br><span class="line">netstat –nltp 查看系统中网络进程的端口监听情况</span><br><span class="line"></span><br><span class="line">防火墙设置</span><br><span class="line">防火墙根据配置文件/etc/sysconfig/iptables来控制本机的”出”、”入”网络访问行为。</span><br><span class="line">service iptables status 查看防火墙状态</span><br><span class="line">service iptables stop 关闭防火墙</span><br><span class="line">service iptables start 启动防火墙</span><br><span class="line">chkconfig  iptables off 禁止防火墙自启</span><br><span class="line">mysql服务打开、关闭、查看状态</span><br><span class="line"></span><br><span class="line">service mysqld start、stop、status</span><br></pre></td></tr></table></figure>

<h2 id="15-其他"><a href="#15-其他" class="headerlink" title="15 其他"></a>15 其他</h2><p>yum 安装</p>
<ul>
<li><p>yum install -y telnet</p>
</li>
<li><p>测试机器之间能否通信</p>
<ul>
<li>ping 192.122…</li>
</ul>
</li>
<li><p>测试能否与某个应用（比如mysql）通信</p>
<ul>
<li>telnet 192.123.. 3306</li>
</ul>
</li>
<li><p>查看进程</p>
<ul>
<li>ps -ef</li>
</ul>
</li>
<li><p>过滤相关信息</p>
<ul>
<li>grep</li>
<li>netstat -nltp | grep 3306 查看端口</li>
<li>jps | grep NameNode</li>
<li>cat | grep -v “#”</li>
</ul>
</li>
<li><p>查看文件</p>
<ul>
<li>cat filename</li>
<li>more filename</li>
<li>tail -f/-F/-300f filename 查看文件后300行</li>
<li>head [-number]filename查看文件头</li>
</ul>
</li>
<li><p>节点传送文件</p>
<ul>
<li>scp -r /export/servers/hadoop node02:/export/servers</li>
<li>scp -r /export/servers/hadoop node02:$PWD (发送到当前同级目录)</li>
<li>scp -r /export/servers/hadoop user@node02:/export/servers</li>
</ul>
</li>
<li><p>查看日期</p>
<ul>
<li>date</li>
<li>date +”%Y-%m-%d %H:%M:%S”</li>
<li>date -d “-1 day” +”%Y-%m-%d %H:%M:%S”</li>
</ul>
</li>
<li><p>创建文本</p>
<ul>
<li>while true; do echo 1 &gt;&gt; /root/a.txt ; sleep 1;done</li>
</ul>
</li>
</ul>
<h3 id="2、用户管理"><a href="#2、用户管理" class="headerlink" title="2、用户管理"></a>2、用户管理</h3><ul>
<li>添加用户<ul>
<li>useradd username</li>
</ul>
</li>
<li>更改用户密码<ul>
<li>password username</li>
</ul>
</li>
<li>删除用户<ul>
<li>userdel username 删除用户（不删除用户数据</li>
<li>userdel -r username 删除用户和用户数据</li>
</ul>
</li>
</ul>
<h3 id="3、压缩包管理"><a href="#3、压缩包管理" class="headerlink" title="3、压缩包管理"></a>3、压缩包管理</h3><ul>
<li>gz压缩包<ul>
<li>tar czf file.tar.gz file 制作file的压缩包</li>
<li>tar zxvf file.tar.gz -C /directory 解压缩包</li>
</ul>
</li>
<li>zip压缩包<ul>
<li>zip file.zip file 将file制成名为file.zip</li>
<li>unzip file.zip 解压缩</li>
</ul>
</li>
</ul>
<h3 id="4、查看属性"><a href="#4、查看属性" class="headerlink" title="4、查看属性"></a>4、查看属性</h3><ul>
<li><p>查看磁盘大小</p>
<ul>
<li>df -h</li>
</ul>
</li>
<li><p>查看内存大小</p>
<ul>
<li>free -h</li>
</ul>
</li>
<li><p>查看文件大小</p>
<ul>
<li>du -h</li>
</ul>
</li>
<li><p>清理缓存</p>
<ul>
<li>echo 1 &gt; /proc/sys/vm/drop_caches</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Ctrl+c 与ctrl+z 的区别</span><br><span class="line"></span><br><span class="line">两者都为中断进程</span><br><span class="line">Ctrl+c  是强制中断程序的执行</span><br><span class="line">ctrl+z  是将任务中断,但任务并未结束,他任然在进程中只是挂起了,用户可以使用fg/bg操作继续前台或后台的任务,fg命令重新启动前台被中断的任务,bg命令把被中断的任务放在后台执行.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/etc/hosts   修改主机名与IP映射</span><br><span class="line">/etc/sysconfig/network  修改主机名</span><br><span class="line">visudo    设置其他用户权限</span><br><span class="line">/etc/profile  配置环境变量</span><br><span class="line">/etc/udev/rules.d/70-persistent-net.rules  更改mac地址</span><br><span class="line">/etc/sysconfig/network-scripts/ifcfg-eth0  更改IP地址</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">三台虚拟机免密登录</span><br><span class="line">1,三台机器生成公钥与私钥</span><br><span class="line">ssh -keygen -t rsa</span><br><span class="line">2.三台机器都把 公钥拷到同一台机器</span><br><span class="line">ssh-copy-id node01.hadoop.com</span><br><span class="line">3.复制第一台机器(有三个公钥的那个)的认证到其他两台机器</span><br><span class="line">scp /root/.ssh/authorized_keys node02.hadoop.com:$PWD</span><br><span class="line">scp /root/.ssh/authorized_keys node03.hadoop.com:/root/.ssh</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">虚拟机的时钟同步</span><br><span class="line">  通过网络连接外网进行时钟同步,必须保证虚拟机连上外网</span><br><span class="line"> ntpdate us.pool.ntp.org</span><br><span class="line">  阿里云时钟同步服务器</span><br><span class="line">  ntpdate ntp4.aliyun.com</span><br><span class="line">定时任务  定时同步时钟</span><br><span class="line">crontab  -e   </span><br><span class="line">*/1 * * * * /usr/sbin/ntpdate ntp4.aliyun.com;</span><br><span class="line">*/1 * * * * /usr/sbin/ntpdate us.pool.ntp.org;  二选一</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Linux常用命令/" data-id="cjz24s2wy0006u8u5e219piu4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Linux开发环境" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Linux开发环境/" class="article-date">
  <time datetime="2019-08-08T03:31:32.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Linux开发环境/">Linux开发环境</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="linux常用软件安装"><a href="#linux常用软件安装" class="headerlink" title="linux常用软件安装"></a>linux常用软件安装</h1><p>如有需要,自行参考</p>
<h2 id="一-mysql的安装"><a href="#一-mysql的安装" class="headerlink" title="一 mysql的安装"></a>一 mysql的安装</h2><p>第一步：在线安装mysql相关的软件包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum  install  mysql  mysql-server  mysql-devel</span><br></pre></td></tr></table></figure>

<p>第二步：启动mysql的服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/mysqld start</span><br></pre></td></tr></table></figure>

<p>第三步：通过mysql安装自带脚本进行设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/mysql_secure_installation</span><br></pre></td></tr></table></figure>

<p>第四步：进入mysql的客户端然后进行授权</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> grant all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;123456&apos; with grant option;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>

<p>yum 安装的卸载</p>
<p><strong>一、使用以下命令查看当前安装mysql情况，查找以前是否装有mysql</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`rpm -qa|``grep` `-i mysql`</span><br></pre></td></tr></table></figure>

<p>显示之前安装了：</p>
<p> MySQL-client-5.5.25a-1.rhel5</p>
<p> MySQL-server-5.5.25a-1.rhel5</p>
<p><strong>2、停止mysql服务、删除之前安装的mysql</strong></p>
<p>service mysqld stop</p>
<p>删除命令：<code>rpm -e –nodeps 包名</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`rpm -ev MySQL-client-5.5.25a-1.rhel5 ``rpm -ev MySQL-server-5.5.25a-1.rhel5`</span><br></pre></td></tr></table></figure>

<p>如果提示依赖包错误，则使用以下命令尝试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`rpm -ev MySQL-client-5.5.25a-1.rhel5 --nodeps`</span><br></pre></td></tr></table></figure>

<p>如果提示错误：<code>error: %preun(xxxxxx) scriptlet failed, exit status 1</code></p>
<p>则用以下命令尝试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`rpm -e --noscripts MySQL-client-5.5.25a-1.rhel5`</span><br></pre></td></tr></table></figure>

<p><strong>3、查找之前老版本mysql的目录、并且删除老版本mysql的文件和库</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`find` `/ -name mysql`</span><br></pre></td></tr></table></figure>

<p>查找结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`find` `/ -name mysql ` `/var/lib/mysql``/var/lib/mysql/mysql``/usr/lib64/mysql`</span><br></pre></td></tr></table></figure>

<p>删除对应的mysql目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`rm` `-rf ``/var/lib/mysql``rm` `-rf ``/var/lib/mysql``rm` `-rf ``/usr/lib64/mysql`</span><br></pre></td></tr></table></figure>

<p><strong>注意：</strong>卸载后/etc/my.cnf不会删除，需要进行手工删除</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`rm` `-rf ``/etc/my``.cnf`</span><br></pre></td></tr></table></figure>

<p><strong>4、再次查找机器是否安装mysql</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`rpm -qa|``grep` `-i mysql`</span><br></pre></td></tr></table></figure>

<h2 id="二-jdk的安装"><a href="#二-jdk的安装" class="headerlink" title="二 jdk的安装"></a>二 jdk的安装</h2><p>1.1 查看自带的openjdk并卸载</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">查询</span><br><span class="line">rpm -qa | grep java</span><br><span class="line">卸载</span><br><span class="line">rpm -e java-1.6.0-openjdk-1.6.0.41-1.13.13.1.el6_8.x86_64 tzdata-java-2016j-1.el6.noarch java-1.7.0-openjdk-1.7.0.131-2.6.9.0.el6_8.x86_64 --nodeps</span><br></pre></td></tr></table></figure>

<p>–nodeps 不管依赖直接删</p>
<p>2上传解压</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u141-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<p>3配置环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">添加</span><br><span class="line">export JAVA_HOME=/export/servers/jdk1.8.0_141</span><br><span class="line">export PATH=:$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>

<p>4 生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h1 id="zookeeper的安装"><a href="#zookeeper的安装" class="headerlink" title="zookeeper的安装"></a>zookeeper的安装</h1><h1 id="Hadoop的安装"><a href="#Hadoop的安装" class="headerlink" title="Hadoop的安装"></a>Hadoop的安装</h1><h1 id="hive的安装"><a href="#hive的安装" class="headerlink" title="hive的安装"></a>hive的安装</h1><p>1上传压塑包并解压</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-2.1.1-bin.tar.gz</span><br></pre></td></tr></table></figure>

<p>2 解压后可重命名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv apache-hive-2.1.1-bin hive</span><br></pre></td></tr></table></figure>

<p>3 安装mysql省略</p>
<p>4修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template hive-env.sh</span><br><span class="line"></span><br><span class="line">配置</span><br><span class="line">HADOOP_HOME=/export/servers/hadoop-2.7.5</span><br><span class="line">export HIVE_CONF_DIR=/export/servers/hive/conf</span><br></pre></td></tr></table></figure>

<p>创建文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">vim hive-site.xml</span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;123456&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">		&lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;node03&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>5将数据库驱动加入hive下的lib目录下</p>
<p>6配置hive环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br><span class="line"></span><br><span class="line">export HIVE_HOME=/export/servers/hive</span><br><span class="line">export PATH=:$HIVE_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">配后:</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h1 id="flume的安装"><a href="#flume的安装" class="headerlink" title="flume的安装"></a>flume的安装</h1><h1 id="sqoop的安装"><a href="#sqoop的安装" class="headerlink" title="sqoop的安装"></a>sqoop的安装</h1><p>安装sqoop的前提是已经具备java和hadoop的环境</p>
<p>上传解压后:</p>
<p>配置文件修改:</p>
<p>conf下的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">vi sqoop-env.sh</span><br><span class="line">export HADOOP_COMMON_HOME= /export/servers/hadoop-2.7.5 </span><br><span class="line">export HADOOP_MAPRED_HOME= /export/servers/hadoop-2.7.5</span><br><span class="line">export HIVE_HOME= /export/servers/hive</span><br></pre></td></tr></table></figure>

<p>把数据库驱动加入lib目录下</p>
<p>测试:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop list-databases \</span><br><span class="line">--connect jdbc:mysql://node01:3306/ \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456</span><br></pre></td></tr></table></figure>

<p>列出数据库中所有的数据库</p>
<h1 id="azkaban的安装"><a href="#azkaban的安装" class="headerlink" title="azkaban的安装"></a>azkaban的安装</h1><h1 id="telnet-安装"><a href="#telnet-安装" class="headerlink" title="telnet 安装"></a>telnet 安装</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum list telnet*              列出telnet相关的安装包</span><br><span class="line">yum install telnet-server          安装telnet服务</span><br><span class="line">yum install telnet.*           安装telnet客户端</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Linux开发环境/" data-id="cjz24s2wn0001u8u5e38el2w2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-shell" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/shell/" class="article-date">
  <time datetime="2019-08-08T03:30:59.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/shell/">shell</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h1><h2 id="一简介"><a href="#一简介" class="headerlink" title="一简介"></a>一简介</h2><p> Shell 是一个用 C 语言编写的程序， 通过 Shell 用户可以访问操作系统内核服务。它类似于 DOS 下的 command 和后来的 cmd.exe。Shell 既是一种命令语言，又是一种程序设计语言。</p>
<p> Shell script 是一种为 shell 编写的脚本程序。 Shell 编程一般指 shell<br>脚本编程，不是指开发 shell 自身。<br>​ Shell 编程跟 java、 php 编程一样，只要有一个能编写代码的文本编辑器<br>和一个能解释执行的脚本解释器就可以了。<br>​ Linux 的 Shell 种类众多，一个系统可以存在多个 shell，可以通过 cat/etc/shells 命令查看系统中安装的 shell。<br>Bash 由于易用和免费，在日常工作中被广泛使用。同时， Bash 也是大多数Linux 系统默认的 Shell.</p>
<h2 id="二-基本格式"><a href="#二-基本格式" class="headerlink" title="二 基本格式"></a>二 基本格式</h2><p> 使用 vi 编辑器新建一个文件 hello.sh。 扩展名并不影响脚本执行，见名知意。 比如用 php 写 shell 脚本，扩展名就用 .php。</p>
<p>例如 #!是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种 Shell。</p>
<p>echo 用于向窗口输出文本.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">echo &quot;hello shell&quot;</span><br></pre></td></tr></table></figure>

<p>执行:</p>
<p>chmod +x ./hello.sh #使脚本具有执行权限</p>
<p>./hello.sh #执行脚本</p>
<p>直接写 hello.sh， linux系统会去PATH里寻找有没有叫 hello.sh的。 用 ./hello.sh 告诉系统说，就在当前目录找。</p>
<p>还可以作为解释器参数运行。 直接运行解释器，其参数就是 shell 脚本的文件名，如：</p>
<p>/bin/sh /root/hello.sh<br>/bin/php test.php<br>这种方式运行脚本，不需要在第一行指定解释器信息，写了也不生效</p>
<h2 id="三shell变量"><a href="#三shell变量" class="headerlink" title="三shell变量"></a>三shell变量</h2><p>注意:</p>
<p><strong>除了等号不空格,其他处处都空格</strong></p>
<p>变量=值 you=”buca”</p>
<p>注意:</p>
<p>变量名和等号之间不能有空格，同时，变量名的命名须遵循如下规则：<br>l 首个字符必须为字母（ a-z， A-Z）<br>l 中间不能有空格，可以使用下划线（ _）<br>l 不能使用标点符号<br>l 不能使用 bash里的关键字（可用 help 命令查看保留关键字）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">name=&quot;nicai&quot;</span><br><span class="line">echo $name            ##使用变量</span><br><span class="line">echo $&#123;name&#125;			##花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界。已定义的变量，可以被重新定义。</span><br></pre></td></tr></table></figure>

<p>使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。<br>使用 unset 命令可以删除变量。 不能删除只读变量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">readonly variable name</span><br><span class="line">unset variable name</span><br></pre></td></tr></table></figure>

<p>变量类型:</p>
<p>局部变量:</p>
<p> 局部变量在脚本或命令中定义，仅在当前 shell 实例中有效，其<br>他 shell 启动的程序不能访问局部变量。</p>
<p>环境变量:</p>
<p> 所有的程序，包括 shell 启动的程序，都能访问环境变量，有些程<br>序需要环境变量来保证其正常运行。 可以用过 set 命令查看当前环境变量。</p>
<p>shell变量:</p>
<p> shell 变量是由 shell 程序设置的特殊变量。 shell 变量中有一<br>部分是环境变量，有一部分是局部变量，这些变量保证了 shell 的正常运行</p>
<p>参数传递:</p>
<p>在执行 Shell 脚本时， 可以向脚本传递参数。<br>脚本内获取参数的格式为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$n</span><br></pre></td></tr></table></figure>

<p>n 代表一个数字， 1 为执行脚本的第一个参<br>数， 2 为执行脚本的第二个参数，以此类推……<br>$0 表示当前脚本名称。</p>
<p>特殊字符:</p>
<table>
<thead>
<tr>
<th align="left">$#</th>
<th align="left">传递到脚本的参数个数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">$*</td>
<td align="left">以一个单字符串显示所有向脚本传递的参数。</td>
</tr>
<tr>
<td align="left">$$</td>
<td align="left">脚本运行的当前进程 ID 号</td>
</tr>
<tr>
<td align="left">$!</td>
<td align="left">后台运行的最后一个进程的 ID 号</td>
</tr>
<tr>
<td align="left">$@</td>
<td align="left">与$*相同，但是使用时加引号，并在引号中返回每个参数。</td>
</tr>
<tr>
<td align="left">$?</td>
<td align="left">显示最后命令的退出状态。 0 表示没有错误，其他任何值表明有错误。</td>
</tr>
</tbody></table>
<p>例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo &quot;第一个参数为： $1&quot;;</span><br><span class="line">echo &quot;参数个数为： $#&quot;;</span><br><span class="line">echo &quot;传递的参数作为一个字符串显示： $*&quot;;</span><br><span class="line">执行脚本： ./test.sh 1 2 3</span><br><span class="line">$*和$@区别</span><br><span class="line">相同点： 都表示传递给脚本的所有参数。</span><br><span class="line">不同点：</span><br><span class="line">不被&quot; &quot;包含时， $*和$@都以$1 $2… $n 的形式组成参数列表。</span><br><span class="line">被&quot; &quot;包含时， &quot;$*&quot; 会将所有的参数作为一个整体，以&quot;$1 $2 … $n&quot;</span><br><span class="line">的形式组成一个整串； &quot;$@&quot; 会将各个参数分开，以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的</span><br><span class="line">形式组成一个参数列表。</span><br></pre></td></tr></table></figure>

<h2 id="四-shell运算符"><a href="#四-shell运算符" class="headerlink" title="四 shell运算符"></a>四 shell运算符</h2><p>Shell 和其他编程语音一样，支持包括：算术、关系、 布尔、字符串等运<br>算符。 原生 bash 不支持简单的数学运算，但是可以通过其他命令来实现，例如<br>expr。 expr 是一款表达式计算工具，使用它能完成表达式的求值操作。例如加，减，乘，除等操作</p>
<p>注意：表达式和运算符之间要有，例如 2+2 是不对的，必须写成 2 + 2。完整的表达式要被 包含，注意不是单引号，在 Esc 键下边。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo &quot;hello world&quot;</span><br><span class="line">a=4</span><br><span class="line">b=20</span><br><span class="line">#加法运算</span><br><span class="line">echo `expr $a + $b`</span><br><span class="line">#减法运算</span><br><span class="line">echo `expr $b - $a`</span><br><span class="line">#乘法运算，注意*号前面需要反斜杠</span><br><span class="line">echo `expr $a \* $b`</span><br><span class="line">#除法运算</span><br><span class="line">echo `expr $b / $a`</span><br><span class="line">此外，还可以通过(())、 $[]进行算术运算。</span><br><span class="line">count=1;</span><br><span class="line">((count++));</span><br><span class="line">echo $count;</span><br><span class="line">a=$((1+2));</span><br><span class="line">a=$[1+2];</span><br></pre></td></tr></table></figure>

<h2 id="五-流程控制"><a href="#五-流程控制" class="headerlink" title="五 流程控制"></a>五 流程控制</h2><h3 id="1-if-else"><a href="#1-if-else" class="headerlink" title="1 if else"></a>1 if else</h3><p>格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">if condition1</span><br><span class="line">then</span><br><span class="line">command1</span><br><span class="line">elif condition2</span><br><span class="line">then</span><br><span class="line">command2</span><br><span class="line">else</span><br><span class="line">commandN</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>条件表达式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">EQ 就是 EQUAL等于</span><br><span class="line">NQ 就是 NOT EQUAL不等于 </span><br><span class="line">GT 就是 GREATER THAN大于　 </span><br><span class="line">LT 就是 LESS THAN小于 </span><br><span class="line">GE 就是 GREATER THAN OR EQUAL 大于等于 </span><br><span class="line">LE 就是 LESS THAN OR EQUAL 小于等于</span><br></pre></td></tr></table></figure>

<p>例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">a=10</span><br><span class="line">b=20</span><br><span class="line">if [ $a -eq $b ]</span><br><span class="line">then</span><br><span class="line">   echo &quot;$a -eq $b : a 等于 b&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;$a -eq $b: a 不等于 b&quot;</span><br><span class="line">fi</span><br><span class="line">if [ $a -ne $b ]</span><br><span class="line">then</span><br><span class="line">   echo &quot;$a -ne $b: a 不等于 b&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;$a -ne $b : a 等于 b&quot;</span><br><span class="line">fi</span><br><span class="line">if [ $a -gt $b ]</span><br><span class="line">then</span><br><span class="line">   echo &quot;$a -gt $b: a 大于 b&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;$a -gt $b: a 不大于 b&quot;</span><br><span class="line">fi</span><br><span class="line">if [ $a -lt $b ]</span><br><span class="line">then</span><br><span class="line">   echo &quot;$a -lt $b: a 小于 b&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;$a -lt $b: a 不小于 b&quot;</span><br><span class="line">fi</span><br><span class="line">if [ $a -ge $b ]</span><br><span class="line">then</span><br><span class="line">   echo &quot;$a -ge $b: a 大于或等于 b&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;$a -ge $b: a 小于 b&quot;</span><br><span class="line">fi</span><br><span class="line">if [ $a -le $b ]</span><br><span class="line">then</span><br><span class="line">   echo &quot;$a -le $b: a 小于或等于 b&quot;</span><br><span class="line">else</span><br><span class="line">   echo &quot;$a -le $b: a 大于 b&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<h3 id="2for循环"><a href="#2for循环" class="headerlink" title="2for循环"></a>2for循环</h3><p>方式一</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for N in 1 2 3</span><br><span class="line">do</span><br><span class="line">echo $N</span><br><span class="line">done</span><br><span class="line">或</span><br><span class="line">for N in 1 2 3; do echo $N; done</span><br><span class="line">或</span><br><span class="line">for N in &#123;1..3&#125;; do echo $N; done</span><br></pre></td></tr></table></figure>

<p>方式二</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for ((i = 0; i &lt;= 5; i++))</span><br><span class="line">do</span><br><span class="line">echo &quot;welcome $i times&quot;</span><br><span class="line">done</span><br><span class="line">或</span><br><span class="line">for ((i = 0; i &lt;= 5; i++)); do echo &quot;welcome $i times&quot;; done</span><br></pre></td></tr></table></figure>

<p>例子:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">for n in 1 2 3</span><br><span class="line">do</span><br><span class="line">echo $n</span><br><span class="line">done</span><br><span class="line">a=1</span><br><span class="line">b=2</span><br><span class="line">c=3</span><br><span class="line">for N in  $a $b $c</span><br><span class="line">do</span><br><span class="line"> echo $N</span><br><span class="line">done</span><br><span class="line">#打印当前系统所有进程</span><br><span class="line">for N in `ps -ef`</span><br><span class="line">do </span><br><span class="line"> echo $N</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h3 id="3-while语法"><a href="#3-while语法" class="headerlink" title="3 while语法"></a>3 while语法</h3><p>方式一</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while expression</span><br><span class="line">do</span><br><span class="line">command</span><br><span class="line">…</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>方式二</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">i=1</span><br><span class="line">while (( i &lt;= 3))</span><br><span class="line">do </span><br><span class="line"> let i++</span><br><span class="line"> echo $i</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>let 命令是 BASH 中用于计算的工具，用于执行一个或多个表达式，变量<br>计算中不需要加上 $ 来表示变量。 自加操作： let<br>no++ 自减操作： let no–</p>
<p>无限循环:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while true</span><br><span class="line">do</span><br><span class="line">command</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<h3 id="3-case语句"><a href="#3-case语句" class="headerlink" title="3 case语句"></a>3 case语句</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">case 值 in</span><br><span class="line">模式 1)</span><br><span class="line">command1</span><br><span class="line">command2</span><br><span class="line">...</span><br><span class="line">commandN</span><br><span class="line">;;</span><br><span class="line">模式 2）</span><br><span class="line">command1</span><br><span class="line">command2</span><br><span class="line">...</span><br><span class="line">commandN</span><br><span class="line">;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<p>例子:</p>
<p>read aNum 等待键盘输入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">echo &apos;输入 1 到 4 之间的数字:&apos;</span><br><span class="line">echo &apos;你输入的数字为:&apos;</span><br><span class="line">read aNum</span><br><span class="line">case $aNum in</span><br><span class="line">    1)  echo &apos;你选择了 1&apos;</span><br><span class="line">    ;;</span><br><span class="line">    2)  echo &apos;你选择了 2&apos;</span><br><span class="line">    ;;</span><br><span class="line">    3)  echo &apos;你选择了 3&apos;</span><br><span class="line">    ;;</span><br><span class="line">    4)  echo &apos;你选择了 4&apos;</span><br><span class="line">    ;;</span><br><span class="line">    *)  echo &apos;你没有输入 1 到 4 之间的数字&apos;</span><br><span class="line">    ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>

<h2 id="六函数的使用"><a href="#六函数的使用" class="headerlink" title="六函数的使用"></a>六函数的使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">hello()&#123;</span><br><span class="line">echo &quot;hello&quot;</span><br><span class="line">echo &quot;第一个参数为 $1&quot;</span><br><span class="line">echo &quot;第二个参数为 $2&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">hello adv 123</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/shell/" data-id="cjz24s2ws0004u8u55mqig9hl" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Zookeeper" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Zookeeper/" class="article-date">
  <time datetime="2019-08-08T03:30:47.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Zookeeper/">Zookeeper</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Zookeeper学习笔记"><a href="#Zookeeper学习笔记" class="headerlink" title="Zookeeper学习笔记"></a>Zookeeper学习笔记</h1><h2 id="一概述"><a href="#一概述" class="headerlink" title="一概述"></a>一概述</h2><p>zookeeper是一个开源的分布式协调服务框架,主要解决分布式集群中应用系统的一致性问题和数据管理问题.</p>
<p><a href="https://manzhong.github.io/images/zookeeper/1-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/zookeeper/1-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.png" alt="img"></a></p>
<h2 id="二特点"><a href="#二特点" class="headerlink" title="二特点"></a>二特点</h2><p>zoo本质上为一个分布式文件系统适合存放小文件.</p>
<p>Zookeeper 中存储的其实是一个又一个 Znode, Znode 是 Zookeeper 中的节点</p>
<ul>
<li>Znode 是有路径的, 例如 <code>/data/host1</code>, <code>/data/host2</code>, 这个路径也可以理解为是 Znode 的 Name</li>
<li>Znode 也可以携带数据, 例如说某个 Znode 的路径是 <code>/data/host1</code>, 其值是一个字符串 `”192.168.0.1”</li>
</ul>
<p>正因为 Znode 的特性, 所以 Zookeeper 可以对外提供出一个类似于文件系统的视图, 可以通过操作文件系统的方式操作 Zookeeper</p>
<ul>
<li>使用路径获取 Znode</li>
<li>获取 Znode 携带的数据</li>
<li>修改 Znode 携带的数据</li>
<li>删除 Znode</li>
<li>添加 Znode</li>
</ul>
<h2 id="三应用场景"><a href="#三应用场景" class="headerlink" title="三应用场景"></a>三应用场景</h2><h5 id="3-1-数据发布-订阅"><a href="#3-1-数据发布-订阅" class="headerlink" title="3.1 数据发布/订阅"></a>3.1 数据发布/订阅</h5><p>　　<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">数据发布/订阅系统</a>,需要发布者将数据发布到Zookeeper的节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新。</p>
<p> 发布/订阅一般有两种设计模式：推模式和拉模式，服务端主动将数据更新发送给所有订阅的客户端称为推模式；客户端主动请求获取最新数据称为拉模式.</p>
<p>Zookeeper采用了推拉相结合的模式，客户端向服务端注册自己需要关注的节点，一旦该节点数据发生变更，那么服务端就会向相应的客户端推送Watcher事件通知，客户端接收到此通知后，主动到服务端获取最新的数据。</p>
<h5 id="3-2-命名服务"><a href="#3-2-命名服务" class="headerlink" title="3.2 命名服务"></a>3.2 命名服务</h5><p>　　命名服务是分步实现系统中较为常见的一类场景，分布式系统中，被命名的实体通常可以是集群中的机器、提供的服务地址或远程对象等，通过命名服务，客户端可以根据指定名字来获取资源的实体，在分布式环境中，上层应用仅仅需要一个全局唯一的名字。Zookeeper可以实现一套分布式全局唯一ID的分配机制。</p>
<p><a href="https://manzhong.github.io/images/zookeeper/1561706612382.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/zookeeper/1561706612382.png" alt="img"></a></p>
<p>　　通过调用Zookeeper节点创建的API接口就可以创建一个顺序节点，并且在API返回值中会返回这个节点的完整名字，利用此特性，可以生成全局ID，其步骤如下</p>
<ol>
<li>客户端根据任务类型，在指定类型的任务下通过调用接口创建一个顺序节点，如”job-“。</li>
<li>创建完成后，会返回一个完整的节点名，如”job-00000001”。</li>
<li>客户端拼接type类型和返回值后，就可以作为全局唯一ID了，如”type2-job-00000001”。</li>
</ol>
<h5 id="3-3-分布式协调-通知"><a href="#3-3-分布式协调-通知" class="headerlink" title="3.3 分布式协调/通知"></a>3.3 分布式协调/通知</h5><p>　　Zookeeper中特有的Watcher注册于异步通知机制，能够很好地实现分布式环境下不同机器，甚至不同系统之间的协调与通知，从而实现对数据变更的实时处理。通常的做法是不同的客户端都对Zookeeper上的同一个数据节点进行Watcher注册，监听数据节点的变化（包括节点本身和子节点），若数据节点发生变化，那么所有订阅的客户端都能够接收到相应的Watcher通知，并作出相应处理。</p>
<p>　　在绝大多数分布式系统中，系统机器间的通信无外乎<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener"><strong>心跳检测</strong>、<strong>工作进度汇报</strong>和<strong>系统调度</strong></a>。</p>
<p>　　① <strong>心跳检测</strong>，不同机器间需要检测到彼此是否在正常运行，可以使用Zookeeper实现机器间的心跳检测，基于其临时节点特性（临时节点的生存周期是客户端会话，客户端若当即后，其临时节点自然不再存在），可以让不同机器都在Zookeeper的一个指定节点下创建临时子节点，不同的机器之间可以根据这个临时子节点来判断对应的客户端机器是否存活。通过Zookeeper可以大大减少系统耦合。</p>
<p>　　② <strong>工作进度汇报</strong>，通常任务被分发到不同机器后，需要实时地将自己的任务执行进度汇报给分发系统，可以在Zookeeper上选择一个节点，每个任务客户端都在这个节点下面创建临时子节点，这样不仅可以判断机器是否存活，同时各个机器可以将自己的任务执行进度写到该临时节点中去，以便中心系统能够实时获取任务的执行进度。</p>
<p>　　③ <strong>系统调度</strong>，Zookeeper能够实现如下系统调度模式：分布式系统由控制台和一些客户端系统两部分构成，控制台的职责就是需要将一些指令信息发送给所有的客户端，以控制他们进行相应的业务逻辑，后台管理人员在控制台上做一些操作，实际上就是修改Zookeeper上某些节点的数据，Zookeeper可以把数据变更以时间通知的形式发送给订阅客户端。</p>
<h5 id="3-4分布式锁"><a href="#3-4分布式锁" class="headerlink" title="3.4分布式锁"></a>3.4分布式锁</h5><p>　　分布式锁用于控制分布式系统之间同步访问共享资源的一种方式，可以保证不同系统访问一个或一组资源时的一致性，主要分为排它锁和共享锁。</p>
<p>　　<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener"><strong>排它锁又称为写锁或独占锁</strong></a>，若事务T1对数据对象O1加上了排它锁，那么在整个加锁期间，只允许事务T1对O1进行读取和更新操作，其他任何事务都不能再对这个数据对象进行任何类型的操作，直到T1释放了排它锁。</p>
<p><a href="https://manzhong.github.io/images/zookeeper/1561706638372.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/zookeeper/1561706638372.png" alt="img"></a>　</p>
<p>　① <strong>获取锁</strong>，在需要获取排它锁时，所有客户端通过调用接口，在/exclusive_lock节点下创建临时子节点/exclusive_lock/lock。Zookeeper可以保证只有一个客户端能够创建成功，没有成功的客户端需要注册/exclusive_lock节点监听。</p>
<p>　② <strong>释放锁</strong>，当获取锁的客户端宕机或者正常完成业务逻辑都会导致临时节点的删除，此时，所有在/exclusive_lock节点上注册监听的客户端都会收到通知，可以重新发起分布式锁获取。</p>
<p><a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener"><strong>共享锁又称为读锁</strong></a>，若事务T1对数据对象O1加上共享锁，那么当前事务只能对O1进行读取操作，其他事务也只能对这个数据对象加共享锁，直到该数据对象上的所有共享锁都被释放。在需要获取共享锁时，所有客户端都会到/shared_lock下面创建一个临时顺序节点</p>
<p><a href="https://manzhong.github.io/images/zookeeper/1558060430149.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/zookeeper/1558060430149.png" alt="img"></a>　</p>
<h5 id="3-5-分布式队列"><a href="#3-5-分布式队列" class="headerlink" title="3.5 分布式队列"></a>3.5 分布式队列</h5><p>　 有一些时候，多个团队需要共同完成一个任务，比如，A团队将Hadoop集群计算的结果交给B团队继续计算，B完成了自己任务再交给C团队继续做。这就有点像业务系统的工作流一样，一环一环地传下 去.</p>
<p>分布式环境下，我们同样需要一个类似单进程队列的组件，用来实现跨进程、跨主机、跨网络的数据共享和数据传递，这就是我们的分布式队列。</p>
<h2 id="四架构"><a href="#四架构" class="headerlink" title="四架构"></a>四架构</h2><p>Zookeeper集群是一个基于主从架构的高可用集群</p>
<p><a href="https://manzhong.github.io/images/zookeeper/Zookeeper%E7%9A%84%E6%9E%B6%E6%9E%84.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/zookeeper/Zookeeper%E7%9A%84%E6%9E%B6%E6%9E%84.jpg" alt="img"></a></p>
<p> 每个服务器承担如下三种角色中的一种</p>
<ul>
<li><strong>Leader</strong> 一个Zookeeper集群同一时间只会有一个实际工作的Leader，它会发起并维护与各Follwer及Observer间的心跳。所有的写操作必须要通过Leader完成再由Leader将写操作广播给其它服务器。</li>
<li><strong>Follower</strong> 一个Zookeeper集群可能同时存在多个Follower，它会响应Leader的心跳。Follower可直接处理并返回客户端的读请求，同时会将写请求转发给Leader处理，并且负责在Leader处理写请求时对请求进行投票。</li>
<li><strong>Observer</strong> 角色与Follower类似，但是无投票权。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">若想使用observer模式可在对应节点的配置文件添加如下配置</span><br><span class="line">peerType=observer</span><br><span class="line">其次,必须在配置文件指定那些节点被指定为observer如:</span><br><span class="line">server.1:localhost:2181:3181:observer</span><br></pre></td></tr></table></figure>

<h4 id="5-Zookeeper的选举机制"><a href="#5-Zookeeper的选举机制" class="headerlink" title="5:Zookeeper的选举机制"></a>5:Zookeeper的选举机制</h4><p>Leader选举是保证分布式<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">数据一致性</a>的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">##### 5.1. 服务器启动时期的Leader选举</span><br></pre></td></tr></table></figure>

<p>　　若进行Leader选举，则至少需要两台机器，这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。选举过程如下</p>
<p>　　<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">(1)</a> <a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">每个Server发出一个投票</a>。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。</p>
<p>　　<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">(2)</a> <a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">接受来自各个服务器的投票</a>。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。</p>
<p>　　<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">(3)</a> <a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">处理投票</a>。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下</p>
<p>　　　　<strong>· 优先检查ZXID</strong>。ZXID比较大的服务器优先作为Leader。</p>
<p>　　　　<strong>· 如果ZXID相同，那么就比较myid</strong>。myid较大的服务器作为Leader服务器。</p>
<p>　　对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。</p>
<p>　　<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">(4) 统计投票</a>。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。</p>
<p>　　<a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">(5) 改变服务器状态</a>。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。</p>
<h5 id="5-2-服务器运行时期的Leader选举"><a href="#5-2-服务器运行时期的Leader选举" class="headerlink" title="5.2.服务器运行时期的Leader选举"></a>5.2.服务器运行时期的Leader选举</h5><p>　　在Zookeeper运行期间，Leader与非Leader服务器各司其职，即便当有非Leader服务器宕机或新加入，此时也不会影响Leader，但是一旦Leader服务器挂了，那么整个集群将暂停对外服务，进入新一轮Leader选举，其过程和启动时期的Leader选举过程基本一致过程相同。</p>
<h2 id="五-安装"><a href="#五-安装" class="headerlink" title="五 安装"></a>五 安装</h2><p>集群规划</p>
<table>
<thead>
<tr>
<th align="left">服务器IP</th>
<th align="left">主机名</th>
<th align="left">myid的值</th>
</tr>
</thead>
<tbody><tr>
<td align="left">192.168.174.100</td>
<td align="left">node01</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">192.168.174.110</td>
<td align="left">node02</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left">192.168.174.120</td>
<td align="left">node03</td>
<td align="left">3</td>
</tr>
</tbody></table>
<p><strong>第一步：下载zookeeeper的压缩包，下载网址如下</strong></p>
<p><a href="http://archive.apache.org/dist/zookeeper/" target="_blank" rel="noopener">http://archive.apache.org/dist/zookeeper/</a></p>
<p>我们在这个网址下载我们使用的zk版本为3.4.9</p>
<p>下载完成之后，上传到我们的linux的/export/softwares路径下准备进行安装</p>
<p><strong>第二步：解压</strong></p>
<p>解压zookeeper的压缩包到/export/servers路径下去，然后准备进行安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/software</span><br><span class="line"></span><br><span class="line">tar -zxvf zookeeper-3.4.9.tar.gz -C ../servers/</span><br></pre></td></tr></table></figure>

<p><strong>第三步：修改配置文件</strong></p>
<p>第一台机器修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/zookeeper-3.4.9/conf/</span><br><span class="line"></span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line"></span><br><span class="line">mkdir -p /export/servers/zookeeper-3.4.9/zkdatas/</span><br><span class="line">vim zoo.cfg</span><br><span class="line">dataDir=/export/servers/zookeeper-3.4.9/zkdatas</span><br><span class="line"># 保留多少个快照</span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"># 日志多少小时清理一次</span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"># 集群中服务器地址</span><br><span class="line">server.1=node01:2888:3888</span><br><span class="line">server.2=node02:2888:3888</span><br><span class="line">server.3=node03:2888:3888</span><br></pre></td></tr></table></figure>

<p><strong>第四步：添加myid配置</strong></p>
<p>在第一台机器的</p>
<p>/export/servers/zookeeper-3.4.9/zkdatas /这个路径下创建一个文件，文件名为myid ,文件内容为1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid</span><br></pre></td></tr></table></figure>

<p><strong>第五步：安装包分发并修改myid的值</strong></p>
<p>安装包分发到其他机器</p>
<p>第一台机器上面执行以下两个命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/servers/zookeeper-3.4.9/ node02:/export/servers/</span><br><span class="line">scp -r /export/servers/zookeeper-3.4.9/ node03:/export/servers/</span><br></pre></td></tr></table></figure>

<p>第二台机器上修改myid的值为2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 2 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid</span><br></pre></td></tr></table></figure>

<p>第三台机器上修改myid的值为3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 3 &gt; /export/servers/zookeeper-3.4.9/zkdatas/myid</span><br></pre></td></tr></table></figure>

<p><strong>第六步</strong>：三台机器启动zookeeper服务</p>
<p>三台机器启动zookeeper服务</p>
<p>这个命令三台机器都要执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/servers/zookeeper-3.4.9/bin/zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>查看启动状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/servers/zookeeper-3.4.9/bin/zkServer.sh status</span><br></pre></td></tr></table></figure>

<h2 id="六-zookeeper的shell操作"><a href="#六-zookeeper的shell操作" class="headerlink" title="六 zookeeper的shell操作"></a>六 zookeeper的shell操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">登录shell客户端: bin/zkCli.sh -server node:2181</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th align="left">命令</th>
<th align="left">说明</th>
<th align="left">参数</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>create [-s] [-e] path data acl</code></td>
<td align="left">创建Znode</td>
<td align="left">-s 指定是顺序节点 -e 指定是临时节点</td>
</tr>
<tr>
<td align="left"><code>ls path [watch]</code></td>
<td align="left">列出Path下所有子Znode</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><code>get path [watch]</code></td>
<td align="left">获取Path对应的Znode的数据和属性</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><code>ls2 path [watch]</code></td>
<td align="left">查看Path下所有子Znode以及子Znode的属性</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><code>set path data [version]</code></td>
<td align="left">更新节点</td>
<td align="left">version 数据版本</td>
</tr>
<tr>
<td align="left"><code>delete path [version]</code></td>
<td align="left">删除节点, 如果要删除的节点有子Znode则无法删除</td>
<td align="left">version 数据版本</td>
</tr>
<tr>
<td align="left"><code>rmr path</code></td>
<td align="left">删除节点, 如果有子Znode则递归删除</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">`setquota -n</td>
<td align="left">-b val path`</td>
<td align="left">修改Znode配额</td>
</tr>
<tr>
<td align="left"><code>history</code></td>
<td align="left">列出历史记录</td>
<td align="left"></td>
</tr>
</tbody></table>
<p>只有ls,ls2,get可以添加watch.</p>
<p>1：创建普通节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create /app1 hello</span><br></pre></td></tr></table></figure>

<p>2: 创建顺序节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create -s /app3 world</span><br></pre></td></tr></table></figure>

<p>3:创建临时节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create -e /tempnode world</span><br></pre></td></tr></table></figure>

<p>4:创建顺序的临时节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create -s -e /tempnode2 aaa</span><br></pre></td></tr></table></figure>

<p>5:获取节点数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get /app1</span><br></pre></td></tr></table></figure>

<p>6:修改节点数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set /app1 xxx</span><br></pre></td></tr></table></figure>

<p>7:删除节点</p>
<p>delete /app1 删除的节点不能有子节点</p>
<p> rmr /app1 递归删除</p>
<p>Znode 的特点</p>
<ul>
<li><p>文件系统的核心是 <code>Znode</code></p>
</li>
<li><p>如果想要选取一个 <code>Znode</code>, 需要使用路径的形式, 例如 <code>/test1/test11</code></p>
</li>
<li><p>Znode 本身并不是文件, 也不是文件夹, Znode 因为具有一个类似于 Name 的路径, 所以可以从<strong>逻辑上</strong>实现一个树状文件系统</p>
</li>
<li><p>ZK 保证 Znode 访问的原子性, 不会出现部分 ZK 节点更新成功, 部分 ZK 节点更新失败的问题</p>
</li>
<li><p><code>Znode</code> 中数据是有大小限制的, 最大只能为<code>1M</code></p>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Znode</span><br></pre></td></tr></table></figure>

<p>是由三个部分构成</p>
<ul>
<li><code>stat</code>: 状态, Znode的权限信息, 版本等</li>
<li><code>data</code>: 数据, 每个Znode都是可以携带数据的, 无论是否有子节点</li>
<li><code>children</code>: 子节点列表</li>
</ul>
</li>
</ul>
<p>Znode 的类型</p>
<ul>
<li><p>每个</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Znode</span><br></pre></td></tr></table></figure>

<p>有两大特性, 可以构成四种不同类型的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Znode</span><br></pre></td></tr></table></figure>

<ul>
<li>持久性<ul>
<li><code>持久</code> 客户端断开时, 不会删除持有的Znode</li>
<li><code>临时</code> 客户端断开时, 删除所有持有的Znode, <strong>临时Znode不允许有子Znode</strong></li>
</ul>
</li>
<li>顺序性<ul>
<li><code>有序</code> 创建的Znode有先后顺序, 顺序就是在后面追加一个序列号, 序列号是由父节点管理的自增,它的格式为“%10d”(10 位数字，没有数值的数位用 0 补充，例如“0000000001”)。</li>
<li><code>无序</code> 创建的Znode没有先后顺序</li>
</ul>
</li>
</ul>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Znode</span><br></pre></td></tr></table></figure>

<p>的属性</p>
<ul>
<li><code>dataVersion</code> 数据版本, 每次当<code>Znode</code>中的数据发生变化的时候, <code>dataVersion</code>都会自增一下</li>
<li><code>cversion</code> 节点版本, 每次当<code>Znode</code>的节点发生变化的时候, <code>cversion</code>都会自增</li>
<li><code>aclVersion</code> <code>ACL(Access Control List)</code>的版本号, 当<code>Znode</code>的权限信息发生变化的时候aclVersion会自增</li>
<li><code>zxid</code> 事务ID</li>
<li><code>ctime</code> 创建时间</li>
<li><code>mtime</code> 最近一次更新的时间</li>
<li><code>ephemeralOwner</code> 如果<code>Znode</code>为临时节点, <code>ephemeralOwner</code>表示与该节点关联的<code>SessionId</code></li>
</ul>
</li>
</ul>
<p>watcher机制</p>
<ul>
<li><p>通知类似于数据库中的触发器, 对某个Znode设置 <code>Watcher</code>, 当Znode发生变化的时候, <code>WatchManager</code>会调用对应的<code>Watcher</code></p>
</li>
<li><p>当Znode发生删除, 修改, 创建, 子节点修改的时候, 对应的<code>Watcher</code>会得到通知</p>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Watcher</span><br></pre></td></tr></table></figure>

<p>的特点</p>
<ul>
<li><strong>一次性触发</strong> 一个 <code>Watcher</code> 只会被触发一次, 如果需要继续监听, 则需要再次添加 <code>Watcher</code></li>
<li>事件封装: <code>Watcher</code> 得到的事件是被封装过的, 包括三个内容 <code>keeperState, eventType, path</code></li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th align="left">KeeperState</th>
<th align="left">EventType</th>
<th align="left">触发条件</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td align="left">None</td>
<td align="left">连接成功</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">SyncConnected</td>
<td align="left">NodeCreated</td>
<td align="left">Znode被创建</td>
<td align="left">此时处于连接状态</td>
</tr>
<tr>
<td align="left">SyncConnected</td>
<td align="left">NodeDeleted</td>
<td align="left">Znode被删除</td>
<td align="left">此时处于连接状态</td>
</tr>
<tr>
<td align="left">SyncConnected</td>
<td align="left">NodeDataChanged</td>
<td align="left">Znode数据被改变</td>
<td align="left">此时处于连接状态</td>
</tr>
<tr>
<td align="left">SyncConnected</td>
<td align="left">NodeChildChanged</td>
<td align="left">Znode的子Znode数据被改变</td>
<td align="left">此时处于连接状态</td>
</tr>
<tr>
<td align="left">Disconnected</td>
<td align="left">None</td>
<td align="left">客户端和服务端断开连接</td>
<td align="left">此时客户端和服务器处于断开连接状态</td>
</tr>
<tr>
<td align="left">Expired</td>
<td align="left">None</td>
<td align="left">会话超时</td>
<td align="left">会收到一个SessionExpiredException</td>
</tr>
<tr>
<td align="left">AuthFailed</td>
<td align="left">None</td>
<td align="left">权限验证失败</td>
<td align="left">会收到一个AuthFailedException</td>
</tr>
</tbody></table>
<p>会话</p>
<ul>
<li>在ZK中所有的客户端和服务器的交互都是在某一个<code>Session</code>中的, 客户端和服务器创建一个连接的时候同时也会创建一个<code>Session</code></li>
<li><code>Session</code>会在不同的状态之间进行切换: <code>CONNECTING</code>, <code>CONNECTED</code>, <code>RECONNECTING</code>, <code>RECONNECTED</code>, <code>CLOSED</code></li>
<li>ZK中的会话两端也需要进行心跳检测, 服务端会检测如果超过超时时间没收到客户端的心跳, 则会关闭连接, 释放资源, 关闭会话</li>
</ul>
<h2 id="七-zookeeper的JavaAPI操作"><a href="#七-zookeeper的JavaAPI操作" class="headerlink" title="七 zookeeper的JavaAPI操作"></a>七 zookeeper的JavaAPI操作</h2><p> 这里操作Zookeeper的JavaAPI使用的是一套zookeeper客户端框架 Curator ，解决了很多Zookeeper客户端非常底层的细节开发工作 。</p>
<p>Curator包含了几个包：</p>
<ul>
<li><p><a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">curator-framework</a>：对zookeeper的底层api的一些封装</p>
</li>
<li><p><a href="https://manzhong.github.io/2017/07/04/zookeeper/" target="_blank" rel="noopener">curator-recipes</a>：封装了一些高级特性，如：Cache事件监听、选举、分布式锁、分布式计数器等</p>
<p>PERSISTENT：永久节点</p>
</li>
</ul>
<p>EPHEMERAL：临时节点</p>
<p>PERSISTENT_SEQUENTIAL：永久节点、序列化</p>
<p>EPHEMERAL_SEQUENTIAL：临时节点、序列化</p>
<p>Maven依赖(使用curator的版本：2.12.0，对应Zookeeper的版本为：3.4.x，如果跨版本会有兼容性问题，很有可能导致节点操作失败)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">maven依赖:</span><br><span class="line">  &lt;!-- &lt;repositories&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">          &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">          &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url</span><br><span class="line">        &lt;/repository&gt;</span><br><span class="line">      &lt;/repositories&gt; --&gt;</span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;curator-framework&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.12.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.12.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.google.collections&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;google-collections&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;RELEASE&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.7.25&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;!-- java编译插件 --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.2&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br></pre></td></tr></table></figure>

<p>创建永久节点:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void createNode() throws Exception &#123;</span><br><span class="line">//参数一  重试连接间隔 参数二  重试次数</span><br><span class="line">   RetryPolicy retryPolicy = new  ExponentialBackoffRetry(1000, 1);</span><br><span class="line">   //获取客户端对象 第一个1000 session超时时间,第二个1000连接超时时间 </span><br><span class="line">   CuratorFramework client =     CuratorFrameworkFactory.newClient(&quot;192.168.174.100:2181,192.168.174.110:2181,192.168.174.120:2181&quot;, 1000, 1000, retryPolicy);</span><br><span class="line">    </span><br><span class="line">  //调用start开启客户端操作</span><br><span class="line">  client.start();</span><br><span class="line">    </span><br><span class="line">  //通过create来进行创建节点，并且需要指定节点类型</span><br><span class="line">  client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(&quot;/hello3/world&quot;);</span><br><span class="line">//关闭资源</span><br><span class="line"> client.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>创建临时节点:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void createNode2() throws Exception&#123;</span><br><span class="line">	RetryPolicy retryPolicy = new  ExponentialBackoffRetry(3000, 1);</span><br><span class="line">  CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;node01:2181,node02:2181,node03:2181&quot;, 3000, 3000, retryPolicy);</span><br><span class="line">client.start();</span><br><span class="line">client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(&quot;/hello5/world&quot;);</span><br><span class="line">Thread.sleep(5000);</span><br><span class="line">client.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修改节点数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"></span><br><span class="line">	 * 节点下面添加数据与修改是类似的，一个节点下面会有一个数据，新的数据会覆盖旧的数据</span><br><span class="line"></span><br><span class="line">	 * @throws Exception</span><br><span class="line"></span><br><span class="line">	 */</span><br><span class="line"></span><br><span class="line">	@Test</span><br><span class="line">	public void nodeData() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">		RetryPolicy retryPolicy = new  ExponentialBackoffRetry(3000, 1);</span><br><span class="line"></span><br><span class="line">		CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;node01:2181,node02:2181,node03:2181&quot;, 3000, 3000, retryPolicy);</span><br><span class="line"></span><br><span class="line">		client.start();</span><br><span class="line"></span><br><span class="line">		client.setData().forPath(&quot;/hello5&quot;, &quot;hello7&quot;.getBytes());</span><br><span class="line"></span><br><span class="line">		client.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>节点数据查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">   public void demo03() throws Exception &#123;</span><br><span class="line">       RetryPolicy retryPolicy = new  ExponentialBackoffRetry(3000, 1);</span><br><span class="line">       CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.80.100:2181,192.168.80.110:2181,192.168.80.120:2181&quot;, 3000, 3000, retryPolicy);</span><br><span class="line">       client.start();</span><br><span class="line">       byte[] forPath = client.getData().forPath(&quot;/aaa3&quot;);</span><br><span class="line">       System.out.println(new String(forPath)+&quot;12&quot;);</span><br><span class="line">       client.close();</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>删除节点:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">    public void demo04() throws Exception&#123;</span><br><span class="line">        RetryPolicy retryPolicy = new  ExponentialBackoffRetry(3000, 1);</span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.80.100:2181,192.168.80.110:2181,192.168.80.120:2181&quot;, 3000, 3000, retryPolicy);</span><br><span class="line">        client.start();</span><br><span class="line">        client.delete().deletingChildrenIfNeeded().forPath(&quot;/aaa3&quot;);</span><br><span class="line">        client.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>节点的监听:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">    public void demo05() throws  Exception&#123;</span><br><span class="line">        RetryPolicy policy = new ExponentialBackoffRetry(3000, 3);</span><br><span class="line">        CuratorFramework client = CuratorFrameworkFactory.newClient(&quot;192.168.80.100:2181,192.168.80.110:2181,192.168.80.120:2181&quot;, policy);</span><br><span class="line">        client.start();</span><br><span class="line">        // ExecutorService pool = Executors.newCachedThreadPool();</span><br><span class="line">        //设置节点的cache</span><br><span class="line">        TreeCache treeCache = new TreeCache(client, &quot;/hello5&quot;);</span><br><span class="line">        //设置监听器和处理过程</span><br><span class="line">        treeCache.getListenable().addListener(new TreeCacheListener() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception &#123;</span><br><span class="line">                ChildData data = event.getData();</span><br><span class="line">                if(data !=null)&#123;</span><br><span class="line">                    switch (event.getType()) &#123;</span><br><span class="line">                        case NODE_ADDED:</span><br><span class="line">                            System.out.println(&quot;NODE_ADDED : &quot;+ data.getPath() +&quot;  数据:&quot;+ new String(data.getData()));</span><br><span class="line">                            break;</span><br><span class="line">                        case NODE_REMOVED:</span><br><span class="line">                            System.out.println(&quot;NODE_REMOVED : &quot;+ data.getPath() +&quot;  数据:&quot;+ new String(data.getData()));</span><br><span class="line">                            break;</span><br><span class="line">                        case NODE_UPDATED:</span><br><span class="line">                            System.out.println(&quot;NODE_UPDATED : &quot;+ data.getPath() +&quot;  数据:&quot;+ new String(data.getData()));</span><br><span class="line">                            break;</span><br><span class="line">                        default:</span><br><span class="line">                            break;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;else&#123;</span><br><span class="line">                    System.out.println( &quot;data is null : &quot;+ event.getType());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        //开始监听</span><br><span class="line">        treeCache.start();</span><br><span class="line">        Thread.sleep(50000000);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Zookeeper/" data-id="cjz24s2xk0008u8u534h68n84" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Hdfs" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Hdfs/" class="article-date">
  <time datetime="2019-08-08T03:30:32.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Hdfs/">Hdfs</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Hadoop的核心-Hdfs"><a href="#Hadoop的核心-Hdfs" class="headerlink" title="Hadoop的核心 Hdfs"></a>Hadoop的核心 Hdfs</h1><h2 id="1-HDFS概述"><a href="#1-HDFS概述" class="headerlink" title="1. HDFS概述"></a>1. HDFS概述</h2><h3 id="1-1-介绍"><a href="#1-1-介绍" class="headerlink" title="1.1 介绍"></a>1.1 介绍</h3><p>在现代的企业环境中，单机容量往往无法存储大量数据，需要跨机器存储。统一管理分布在集群上的文件系统称为<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">分布式文件系统</a> 。</p>
<p> <a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">HDFS</a>（Hadoop Distributed File System）是 Apache Hadoop 项目的一个子项目. Hadoop 非常适于存储大型数据 (比如 TB 和 PB), 其就是使用 HDFS 作为存储系统. HDFS 使用多台计算机存储文件, 并且提供统一的访问接口, 像是访问一个普通文件系统一样使用分布式文件系统.</p>
<h3 id="1-2-历史"><a href="#1-2-历史" class="headerlink" title="1.2 历史"></a>1.2 历史</h3><ol>
<li><a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">Doug Cutting</a> 在做 Lucene 的时候, 需要编写一个爬虫服务, 这个爬虫写的并不顺利, 遇到了一些问题, 诸如: 如何存储大规模的数据, 如何保证集群的可伸缩性, 如何动态容错等</li>
<li>2013年的时候, Google 发布了三篇论文, 被称作为三驾马车, 其中有一篇叫做 GFS, 是描述了 Google 内部的一个叫做 <a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">GFS</a> 的分布式大规模文件系统, 具有强大的可伸缩性和容错性</li>
<li>Doug Cutting 后来根据 GFS 的论文, 创造了一个新的文件系统, 叫做 HDFS</li>
</ol>
<h2 id="2-HDFS应用场景"><a href="#2-HDFS应用场景" class="headerlink" title="2. HDFS应用场景"></a>2. HDFS应用场景</h2><h3 id="2-1-适合的应用场景"><a href="#2-1-适合的应用场景" class="headerlink" title="2.1 适合的应用场景"></a>2.1 适合的应用场景</h3><ul>
<li>存储非常大的文件：这里非常大指的是几百M、G、或者TB级别，需要<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">高吞吐量</a>，对<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">延时没有要求</a>。</li>
<li>采用流式的数据访问方式: 即<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">一次写入、多次读取</a>，数据集经常从数据源生成或者拷贝一次，然后在其上做很多分析工作 。</li>
<li>运行于商业硬件上: Hadoop不需要特别贵的机器，可运行于普通廉价机器，可以处<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">节约成本</a></li>
<li>需要高<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">容错性</a></li>
<li>为数据存储提供所需的<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">扩展能力</a></li>
</ul>
<h3 id="2-2-不适合的应用场景"><a href="#2-2-不适合的应用场景" class="headerlink" title="2.2 不适合的应用场景"></a>2.2 不适合的应用场景</h3><p>1） 低延时的数据访问<br>对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的,因此可能<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">牺牲延时</a></p>
<p>2）大量小文件<br>文件的元数据保存在<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">NameNode的内存中</a>， 整个文件系统的文件数量会受限于NameNode的内存大小。<br>经验而言，一个文件/目录/文件块一般占有150字节的元数据内存空间。如果有100万个文件，每个文件占用1个文件块，则需要大约300M的内存。因此十亿级别的文件数量在现有商用机器上难以支持。</p>
<p>3）多方读写，需要任意的文件修改<br>HDFS采用追加（append-only）的方式写入数据。<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">不支持文件任意offset的修改</a>。不支持多个写入器（writer）</p>
<h2 id="3-HDFS-的架构"><a href="#3-HDFS-的架构" class="headerlink" title="3. HDFS 的架构"></a>3. HDFS 的架构</h2><p>HDFS是一个<code>主/从（Mater/Slave）体系结构</code>，</p>
<p>HDFS由四部分组成，<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">HDFS Client</a>、<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">NameNod</a><a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">e</a>、<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">DataNode</a>和<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">Secondary NameNode</a>。</p>
<p>　**1、Client：就是客户端。</p>
<ul>
<li>文件切分。文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储。</li>
<li>与 NameNode 交互，获取文件的位置信息。</li>
<li>与 DataNode 交互，读取或者写入数据。</li>
<li>Client 提供一些命令来管理 和访问HDFS，比如启动或者关闭HDFS。</li>
</ul>
<p>　　<strong>2、NameNode：就是 master，它是一个主管、管理者。</strong></p>
<ul>
<li>管理 HDFS 的名称空间</li>
<li>管理数据块（Block）映射信息</li>
<li>配置副本策略</li>
<li>处理客户端读写请求。</li>
</ul>
<p>　　<strong>3、DataNode：就是Slave。NameNode 下达命令，DataNode 执行实际的操作。</strong></p>
<ul>
<li>存储实际的数据块。</li>
<li>执行数据块的读/写操作。</li>
</ul>
<p>　　<strong>4、Secondary NameNode：并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。</strong></p>
<ul>
<li>辅助 NameNode，分担其工作量。</li>
<li>定期合并 fsimage和fsedits，并推送给NameNode。</li>
<li>在紧急情况下，可辅助恢复 NameNode。</li>
</ul>
<h2 id="4-NameNode和DataNode"><a href="#4-NameNode和DataNode" class="headerlink" title="4:NameNode和DataNode"></a>4:NameNode和DataNode</h2><p>###4.1 NameNode作用</p>
<ul>
<li><p>NameNode在内存中保存着整个文件系统的<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">名称</a><a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">空间</a>和文件数据块的<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">地址映射</a></p>
</li>
<li><p>整个HDFS可存储的文件数受限于<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">NameNode的内存大小</a></p>
<p><code>1、NameNode元数据信息</code><br>文件名，文件目录结构，文件属性(生成时间，副本数，权限)每个文件的块列表。<br>以及列表中的块与块所在的DataNode之间的地址映射关系<br>在内存中加载文件系统中每个文件和每个数据块的引用关系(文件、block、datanode之间的映射信息)<br>数据会定期保存到本地磁盘（fsImage文件和edits文件）</p>
</li>
</ul>
<p><code>2、NameNode文件操作</code><br>NameNode负责文件元数据的操作<br>DataNode负责处理文件内容的读写请求，数据流不经过NameNode，会询问它跟那个DataNode联系</p>
<p><code>3、NameNode副本</code><br>文件数据块到底存放到哪些DataNode上，是由NameNode决定的，NN根据全局情况做出放置副本的决定</p>
<p><code>4、NameNode心跳机制</code><br>全权管理数据块的复制，周期性的接受心跳和块的状态报告信息（包含该DataNode上所有数据块的列表）<br>若接受到心跳信息，NameNode认为DataNode工作正常，如果在10分钟后还接受到不到DN的心跳，那么NameNode认为DataNode已经宕机 ,这时候NN准备要把DN上的数据块进行重新的复制。 块的状态报告包含了一个DN上所有数据块的列表，blocks report 每个1小时发送一次.</p>
<h3 id="4-2-DataNode作用"><a href="#4-2-DataNode作用" class="headerlink" title="4.2 DataNode作用"></a>4.2 DataNode作用</h3><p>提供真实文件数据的存储服务。</p>
<p>1、Data Node以数据块的形式存储HDFS文件</p>
<p>2、Data Node 响应HDFS 客户端读写请求</p>
<p>3、Data Node 周期性向NameNode汇报心跳信息</p>
<p>4、Data Node 周期性向NameNode汇报数据块信息</p>
<p>5、Data Node 周期性向NameNode汇报缓存数据块信息</p>
<h2 id="5-HDFS的副本机制和机架感知"><a href="#5-HDFS的副本机制和机架感知" class="headerlink" title="5:HDFS的副本机制和机架感知"></a>5:HDFS的副本机制和机架感知</h2><h3 id="5-1-HDFS-文件副本机制"><a href="#5-1-HDFS-文件副本机制" class="headerlink" title="5.1 HDFS 文件副本机制"></a>5.1 HDFS 文件副本机制</h3><p>所有的文件都是以 block 块的方式存放在 HDFS 文件系统当中,作用如下</p>
<ol>
<li>一个文件有可能大于集群中任意一个磁盘，引入块机制,可以很好的解决这个问题</li>
<li>使用块作为文件存储的逻辑单位可以简化存储子系统</li>
<li>块非常适合用于数据备份进而提供数据容错能力</li>
</ol>
<p>在 Hadoop1 当中, 文件的 block 块默认大小是 64M, hadoop2 当中, 文件的 block 块大小默认是 128M, block 块的大小可以通过 hdfs-site.xml 当中的配置文件进行指定</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.block.size&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;块大小 以字节为单位&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="5-2-机架感知"><a href="#5-2-机架感知" class="headerlink" title="5.2 机架感知"></a>5.2 机架感知</h3><p>HDFS分布式文件系统的内部有一个副本存放策略：以默认的副本数=3为例：</p>
<p>1、第一个副本块存本机</p>
<p>2、第二个副本块存跟本机同机架内的其他服务器节点</p>
<p>3、第三个副本块存不同机架的一个服务器节点上</p>
<h2 id="6、hdfs的命令行使用"><a href="#6、hdfs的命令行使用" class="headerlink" title="6、hdfs的命令行使用"></a>6、hdfs的命令行使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">ls</span><br><span class="line">格式：  hdfs dfs -ls  URI</span><br><span class="line">作用：类似于Linux的ls命令，显示文件列表</span><br><span class="line">hdfs  dfs   -ls  /</span><br><span class="line">lsr</span><br><span class="line">格式  :   hdfs  dfs -lsr URI</span><br><span class="line">作用  : 在整个目录下递归执行ls, 与UNIX中的ls-R类似</span><br><span class="line">hdfs  dfs   -lsr  /</span><br><span class="line">mkdir</span><br><span class="line">格式 ： hdfs  dfs [-p] -mkdir &lt;paths&gt;</span><br><span class="line">作用 :   以&lt;paths&gt;中的URI作为参数，创建目录。使用-p参数可以递归创建目录</span><br><span class="line">put</span><br><span class="line">格式   ： hdfs dfs -put &lt;localsrc &gt;  ... &lt;dst&gt;</span><br><span class="line">作用 ：  将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标文件系统中（&lt;dst&gt;对应的路径）。也可以从标准输入中读取输入，写入目标文件系统中</span><br><span class="line">hdfs dfs -put  /rooot/a.txt  /dir1</span><br><span class="line">moveFromLocal</span><br><span class="line">格式： hdfs  dfs -moveFromLocal  &lt;localsrc&gt;   &lt;dst&gt;</span><br><span class="line">作用:   和put命令类似，但是源文件localsrc拷贝之后自身被删除</span><br><span class="line">hdfs  dfs -moveFromLocal  /root/install.log  /</span><br><span class="line">moveToLocal</span><br><span class="line">未实现</span><br><span class="line">get</span><br><span class="line">格式   hdfs dfs  -get [-ignorecrc ]  [-crc]  &lt;src&gt; &lt;localdst&gt;</span><br><span class="line"></span><br><span class="line">作用：将文件拷贝到本地文件系统。 CRC 校验失败的文件通过-ignorecrc选项拷贝。 文件和CRC校验和可以通过-CRC选项拷贝</span><br><span class="line">hdfs dfs  -get   /install.log  /export/servers</span><br><span class="line">mv</span><br><span class="line">格式  ： hdfs  dfs -mv URI   &lt;dest&gt;</span><br><span class="line">作用： 将hdfs上的文件从原路径移动到目标路径（移动之后文件删除），该命令不能夸文件系统</span><br><span class="line">hdfs  dfs  -mv  /dir1/a.txt   /dir2</span><br><span class="line">rm</span><br><span class="line">格式： hdfs dfs -rm [-r] 【-skipTrash】 URI 【URI 。。。】</span><br><span class="line">作用：   删除参数指定的文件，参数可以有多个。   此命令只删除文件和非空目录。</span><br><span class="line">如果指定-skipTrash选项，那么在回收站可用的情况下，该选项将跳过回收站而直接删除文件；</span><br><span class="line">否则，在回收站可用时，在HDFS Shell 中执行此命令，会将文件暂时放到回收站中。</span><br><span class="line">hdfs  dfs  -rm  -r  /dir1</span><br><span class="line">cp</span><br><span class="line">格式:     hdfs  dfs  -cp URI [URI ...] &lt;dest&gt;</span><br><span class="line">作用：    将文件拷贝到目标路径中。如果&lt;dest&gt;  为目录的话，可以将多个文件拷贝到该目录下。</span><br><span class="line">-f</span><br><span class="line">选项将覆盖目标，如果它已经存在。</span><br><span class="line">-p</span><br><span class="line">选项将保留文件属性（时间戳、所有权、许可、ACL、XAttr）。</span><br><span class="line">hdfs dfs -cp /dir1/a.txt  /dir2/b.txt</span><br><span class="line">cat</span><br><span class="line">hdfs dfs  -cat  URI [uri  ...]</span><br><span class="line">作用：将参数所指示的文件内容输出到stdout</span><br><span class="line">hdfs dfs  -cat /install.log</span><br><span class="line">chmod</span><br><span class="line">格式:      hdfs   dfs  -chmod  [-R]  URI[URI  ...]</span><br><span class="line">作用：    改变文件权限。如果使用  -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。</span><br><span class="line">hdfs dfs -chmod -R 777 /install.log</span><br><span class="line">chown</span><br><span class="line">格式:      hdfs   dfs  -chmod  [-R]  URI[URI  ...]</span><br><span class="line">作用：    改变文件的所属用户和用户组。如果使用  -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。</span><br><span class="line">hdfs  dfs  -chown  -R hadoop:hadoop  /install.log</span><br><span class="line">appendToFile</span><br><span class="line">格式: hdfs dfs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt;</span><br><span class="line">作用: 追加一个或者多个文件到hdfs指定文件中.也可以从命令行读取输入.</span><br><span class="line"> hdfs dfs -appendToFile  a.xml b.xml  /big.xml</span><br></pre></td></tr></table></figure>

<h2 id="7、hdfs的高级使用命令"><a href="#7、hdfs的高级使用命令" class="headerlink" title="7、hdfs的高级使用命令"></a>7、hdfs的高级使用命令</h2><h3 id="7-1、HDFS文件限额配置"><a href="#7-1、HDFS文件限额配置" class="headerlink" title="7. 1、HDFS文件限额配置"></a>7. 1、HDFS文件限额配置</h3><p> 在多人共用HDFS的环境下，配置设置非常重要。特别是在Hadoop处理大量资料的环境，如果没有配额管理，很容易把所有的空间用完造成别人无法存取。Hdfs的配额设定是针对目录而不是针对账号，可以 让每个账号仅操作某一个目录，然后对目录设置配置。</p>
<p> hdfs文件的限额配置允许我们以文件个数，或者文件大小来限制我们在某个目录下上传的文件数量或者文件内容总量，以便达到我们类似百度网盘网盘等限制每个用户允许上传的最大的文件的量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -count -q -h /user/root/dir1  #查看配额信息</span><br></pre></td></tr></table></figure>

<p>所谓的空间限额</p>
<h4 id="7-1-1、数量限额"><a href="#7-1-1、数量限额" class="headerlink" title="7.1.1、数量限额"></a>7.1.1、数量限额</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs  -mkdir -p /user/root/dir    #创建hdfs文件夹</span><br><span class="line">hdfs dfsadmin -setQuota 2  dir      # 给该文件夹下面设置最多上传两个文件，发现只能上传一个文件</span><br><span class="line">hdfs dfsadmin -clrQuota /user/root/dir  # 清除文件数量限制</span><br></pre></td></tr></table></figure>

<h4 id="7-1-2、空间大小限额"><a href="#7-1-2、空间大小限额" class="headerlink" title="7.1.2、空间大小限额"></a>7.1.2、空间<strong>大小限额</strong></h4><p>在设置空间配额时，设置的空间至少是block_size * 3大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -setSpaceQuota 4k /user/root/dir   # 限制空间大小4KB</span><br><span class="line">hdfs dfs -put  /root/a.txt  /user/root/dir</span><br></pre></td></tr></table></figure>

<p>生成任意大小文件的命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dd if=/dev/zero of=1.txt  bs=1M count=2     #生成2M的文件</span><br></pre></td></tr></table></figure>

<p>清除空间配额限制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -clrSpaceQuota /user/root/dir</span><br></pre></td></tr></table></figure>

<h3 id="7-2、hdfs的安全模式"><a href="#7-2、hdfs的安全模式" class="headerlink" title="7.2、hdfs的安全模式"></a>7.2、hdfs的安全模式</h3><p>安全模式是hadoop的一种<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">保护机制</a>，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。</p>
<p>假设我们设置的副本数（即参数dfs.replication）是3，那么在datanode上就应该有3个副本存在，假设只存在2个副本，那么比例就是2/3=0.666。hdfs默认的副本率0.999。我们的副本率0.666明显小于0.999，因此系统会自动的复制副本到其他dataNode，使得副本率不小于0.999。如果系统中有5个副本，超过我们设定的3个副本，那么系统也会删除多于的2个副本。</p>
<p><a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求</a>。在，当整个系统达到安全标准时，HDFS自动离开安全模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">安全模式操作命令</span><br><span class="line">hdfs  dfsadmin  -safemode  get #查看安全模式状态</span><br><span class="line">hdfs  dfsadmin  -safemode  enter #进入安全模式</span><br><span class="line">hdfs  dfsadmin  -safemode  leave #离开安全模式</span><br></pre></td></tr></table></figure>

<h2 id="8-HDFS基准测试"><a href="#8-HDFS基准测试" class="headerlink" title="8. HDFS基准测试"></a>8. HDFS基准测试</h2><p>实际生产环境当中，hadoop的环境搭建完成之后，第一件事情就是进行压力测试，测试我们的集群的读取和写入速度，测试我们的网络带宽是否足够等一些基准测试</p>
<h3 id="8-1-测试写入速度"><a href="#8-1-测试写入速度" class="headerlink" title="8.1 测试写入速度"></a>8.1 测试写入速度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">向HDFS文件系统中写入数据,10个文件,每个文件10MB,文件存放到/benchmarks/TestDFSIO中</span><br><span class="line">hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar  TestDFSIO -write -nrFiles 10  -fileSize 10MB</span><br></pre></td></tr></table></figure>

<p>完成之后查看写入速度结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -text  /benchmarks/TestDFSIO/io_write/part-00000</span><br></pre></td></tr></table></figure>

<h3 id="8-2-测试读取速度"><a href="#8-2-测试读取速度" class="headerlink" title="8.2 测试读取速度"></a>8.2 测试读取速度</h3><p>测试hdfs的读取文件性能</p>
<p>在HDFS文件系统中读入10个文件,每个文件10M</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar  TestDFSIO -read -nrFiles 10 -fileSize 10MB</span><br></pre></td></tr></table></figure>

<p>查看读取果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -text /benchmarks/TestDFSIO/io_read/part-00000</span><br></pre></td></tr></table></figure>

<h3 id="8-3-清除测试数据"><a href="#8-3-清除测试数据" class="headerlink" title="8.3 清除测试数据"></a>8.3 清除测试数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar   TestDFSIO -clean</span><br></pre></td></tr></table></figure>

<h2 id="9-HDFS-文件写入过程"><a href="#9-HDFS-文件写入过程" class="headerlink" title="9.HDFS 文件写入过程"></a>9.HDFS 文件写入过程</h2><p><a href="https://manzhong.github.io/images/hdfs/w.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/w.bmp" alt="img"></a></p>
<ol>
<li>Client 发起文件上传请求, 通过 RPC 与 NameNode 建立通讯, NameNode 检查目标文件是否已存在, 父目录是否存在, 返回是否可以上传</li>
<li>Client 请求第一个 block 该传输到哪些 DataNode 服务器上</li>
<li>NameNode 根据配置文件中指定的备份数量及机架感知原理进行文件分配, 返回可用的 DataNode 的地址如: A, B, C<ul>
<li>Hadoop 在设计时考虑到数据的安全与高效, 数据文件默认在 HDFS 上存放三份, 存储策略为本地一份, 同机架内其它某一节点上一份, 不同机架的某一节点上一份。</li>
</ul>
</li>
<li>Client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline ）, A 收到请求会继续调用 B, 然后 B 调用 C, 将整个 pipeline 建立完成, 后逐级返回 client</li>
<li>Client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存）, 以 packet 为单位（默认64K）, A 收到一个 packet 就会传给 B, B 传给 C. A 每传一个 packet 会放入一个应答队列等待应答</li>
<li>数据被分割成一个个 packet 数据包在 pipeline 上依次传输, 在 pipeline 反方向上, 逐个发送 ack（命令正确应答）, 最终由 pipeline 中第一个 DataNode 节点 A 将 pipelineack 发送给 Client</li>
<li>当一个 block 传输完成之后, Client 再次请求 NameNode 上传第二个 block 到服务 1</li>
</ol>
<h2 id="10-HDFS-文件读取过程"><a href="#10-HDFS-文件读取过程" class="headerlink" title="10.HDFS 文件读取过程"></a>10.HDFS 文件读取过程</h2><p><a href="https://manzhong.github.io/images/hdfs/r.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/r.bmp" alt="img"></a></p>
<ol>
<li>Client向NameNode发起RPC请求，来确定请求文件block所在的位置；</li>
<li>NameNode会视情况返回文件的部分或者全部block列表，对于每个block，NameNode 都会返回含有该 block 副本的 DataNode 地址； 这些返回的 DN 地址，会按照集群拓扑结构得出 DataNode 与客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后；</li>
<li>Client 选取排序靠前的 DataNode 来读取 block，如果客户端本身就是DataNode,那么将从本地直接获取数据(短路读取特性)；</li>
<li>底层上本质是建立 Socket Stream（FSDataInputStream），重复的调用父类 DataInputStream 的 read 方法，直到这个块上的数据读取完毕；</li>
<li>当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表；</li>
<li>读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。</li>
<li>read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据；</li>
<li>最终读取来所有的 block 会合并成一个完整的最终文件。</li>
</ol>
<h2 id="11-HDFS-的元数据辅助管理"><a href="#11-HDFS-的元数据辅助管理" class="headerlink" title="11.HDFS 的元数据辅助管理"></a>11.HDFS 的元数据辅助管理</h2><p>当 Hadoop 的集群当中, NameNode的所有元数据信息都保存在了 FsImage 与 Eidts 文件当中, 这两个文件就记录了所有的数据的元数据信息, 元数据信息的保存目录配置在了 <code>hdfs-site.xml</code> 当中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    </span><br><span class="line">    &lt;value&gt;</span><br><span class="line">        file:///export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas,          	        		file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2</span><br><span class="line">    &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/nn/edits&lt;/value</span><br><span class="line">&lt;/property&gt;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="11-1-FsImage-和-Edits-详解"><a href="#11-1-FsImage-和-Edits-详解" class="headerlink" title="11.1 FsImage 和 Edits 详解"></a>11.1 FsImage 和 Edits 详解</h4><ul>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edits</span><br></pre></td></tr></table></figure>

<ul>
<li><code>edits</code> 存放了客户端最近一段时间的操作日志</li>
<li>客户端对 HDFS 进行写文件时会首先被记录在 <code>edits</code> 文件中</li>
<li><code>edits</code> 修改时元数据也会更新</li>
</ul>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fsimage</span><br></pre></td></tr></table></figure>

<ul>
<li>NameNode 中关于元数据的镜像, 一般称为检查点, <code>fsimage</code> 存放了一份比较完整的元数据信息</li>
<li>因为 <code>fsimage</code> 是 NameNode 的完整的镜像, 如果每次都加载到内存生成树状拓扑结构，这是非常耗内存和CPU, 所以一般开始时对 NameNode 的操作都放在 edits 中</li>
<li><code>fsimage</code> 内容包含了 NameNode 管理下的所有 DataNode 文件及文件 block 及 block 所在的 DataNode 的元数据信息.</li>
<li>随着 <code>edits</code> 内容增大, 就需要在一定时间点和 <code>fsimage</code> 合并</li>
</ul>
</li>
</ul>
<h4 id="11-2-fsimage-中的文件信息查看"><a href="#11-2-fsimage-中的文件信息查看" class="headerlink" title="11.2 fsimage 中的文件信息查看"></a>11.2 fsimage 中的文件信息查看</h4><p>使用命令 <code>hdfs oiv</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas</span><br><span class="line">hdfs oiv -i fsimage_0000000000000000864 -p XML -o hello.xml</span><br></pre></td></tr></table></figure>

<h4 id="11-3-edits-中的文件信息查看"><a href="#11-3-edits-中的文件信息查看" class="headerlink" title="11.3. edits 中的文件信息查看"></a>11.3. edits 中的文件信息查看</h4><p>使用命令 <code>hdfs oev</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas</span><br><span class="line">hdfs oev -i  edits_0000000000000000865-0000000000000000866 -p XML -o myedit.xml</span><br></pre></td></tr></table></figure>

<h4 id="11-4-SecondaryNameNode-如何辅助管理-fsimage-与-edits-文件"><a href="#11-4-SecondaryNameNode-如何辅助管理-fsimage-与-edits-文件" class="headerlink" title="11.4 SecondaryNameNode 如何辅助管理 fsimage 与 edits 文件?"></a>11.4 SecondaryNameNode 如何辅助管理 fsimage 与 edits 文件?</h4><ul>
<li><p>SecondaryNameNode 定期合并 fsimage 和 edits, 把 edits 控制在一个范围内</p>
</li>
<li><p>配置 SecondaryNameNode</p>
<ul>
<li><p>SecondaryNameNode 在 <code>conf/masters</code> 中指定</p>
</li>
<li><p>在 masters 指定的机器上, 修改 <code>hdfs-site.xml</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.http.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;host:50070&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改 <code>core-site.xml</code>, 这一步不做配置保持默认也可以</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 多久记录一次 HDFS 镜像, 默认 1小时 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.checkpoint.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3600&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 一次记录多大, 默认 64M --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.checkpoint.size&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;67108864&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p><a href="https://manzhong.github.io/images/hdfs/s.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/s.bmp" alt="img"></a></p>
<ol>
<li>SecondaryNameNode 通知 NameNode 切换 editlog</li>
<li>SecondaryNameNode 从 NameNode 中获得 fsimage 和 editlog(通过http方式)</li>
<li>SecondaryNameNode 将 fsimage 载入内存, 然后开始合并 editlog, 合并之后成为新的 fsimage</li>
<li>SecondaryNameNode 将新的 fsimage 发回给 NameNode</li>
<li>NameNode 用新的 fsimage 替换旧的 fsimage</li>
</ol>
<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><ul>
<li>完成合并的是 SecondaryNameNode, 会请求 NameNode 停止使用 edits, 暂时将新写操作放入一个新的文件中 <code>edits.new</code></li>
<li>SecondaryNameNode 从 NameNode 中通过 Http GET 获得 edits, 因为要和 fsimage 合并, 所以也是通过 Http Get 的方式把 fsimage 加载到内存, 然后逐一执行具体对文件系统的操作, 与 fsimage 合并, 生成新的 fsimage, 然后通过 Http POST 的方式把 fsimage 发送给 NameNode. NameNode 从 SecondaryNameNode 获得了 fsimage 后会把原有的 fsimage 替换为新的 fsimage, 把 edits.new 变成 edits. 同时会更新 fstime</li>
<li>Hadoop 进入安全模式时需要管理员使用 dfsadmin 的 save namespace 来创建新的检查点</li>
<li>SecondaryNameNode 在合并 edits 和 fsimage 时需要消耗的内存和 NameNode 差不多, 所以一般把 NameNode 和 SecondaryNameNode 放在不同的机器上</li>
</ul>
<h2 id="1-HDFS-的-API-操作"><a href="#1-HDFS-的-API-操作" class="headerlink" title="1:HDFS 的 API 操作"></a>1:HDFS 的 API 操作</h2><h3 id="1-1-配置Windows下Hadoop环境"><a href="#1-1-配置Windows下Hadoop环境" class="headerlink" title="1.1 配置Windows下Hadoop环境"></a>1.1 配置Windows下Hadoop环境</h3><p>在windows系统需要配置hadoop运行环境，否则直接运行代码会出现以下问题:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">缺少winutils.exe</span><br><span class="line">Could not locate executable null \bin\winutils.exe in the hadoop binaries</span><br><span class="line">缺少hadoop.dll</span><br><span class="line">Unable to load native-hadoop library for your platform… using builtin-Java classes where applicable</span><br></pre></td></tr></table></figure>

<p>步骤:</p>
<p>第一步：将hadoop2.7.5文件夹拷贝到一个没有中文没有空格的路径下面</p>
<p>第二步：在windows上面配置hadoop的环境变量： HADOOP_HOME，并将%HADOOP_HOME%\bin添加到path中</p>
<p>第三步：把hadoop2.7.5文件夹中bin目录下的hadoop.dll文件放到系统盘: C:\Windows\System32 目录</p>
<p>第四步：关闭windows重启</p>
<h3 id="1-2-导入-Maven-依赖"><a href="#1-2-导入-Maven-依赖" class="headerlink" title="1.2 导入 Maven 依赖"></a>1.2 导入 Maven 依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">	 &lt;dependencies&gt;</span><br><span class="line">    	&lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">   &lt;/dependency&gt;</span><br><span class="line">      &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">      &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">      &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;RELEASE&lt;/version&gt;</span><br><span class="line">      &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">  &lt;build&gt;</span><br><span class="line">      &lt;plugins&gt;</span><br><span class="line">          &lt;plugin&gt;</span><br><span class="line">              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">              &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">              &lt;version&gt;3.1&lt;/version&gt;</span><br><span class="line">              &lt;configuration&gt;</span><br><span class="line">                  &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                  &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                  &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">                  &lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span><br><span class="line">              &lt;/configuration&gt;</span><br><span class="line">          &lt;/plugin&gt;</span><br><span class="line">          &lt;plugin&gt;</span><br><span class="line">              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">              &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">              &lt;version&gt;2.4.3&lt;/version&gt;</span><br><span class="line">              &lt;executions&gt;</span><br><span class="line">                  &lt;execution&gt;</span><br><span class="line">                      &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                      &lt;goals&gt;</span><br><span class="line">                          &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                      &lt;/goals&gt;</span><br><span class="line">                      &lt;configuration&gt;</span><br><span class="line">                          &lt;minimizeJar&gt;true&lt;/minimizeJar&gt;</span><br><span class="line">                      &lt;/configuration&gt;</span><br><span class="line">                  &lt;/execution&gt;</span><br><span class="line">              &lt;/executions&gt;</span><br><span class="line">          &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">      &lt;/plugins&gt;</span><br><span class="line">  &lt;/build&gt;</span><br></pre></td></tr></table></figure>

<h3 id="1-3-使用url方式访问数据（了解）"><a href="#1-3-使用url方式访问数据（了解）" class="headerlink" title="1.3 使用url方式访问数据（了解）"></a>1.3 使用url方式访问数据（了解）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void demo1()throws  Exception&#123;</span><br><span class="line">    //第一步：注册hdfs 的url</span><br><span class="line">    URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());</span><br><span class="line"></span><br><span class="line">    //获取文件输入流</span><br><span class="line">    InputStream inputStream  = new URL(&quot;hdfs://node01:8020/a.txt&quot;).openStream();</span><br><span class="line">    //获取文件输出流</span><br><span class="line">    FileOutputStream outputStream = new FileOutputStream(new File(&quot;D:\\hello.txt&quot;));</span><br><span class="line"></span><br><span class="line">    //实现文件的拷贝</span><br><span class="line">    IOUtils.copy(inputStream, outputStream);</span><br><span class="line"></span><br><span class="line">    //关闭流</span><br><span class="line">    IOUtils.closeQuietly(inputStream);</span><br><span class="line">    IOUtils.closeQuietly(outputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="1-4-使用文件系统方式访问数据（掌握）"><a href="#1-4-使用文件系统方式访问数据（掌握）" class="headerlink" title="1.4 使用文件系统方式访问数据（掌握）"></a>1.4 <strong>使用</strong>文件系统方式访问数据（掌握）</h3><h4 id="1-4-1-涉及的主要类"><a href="#1-4-1-涉及的主要类" class="headerlink" title="1.4.1 涉及的主要类"></a>1.4.1 涉及的主要类</h4><p>在 Java 中操作 HDFS, 主要涉及以下 Class:</p>
<ul>
<li><p><code>Configuration</code></p>
<ul>
<li>该类的对象封转了客户端或者服务器的配置</li>
</ul>
</li>
<li><p><code>FileSystem</code></p>
<ul>
<li><p>该类的对象是一个文件系统对象, 可以用该对象的一些方法来对文件进行操作, 通过 FileSystem 的静态方法 get 获得该对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileSystem fs = FileSystem.get(conf)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>get</code> 方法从 <code>conf</code> 中的一个参数 <code>fs.defaultFS</code> 的配置值判断具体是什么类型的文件系统</li>
<li>如果我们的代码中没有指定 <code>fs.defaultFS</code>, 并且工程 ClassPath 下也没有给定相应的配置, <code>conf</code> 中的默认值就来自于 Hadoop 的 Jar 包中的 <code>core-default.xml</code></li>
<li>默认值为 <code>file:///</code>, 则获取的不是一个 DistributedFileSystem 的实例, 而是一个本地文件系统的客户端对象</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-4-2-获取-FileSystem-的几种方式"><a href="#1-4-2-获取-FileSystem-的几种方式" class="headerlink" title="1.4.2 获取 FileSystem 的几种方式"></a>1.4.2 获取 FileSystem 的几种方式</h4><ul>
<li>第一种方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileSystem1() throws IOException &#123;</span><br><span class="line">    Configuration configuration = new Configuration();</span><br><span class="line">    //指定我们使用的文件系统类型:</span><br><span class="line">    configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://node01:8020/&quot;);</span><br><span class="line"></span><br><span class="line">    //获取指定的文件系统</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line">    System.out.println(fileSystem.toString());</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第二种方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileSystem2() throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    System.out.println(&quot;fileSystem:&quot;+fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第三种方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileSystem3() throws  Exception&#123;</span><br><span class="line">    Configuration configuration = new Configuration();</span><br><span class="line">    configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://node01:8020&quot;);</span><br><span class="line">    FileSystem fileSystem = FileSystem.newInstance(configuration);</span><br><span class="line">    System.out.println(fileSystem.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第四种方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//@Test</span><br><span class="line">public void getFileSystem4() throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.newInstance(new URI(&quot;hdfs://node01:8020&quot;) ,new Configuration());</span><br><span class="line">    System.out.println(fileSystem.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-3-遍历-HDFS-中所有文件"><a href="#1-4-3-遍历-HDFS-中所有文件" class="headerlink" title="1.4.3 遍历 HDFS 中所有文件"></a>1.4.3 遍历 HDFS 中所有文件</h4><ul>
<li>使用 API 遍历</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void listMyFiles()throws Exception&#123;</span><br><span class="line">    //获取fileSystem类</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    //获取RemoteIterator 得到所有的文件或者文件夹，第一个参数指定遍历的路径，第二个参数表示是否要递归遍历</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; locatedFileStatusRemoteIterator = fileSystem.listFiles(new Path(&quot;/&quot;), true);</span><br><span class="line">    while (locatedFileStatusRemoteIterator.hasNext())&#123;</span><br><span class="line">        LocatedFileStatus next = locatedFileStatusRemoteIterator.next();</span><br><span class="line">        System.out.println(next.getPath().toString());</span><br><span class="line">    &#125;</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-4-HDFS-上创建文件夹"><a href="#1-4-4-HDFS-上创建文件夹" class="headerlink" title="1.4.4 HDFS 上创建文件夹"></a>1.4.4 HDFS 上创建文件夹</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void mkdirs() throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    boolean mkdirs = fileSystem.mkdirs(new Path(&quot;/hello/mydir/test&quot;));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-4-下载文件"><a href="#1-4-4-下载文件" class="headerlink" title="1.4.4 下载文件"></a>1.4.4 下载文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileToLocal()throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(new Path(&quot;/timer.txt&quot;));</span><br><span class="line">    FileOutputStream  outputStream = new FileOutputStream(new File(&quot;e:\\timer.txt&quot;));</span><br><span class="line">    IOUtils.copy(inputStream,outputStream );</span><br><span class="line">    IOUtils.closeQuietly(inputStream);</span><br><span class="line">    IOUtils.closeQuietly(outputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-5-HDFS-文件上传"><a href="#1-4-5-HDFS-文件上传" class="headerlink" title="1.4.5 HDFS 文件上传"></a>1.4.5 HDFS 文件上传</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void putData() throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    fileSystem.copyFromLocalFile(new Path(&quot;file:///c:\\install.log&quot;),new Path(&quot;/hello/mydir/test&quot;));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-6-hdfs访问权限控制"><a href="#1-4-6-hdfs访问权限控制" class="headerlink" title="1.4.6 hdfs访问权限控制"></a>1.4.6 hdfs访问权限控制</h4><ol>
<li>停止hdfs集群，在node01机器上执行以下命令</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5</span><br><span class="line">sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>

<ol>
<li>修改node01机器上的hdfs-site.xml当中的配置文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5/etc/hadoop</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol>
<li>修改完成之后配置文件发送到其他机器上面去</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp hdfs-site.xml node02:$PWD</span><br><span class="line">scp hdfs-site.xml node03:$PWD</span><br></pre></td></tr></table></figure>

<ol>
<li>重启hdfs集群</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5</span><br><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<ol>
<li>随意上传一些文件到我们hadoop集群当中准备测试使用</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5/etc/hadoop</span><br><span class="line">hdfs dfs -mkdir /config</span><br><span class="line">hdfs dfs -put *.xml /config</span><br><span class="line">hdfs dfs -chmod 600 /config/core-site.xml</span><br></pre></td></tr></table></figure>

<ol>
<li>使用代码准备下载文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getConfig()throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration(),&quot;hadoop&quot;);</span><br><span class="line">    fileSystem.copyToLocalFile(new Path(&quot;/config/core-site.xml&quot;),new Path(&quot;file:///c:/core-site.xml&quot;));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-7-小文件合并"><a href="#1-4-7-小文件合并" class="headerlink" title="1.4.7 小文件合并"></a>1.4.7 小文件合并</h4><p>由于 Hadoop 擅长存储大文件，因为大文件的元数据信息比较少，如果 Hadoop 集群当中有大量的小文件，那么每个小文件都需要维护一份元数据信息，会大大的增加集群管理元数据的内存压力，所以在实际工作当中，如果有必要一定要将小文件合并成大文件进行一起处理</p>
<p>在我们的 HDFS 的 Shell 命令模式下，可以通过命令行将很多的 hdfs 文件合并成一个大文件下载到本地</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">hdfs dfs -getmerge /config/*.xml ./hello.xml</span><br></pre></td></tr></table></figure>

<p>既然可以在下载的时候将这些小文件合并成一个大文件一起下载，那么肯定就可以在上传的时候将小文件合并到一个大文件里面去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void mergeFile() throws  Exception&#123;</span><br><span class="line">    //获取分布式文件系统</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://192.168.52.250:8020&quot;), new Configuration(),&quot;root&quot;);</span><br><span class="line">    FSDataOutputStream outputStream = fileSystem.create(new Path(&quot;/bigfile.txt&quot;));</span><br><span class="line">    //获取本地文件系统</span><br><span class="line">    LocalFileSystem local = FileSystem.getLocal(new Configuration());</span><br><span class="line">    //通过本地文件系统获取文件列表，为一个集合</span><br><span class="line">    FileStatus[] fileStatuses = local.listStatus(new Path(&quot;file:///E:\\input&quot;));</span><br><span class="line">    for (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        FSDataInputStream inputStream = local.open(fileStatus.getPath());</span><br><span class="line">       IOUtils.copy(inputStream,outputStream);</span><br><span class="line">        IOUtils.closeQuietly(inputStream);</span><br><span class="line">    &#125;</span><br><span class="line">    IOUtils.closeQuietly(outputStream);</span><br><span class="line">    local.close();</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2：HDFS的高可用机制"><a href="#2：HDFS的高可用机制" class="headerlink" title="2：HDFS的高可用机制"></a>2：HDFS的高可用机制</h2><h3 id="2-1-HDFS高可用介绍"><a href="#2-1-HDFS高可用介绍" class="headerlink" title="2.1 HDFS高可用介绍"></a>2.1 HDFS高可用介绍</h3><p>在Hadoop 中，NameNode 所处的位置是非常重要的，整个HDFS文件系统的元数据信息都由NameNode 来管理，NameNode的可用性直接决定了Hadoop 的可用性，一旦NameNode进程不能工作了，就会影响整个集群的正常使用。</p>
<p>在典型的HA集群中，两台独立的机器被配置为NameNode。在工作集群中，NameNode机器中的一个处于Active状态，另一个处于Standby状态。Active NameNode负责群集中的所有客户端操作，而Standby充当从服务器。Standby机器保持足够的状态以提供快速故障切换（如果需要）。</p>
<h3 id="2-2-组件介绍"><a href="#2-2-组件介绍" class="headerlink" title="2.2 组件介绍"></a>2.2 组件介绍</h3><p><a href="https://manzhong.github.io/images/hdfs/ha.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/ha.jpg" alt="img"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ZKFailoverController</span><br></pre></td></tr></table></figure>

<p>是基于Zookeeper的故障转移控制器，它负责控制NameNode的主备切换，ZKFailoverController会监测NameNode的健康状态，当发现Active NameNode出现异常时会通过Zookeeper进行一次新的选举，完成Active和Standby状态的切换</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HealthMonitor</span><br></pre></td></tr></table></figure>

<p>周期性调用NameNode的HAServiceProtocol RPC接口（monitorHealth 和 getServiceStatus），监控NameNode的健康状态并向ZKFailoverController反馈</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ActiveStandbyElector</span><br></pre></td></tr></table></figure>

<p>接收ZKFC的选举请求，通过Zookeeper自动完成主备选举，选举完成后回调ZKFailoverController的主备切换方法对NameNode进行Active和Standby状态的切换.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataNode</span><br></pre></td></tr></table></figure>

<p>NameNode包含了HDFS的元数据信息和数据块信息（blockmap），其中数据块信息通过DataNode主动向Active NameNode和Standby NameNode上报</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">共享存储系统</span><br></pre></td></tr></table></figure>

<p>共享存储系统负责存储HDFS的元数据（EditsLog），Active NameNode（写入）和 Standby NameNode（读取）通过共享存储系统实现元数据同步，在主备切换过程中，新的Active NameNode必须确保元数据同步完成才能对外提供服务</p>
<h2 id="3-Hadoop的联邦机制-Federation"><a href="#3-Hadoop的联邦机制-Federation" class="headerlink" title="3: Hadoop的联邦机制(Federation)"></a>3: Hadoop的联邦机制(Federation)</h2><p><a href="https://manzhong.github.io/images/hdfs/lb.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/lb.jpg" alt="img"></a></p>
<h3 id="3-1背景概述"><a href="#3-1背景概述" class="headerlink" title="3.1背景概述"></a>3.1<strong>背景概述</strong></h3><p>单NameNode的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NameNode进程使用的内存可能会达到上百G，NameNode成为了性能的瓶颈。因而提出了namenode水平扩展方案– Federation。</p>
<p>Federation中文意思为联邦,联盟，是NameNode的Federation,也就是会有多个NameNode。多个NameNode的情况意味着有多个namespace(命名空间)，区别于HA模式下的多NameNode，它们是拥有着同一个namespace。既然说到了NameNode的命名空间的概念,</p>
<p>我们可以很明显地看出现有的HDFS数据管理,数据存储2层分层的结构.也就是说,所有关于存储数据的信息和管理是放在NameNode这边,而真实数据的存储则是在各个DataNode下.而这些隶属于同一个NameNode所管理的数据都是在同一个命名空间下的.而一个namespace对应一个block pool。Block Pool是同一个namespace下的block的集合.当然这是我们最常见的单个namespace的情况,也就是一个NameNode管理集群中所有元数据信息的时候.如果我们遇到了之前提到的NameNode内存使用过高的问题,这时候怎么办?元数据空间依然还是在不断增大,一味调高NameNode的jvm大小绝对不是一个持久的办法.这时候就诞生了HDFS Federation的机制.</p>
<h3 id="3-2-Federation架构设计"><a href="#3-2-Federation架构设计" class="headerlink" title="3.2 Federation架构设计"></a>3.2 <strong>Federation架构设计</strong></h3><p>HDFS Federation是解决namenode内存瓶颈问题的水平横向扩展方案。</p>
<p>Federation意味着在集群中将会有多个namenode/namespace。这些namenode之间是联合的，也就是说，他们之间相互独立且不需要互相协调，各自分工，管理自己的区域。分布式的datanode被用作通用的数据块存储存储设备。每个datanode要向集群中所有的namenode注册，且周期性地向所有namenode发送心跳和块报告，并执行来自所有namenode的命令。</p>
<p>Federation一个典型的例子就是上面提到的NameNode内存过高问题,我们完全可以将上面部分大的文件目录移到另外一个NameNode上做管理.<strong>更重要的一点在于,这些NameNode是共享集群中所有的DataNode的,它们还是在同一个集群内的**</strong>。**</p>
<p>这时候在DataNode上就不仅仅存储一个Block Pool下的数据了,而是多个(在DataNode的datadir所在目录里面查看BP-xx.xx.xx.xx打头的目录)。</p>
<p><strong>概括起来：</strong></p>
<p>多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务。</p>
<p>每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储。</p>
<p>DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况。</p>
<p><strong>HDFS Federation不足</strong></p>
<p>HDFS Federation并没有完全解决单点故障问题。虽然namenode/namespace存在多个，但是从单个namenode/namespace看，仍然存在单点故障：如果某个namenode挂掉了，其管理的相应的文件便不可以访问。Federation中每个namenode仍然像之前HDFS上实现一样，配有一个secondary namenode，以便主namenode挂掉一下，用于还原元数据信息。</p>
<p>所以一般集群规模真的很大的时候，会采用HA+Federation的部署方案。也就是每个联合的namenodes都是ha的。</p>
<p>总结:</p>
<p>主备:解决了单点故障,形成高可用.</p>
<p>联邦: 解决了namenode的内存问题.</p>
<p>namenode的可用性决定了Hadoop的可用性</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Hdfs/" data-id="cjz24s2xq000au8u56mlipy24" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Hadoop" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Hadoop/" class="article-date">
  <time datetime="2019-08-08T03:30:17.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Hadoop/">Hadoop</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="一概述"><a href="#一概述" class="headerlink" title="一概述"></a>一概述</h2><p>狭义上来说Hadoop就是:</p>
<ul>
<li>HDFS ：分布式文件系统</li>
<li>MapReduce : 分布式计算系统</li>
<li>Yarn：分布式样集群资源管理</li>
</ul>
<p>广义上来说:</p>
<p>Hadoop指代大数据的一个生态圈,包括很多其他软件:</p>
<p><a href="https://manzhong.github.io/images/hadoop/1558225014064.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hadoop/1558225014064.png" alt="img"></a></p>
<h3 id="历史版本与发行公司"><a href="#历史版本与发行公司" class="headerlink" title="历史版本与发行公司"></a>历史版本与发行公司</h3><p>2.1 Hadoop历史版本</p>
<p>1.x版本系列：hadoop版本当中的第二代开源版本，主要修复0.x版本的一些bug等</p>
<p>2.x版本系列：架构产生重大变化，引入了yarn平台等许多新特性</p>
<p>3.x版本系列: 加入多namenoode新特性</p>
<h5 id="2-2-Hadoop三大发行版公司"><a href="#2-2-Hadoop三大发行版公司" class="headerlink" title="2.2 Hadoop三大发行版公司"></a>2.2 Hadoop三大发行版公司</h5><ul>
<li>免费开源版本apache:</li>
</ul>
<p><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">http://hadoop.apache.org/</a></p>
<p>优点：拥有全世界的开源贡献者，代码更新迭代版本比较快，</p>
<p>缺点：版本的升级，版本的维护，版本的兼容性，版本的补丁都可能考虑不太周到，</p>
<p>apache所有软件的下载地址（包括各种历史版本）：</p>
<p><a href="http://archive.apache.org/dist/" target="_blank" rel="noopener">http://archive.apache.org/dist/</a></p>
<ul>
<li>免费开源版本hortonWorks：</li>
</ul>
<p><a href="https://hortonworks.com/" target="_blank" rel="noopener">https://hortonworks.com/</a></p>
<p>hortonworks主要是雅虎主导Hadoop开发的副总裁，带领二十几个核心成员成立Hortonworks，核心产品软件HDP（ambari），HDF免费开源，并且提供一整套的web管理界面，供我们可以通过web界面管理我们的集群状态，web管理界面软件HDF网址（<a href="http://ambari.apache.org/" target="_blank" rel="noopener">http://ambari.apache.org/</a>）</p>
<ul>
<li>软件收费版本ClouderaManager:</li>
</ul>
<p><a href="https://www.cloudera.com/" target="_blank" rel="noopener">https://www.cloudera.com/</a></p>
<p>cloudera主要是美国一家大数据公司在apache开源hadoop的版本上，通过自己公司内部的各种补丁，实现版本之间的稳定运行，大数据生态圈的各个版本的软件都提供了对应的版本，解决了版本的升级困难，版本兼容性等各种问题</p>
<h2 id="二架构"><a href="#二架构" class="headerlink" title="二架构"></a>二架构</h2><h4 id="1-x的版本架构模型介绍"><a href="#1-x的版本架构模型介绍" class="headerlink" title="1.x的版本架构模型介绍"></a>1.x的版本架构模型介绍</h4><p><a href="https://manzhong.github.io/images/hadoop/1558232908565.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hadoop/1558232908565.png" alt="img"></a></p>
<p>文件系统核心模块：</p>
<p>NameNode：集群当中的主节点，管理元数据(文件的大小，文件的位置，文件的权限)，主要用于管理集群当中的各种数据</p>
<p>secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理</p>
<p>DataNode：集群当中的从节点，主要用于存储集群当中的各种数据</p>
<p>数据计算核心模块：</p>
<p>JobTracker：接收用户的计算请求任务，并分配任务给从节点</p>
<p>TaskTracker：负责执行主节点JobTracker分配的任务</p>
<h4 id="2-x的版本架构模型介绍"><a href="#2-x的版本架构模型介绍" class="headerlink" title="2.x的版本架构模型介绍"></a>2.x的版本架构模型介绍</h4><p>引入了yarn,其中MapReduce运行在yarn中</p>
<p><strong>第一种：NameNode与ResourceManager单节点架构模型</strong></p>
<p><a href="https://manzhong.github.io/images/hadoop/1558232924095.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hadoop/1558232924095.png" alt="img"></a></p>
<p>文件系统核心模块：</p>
<p>NameNode：集群当中的主节点，主要用于管理集群当中的各种数据</p>
<p>secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理</p>
<p>DataNode：集群当中的从节点，主要用于存储集群当中的各种数据</p>
<p>数据计算核心模块：</p>
<p>ResourceManager：接收用户的计算请求任务，并负责集群的资源分配</p>
<p>NodeManager：负责执行主节点APPmaster分配的任务</p>
<p><strong>第二种：NameNode单节点与ResourceManager高可用架构模型</strong></p>
<p><a href="https://manzhong.github.io/images/hadoop/1558232966712.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hadoop/1558232966712.png" alt="img"></a></p>
<p>文件系统核心模块：</p>
<p>NameNode：集群当中的主节点，主要用于管理集群当中的各种数据</p>
<p>secondaryNameNode：主要能用于hadoop当中元数据信息的辅助管理</p>
<p>DataNode：集群当中的从节点，主要用于存储集群当中的各种数据</p>
<p>数据计算核心模块：</p>
<p>ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分，通过zookeeper实现ResourceManager的高可用</p>
<p>NodeManager：负责执行主节点ResourceManager分配的任务</p>
<p><strong>第三种：NameNode高可用与ResourceManager单节点架构模型</strong></p>
<p><a href="https://manzhong.github.io/images/hadoop/1558232980575.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hadoop/1558232980575.png" alt="img"></a></p>
<p>文件系统核心模块：</p>
<p>NameNode：集群当中的主节点，主要用于管理集群当中的各种数据，其中nameNode可以有两个，形成高可用状态</p>
<p>DataNode：集群当中的从节点，主要用于存储集群当中的各种数据</p>
<p>JournalNode：文件系统元数据信息管理</p>
<p>数据计算核心模块：</p>
<p>ResourceManager：接收用户的计算请求任务，并负责集群的资源分配，以及计算任务的划分</p>
<p>NodeManager：负责执行主节点ResourceManager分配的任务</p>
<p><strong>第四种：NameNode高可用与ResourceManager高可用架构模型</strong></p>
<p><a href="https://manzhong.github.io/images/hadoop/1558232995675.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hadoop/1558232995675.png" alt="img"></a></p>
<p>文件系统核心模块：</p>
<p>NameNode：集群当中的主节点，主要用于管理集群当中的各种数据，一般都是使用两个，实现HA高可用</p>
<p>JournalNode：元数据信息管理进程，一般都是奇数个</p>
<p>DataNode：从节点，用于数据的存储</p>
<p>数据计算核心模块：</p>
<p>ResourceManager：Yarn平台的主节点，主要用于接收各种任务，通过两个，构建成高可用</p>
<p>NodeManager：Yarn平台的从节点，主要用于处理ResourceManager分配的任务</p>
<h2 id="三Apache-hadoop-编译"><a href="#三Apache-hadoop-编译" class="headerlink" title="三Apache hadoop 编译"></a>三Apache hadoop 编译</h2><h2 id="四-安装Apache-Hadoop"><a href="#四-安装Apache-Hadoop" class="headerlink" title="四 安装Apache Hadoop"></a>四 安装Apache Hadoop</h2><p>例如 以三台服务为例:</p>
<p><strong>节点规划:</strong></p>
<table>
<thead>
<tr>
<th align="left">服务器IP</th>
<th align="left">192.168.174.***</th>
<th align="left">192.168.174.***</th>
<th align="left">192.168.174.***</th>
</tr>
</thead>
<tbody><tr>
<td align="left">主机名</td>
<td align="left">node01</td>
<td align="left">node02</td>
<td align="left">node03</td>
</tr>
<tr>
<td align="left">NameNode</td>
<td align="left">是</td>
<td align="left">否</td>
<td align="left">否</td>
</tr>
<tr>
<td align="left">SecondaryNameNode</td>
<td align="left">是</td>
<td align="left">否</td>
<td align="left">否</td>
</tr>
<tr>
<td align="left">dataNode</td>
<td align="left">是</td>
<td align="left">是</td>
<td align="left">是</td>
</tr>
<tr>
<td align="left">ResourceManager</td>
<td align="left">是</td>
<td align="left">否</td>
<td align="left">否</td>
</tr>
<tr>
<td align="left">NodeManager</td>
<td align="left">是</td>
<td align="left">是</td>
<td align="left">是</td>
</tr>
</tbody></table>
<p>解压:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.5.tar.gz -C ../servers/</span><br></pre></td></tr></table></figure>

<h3 id="2-修改配置文件"><a href="#2-修改配置文件" class="headerlink" title="2 修改配置文件"></a>2 修改配置文件</h3><p>配置文件位置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd  /export/servers/hadoop-2.7.5/etc/hadoop</span><br><span class="line">vim  core-site.xml</span><br></pre></td></tr></table></figure>

<p>修改core-site.xml文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;!--  指定集群的文件系统类型:分布式文件系统 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;hdfs://node01:8020&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;!--  指定临时文件存储目录 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;/export/servers/hadoop-2.7.5/hadoopDatas/tempDatas&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;!--  缓冲区大小，实际工作中根据服务器性能动态调整 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!--  开启hdfs的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，单位分钟 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">	 &lt;property&gt;</span><br><span class="line">			&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">			&lt;value&gt;node01:50090&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 指定namenode的访问地址和端口 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;node01:50070&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;!-- 指定namenode元数据的存放位置 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas,file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;!--  定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割  --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas,file:///export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	</span><br><span class="line">	&lt;!-- 指定namenode日志文件的存放目录 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/nn/edits&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/snn/name&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;!-- 文件切片的副本个数--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;3&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 设置HDFS的文件权限--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 设置一个文件切片的大小：128M--&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.blocksize&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;134217728&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改hadoop-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/export/servers/jdk1.8.0_141</span><br></pre></td></tr></table></figure>

<p>修改mapred-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    </span><br><span class="line">     &lt;!-- 如果启用了该功能，则会将一个“小的application”的所有子task在同一个JVM里面执行，达到JVM重用的目的。这个JVM便是负责该application的ApplicationMaster所用的JVM --&gt;</span><br><span class="line">  &lt;!--mapreduce.job.ubertask.maxmaps | 9 | map任务数的阀值，如果一个application包含的map数小于该值的定义，那么该application就会被认为是一个小的application--&gt;</span><br><span class="line">  &lt;!-- mapreduce.job.ubertask.maxreduces | 1 | reduce任务数的阀值，如果一个application包含的reduce数小于该值的定义，那么该application就会被认为是一个小的application。不过目前Yarn不支持该值大于1的情况 --&gt;</span><br><span class="line">  &lt;!-- mapreduce.job.ubertask.maxbytes | | application的输入大小的阀值。默认为dfs.block.size的值。当实际的输入大小部超过该值的设定，便会认为该application为一个小的application。 --&gt;</span><br><span class="line">    </span><br><span class="line">	&lt;!-- 开启MapReduce小任务模式 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	</span><br><span class="line">	&lt;!-- 设置历史任务的主机和端口 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;node01:10020&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;!-- 设置网页访问历史任务的主机和端口 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;node01:19888&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;!-- 配置yarn主节点的位置 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;node01&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	</span><br><span class="line">	&lt;!-- 开启日志聚合功能 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;!-- 设置聚合日志在hdfs上的保存时间 --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;!-- 设置yarn集群的内存分配方案 --&gt;</span><br><span class="line">	&lt;property&gt;    </span><br><span class="line">		&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;    </span><br><span class="line">		&lt;value&gt;20480&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;  </span><br><span class="line">        	 &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">         	&lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">		&lt;value&gt;2.1&lt;/value&gt;</span><br><span class="line">	&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>修改mapred-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/export/servers/jdk1.8.0_141</span><br></pre></td></tr></table></figure>

<p>修改slaves</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node01</span><br><span class="line">node02</span><br><span class="line">node03</span><br></pre></td></tr></table></figure>

<p>创建目录:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/tempDatas</span><br><span class="line">mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas</span><br><span class="line">mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2</span><br><span class="line">mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas</span><br><span class="line">mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/datanodeDatas2</span><br><span class="line">mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/nn/edits</span><br><span class="line">mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/snn/name</span><br><span class="line">mkdir -p /export/servers/hadoop-2.7.5/hadoopDatas/dfs/snn/edits</span><br></pre></td></tr></table></figure>

<p>安装包分发:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r hadoop-2.7.5 node02:$PWD</span><br><span class="line">scp -r hadoop-2.7.5 node03:$PWD</span><br></pre></td></tr></table></figure>

<p>配置Hadoop环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim  /etc/profile</span><br><span class="line">export HADOOP_HOME=/export/servers/hadoop-2.7.5</span><br><span class="line">export PATH=:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="第四步：启动集群"><a href="#第四步：启动集群" class="headerlink" title="第四步：启动集群"></a>第四步：启动集群</h4><p>要启动 Hadoop 集群，需要启动 HDFS 和 YARN 两个模块。<br>注意： 首次启动 HDFS 时，必须对其进行格式化操作。 本质上是一些清理和<br>准备工作，因为此时的 HDFS 在物理上还是不存在的。</p>
<p>hdfs namenode -format 或者 hadoop namenode –format</p>
<p>准备启动</p>
<p>第一台机器执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd  /export/servers/hadoop-2.7.5/</span><br><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/start-dfs.sh</span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Hadoop/" data-id="cjz24s2wx0005u8u5ti4fgkjn" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-MapReduce" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/MapReduce/" class="article-date">
  <time datetime="2019-08-08T03:22:02.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/MapReduce/">MapReduce</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="hadoop的核心-MapReduce"><a href="#hadoop的核心-MapReduce" class="headerlink" title="hadoop的核心 MapReduce"></a>hadoop的核心 MapReduce</h1><h2 id="1-MapReduce-介绍"><a href="#1-MapReduce-介绍" class="headerlink" title="1. MapReduce 介绍"></a>1. MapReduce 介绍</h2><p>MapReduce思想在生活中处处可见。或多或少都曾接触过这种思想。MapReduce的思想核心是“分而治之”，适用于大量复杂的任务处理场景（大规模数据处理场景）。</p>
<ul>
<li>Map负责“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。可以进行拆分的前提<strong>是这些小任务可以并行计算，彼此间几乎没有依赖关系</strong>。</li>
<li>Reduce负责“合”，即对map阶段的结果进行全局汇总。</li>
<li>MapReduce运行在yarn集群<ol>
<li>ResourceManager</li>
<li>NodeManager</li>
</ol>
</li>
</ul>
<p>这两个阶段合起来正是MapReduce思想的体现。</p>
<p>还有一个比较形象的语言解释MapReduce:</p>
<p>我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。</p>
<p>现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。</p>
<h3 id="1-1-MapReduce-设计构思"><a href="#1-1-MapReduce-设计构思" class="headerlink" title="1.1. MapReduce 设计构思"></a>1.1. MapReduce 设计构思</h3><p>MapReduce是一个分布式运算程序的编程框架，核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在Hadoop集群上。</p>
<p>MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。为程序员提供一个抽象和高层的编程接口和框架。程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的程序代码。如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理：</p>
<p>Map和Reduce为程序员提供了一个清晰的操作接口抽象描述。MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现.Map和Reduce,MapReduce处理的数据类型是&lt;key,value&gt;键值对。</p>
<ul>
<li>Map: <code>(k1; v1) → [(k2; v2)]</code></li>
<li>Reduce: <code>(k2; [v2]) → [(k3; v3)]</code></li>
</ul>
<p>一个完整的mapreduce程序在分布式运行时有三类实例进程：</p>
<ol>
<li><code>MRAppMaster</code> 负责整个程序的过程调度及状态协调</li>
<li><code>MapTask</code> 负责map阶段的整个数据处理流程</li>
<li><code>ReduceTask</code> 负责reduce阶段的整个数据处理流程</li>
</ol>
<h2 id="2-MapReduce-编程规范"><a href="#2-MapReduce-编程规范" class="headerlink" title="2. MapReduce 编程规范"></a>2. MapReduce 编程规范</h2><p>MapReduce 的开发一共有八个步骤, 其中 Map 阶段分为 2 个步骤，Shuffle 阶段 4 个步骤，Reduce 阶段分为 2 个步骤</p>
<p><a href="https://manzhong.github.io/images/MapReduce/lc.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/lc.jpg" alt="img"></a></p>
<h5 id="Map-阶段-2-个步骤"><a href="#Map-阶段-2-个步骤" class="headerlink" title="Map 阶段 2 个步骤"></a>Map 阶段 2 个步骤</h5><ol>
<li>设置 InputFormat 类, 将数据切分为 Key-Value<strong>(K1和V1)</strong> 对, 输入到第二步 <strong>k1为源数据的每行的偏移量,v1为行数据</strong></li>
<li>自定义 Map 逻辑, 将第一步的结果转换成另外的 Key-Value（<strong>K2和V2</strong>） 对, 输出结果</li>
</ol>
<h5 id="Shuffle-阶段-4-个步骤"><a href="#Shuffle-阶段-4-个步骤" class="headerlink" title="Shuffle 阶段 4 个步骤"></a>Shuffle 阶段 4 个步骤</h5><ol>
<li><p>对输出的 Key-Value 对进行<strong>分区</strong> 自定义类继承Partitioner类 方法 getPartition 类的参数与k2v2同</p>
<p>reduce的默认分区只有一个,k2的哈希值与上int的最大值,模上ReduceTask的个数 指定分区就是相同类型的,共性的数据发送到同一个reduce中处理.</p>
</li>
<li><p>对不同分区的数据按照相同的 Key <strong>排序</strong> 自定义类实现WritableComparable 成员变量为不要排序的列</p>
</li>
<li><p>(可选) 对分组过的数据初步<strong>规约</strong>, 降低数据的网络拷贝 每个map都会产生大量本地输出 作用: 对map端的输出先做一次合并,减少在map和reduce节点间的数据传输量,提高网络io性能</p>
<p>应用前提: 不能影响最终业务逻辑</p>
<p>自定义类 继承 Reduce类</p>
</li>
<li><p>对数据进行<strong>分组</strong>, 相同 Key 的 Value 放入一个集合中</p>
</li>
</ol>
<h5 id="Reduce-阶段-2-个步骤"><a href="#Reduce-阶段-2-个步骤" class="headerlink" title="Reduce 阶段 2 个步骤"></a>Reduce 阶段 2 个步骤</h5><ol>
<li>对多个 Map 任务的结果进行排序以及合并, 编写 Reduce 函数实现自己的逻辑, 对输入的 Key-Value 进行处理, 转为新的 Key-Value（<strong>K3和V3</strong>）输出</li>
<li>设置 OutputFormat 处理并保存 Reduce 输出的 Key-Value 数据</li>
</ol>
<h2 id="3-WordCount"><a href="#3-WordCount" class="headerlink" title="3. WordCount"></a>3. WordCount</h2><blockquote>
<p>需求: 在一堆给定的文本文件中统计输出每一个单词出现的总次数</p>
</blockquote>
<h5 id="Step-1-数据格式准备"><a href="#Step-1-数据格式准备" class="headerlink" title="Step 1. 数据格式准备"></a>Step 1. 数据格式准备</h5><ol>
<li><p>创建一个新的文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">vim wordcount.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>向其中放入以下内容并保存</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello,world,hadoop</span><br><span class="line">hive,sqoop,flume,hello</span><br><span class="line">kitty,tom,jerry,world</span><br><span class="line">hadoop</span><br></pre></td></tr></table></figure>
</li>
<li><p>上传到 HDFS</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /wordcount/</span><br><span class="line">hdfs dfs -put wordcount.txt /wordcount/</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h5 id="Step-2-Mapper"><a href="#Step-2-Mapper" class="headerlink" title="Step 2. Mapper"></a>Step 2. Mapper</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,LongWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        String line = value.toString();</span><br><span class="line">        String[] split = line.split(&quot;,&quot;);</span><br><span class="line">        for (String word : split) &#123;</span><br><span class="line">            context.write(new Text(word),new LongWritable(1));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-3-Reducer"><a href="#Step-3-Reducer" class="headerlink" title="Step 3. Reducer"></a>Step 3. Reducer</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public class WordCountReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt; &#123;</span><br><span class="line">    /**</span><br><span class="line">     * 自定义我们的reduce逻辑</span><br><span class="line">     * 所有的key都是我们的单词，所有的values都是我们单词出现的次数</span><br><span class="line">     * @param key</span><br><span class="line">     * @param values</span><br><span class="line">     * @param context</span><br><span class="line">     * @throws IOException</span><br><span class="line">     * @throws InterruptedException</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        long count = 0;</span><br><span class="line">        for (LongWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,new LongWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-4-定义主类-描述-Job-并提交-Job"><a href="#Step-4-定义主类-描述-Job-并提交-Job" class="headerlink" title="Step 4. 定义主类, 描述 Job 并提交 Job"></a>Step 4. 定义主类, 描述 Job 并提交 Job</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), JobMain.class.getSimpleName());</span><br><span class="line">        //打包到集群上面运行时候，必须要添加以下配置，指定程序的main函数</span><br><span class="line">        job.setJarByClass(JobMain.class);</span><br><span class="line">        //第一步：读取输入文件解析成key，value对</span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/wordcount&quot;));</span><br><span class="line"></span><br><span class="line">        //第二步：设置我们的mapper类</span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        //设置我们map阶段完成之后的输出类型</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(LongWritable.class);</span><br><span class="line">        //第三步，第四步，第五步，第六步，省略</span><br><span class="line">        //第七步：设置我们的reduce类</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        //设置我们reduce阶段完成之后的输出类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(LongWritable.class);</span><br><span class="line">        //第八步：设置输出类以及输出路径</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/wordcount_out&quot;));</span><br><span class="line">        boolean b = job.waitForCompletion(true);</span><br><span class="line">        return b?0:1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 程序main函数的入口类</span><br><span class="line">     * @param args</span><br><span class="line">     * @throws Exception</span><br><span class="line">     */</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line">        Tool tool  =  new JobMain();</span><br><span class="line">        int run = ToolRunner.run(configuration, tool, args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h5><p>如果遇到如下错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=admin, access=WRITE, inode=&quot;/&quot;:root:supergroup:drwxr-xr-x</span><br></pre></td></tr></table></figure>

<p>直接将hdfs-site.xml当中的权限关闭即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>最后重启一下 HDFS 集群</p>
<h5 id="小细节"><a href="#小细节" class="headerlink" title="小细节"></a>小细节</h5><p>本地运行完成之后，就可以打成jar包放到服务器上面去运行了，实际工作当中，都是将代码打成jar包，开发main方法作为程序的入口，然后放到集群上面去运行</p>
<h2 id="4-MapReduce-运行模式"><a href="#4-MapReduce-运行模式" class="headerlink" title="4. MapReduce 运行模式"></a>4. MapReduce 运行模式</h2><h5 id="本地运行模式"><a href="#本地运行模式" class="headerlink" title="本地运行模式"></a>本地运行模式</h5><ol>
<li>MapReduce 程序是被提交给 LocalJobRunner 在本地以单进程的形式运行</li>
<li>处理的数据及输出结果可以在本地文件系统, 也可以在hdfs上</li>
<li>怎样实现本地运行? 写一个程序, 不要带集群的配置文件, 本质是程序的 <code>conf</code> 中是否有 <code>mapreduce.framework.name=local</code> 以及 <code>yarn.resourcemanager.hostname=local</code> 参数</li>
<li>本地模式非常便于进行业务逻辑的 <code>Debug</code>, 只要在 idea 中打断点即可</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">configuration.set(&quot;mapreduce.framework.name&quot;,&quot;local&quot;);</span><br><span class="line">configuration.set(&quot; yarn.resourcemanager.hostname&quot;,&quot;local&quot;);</span><br><span class="line">TextInputFormat.addInputPath(job,new Path(&quot;file:///F:\\wordcount\\input&quot;));</span><br><span class="line">TextOutputFormat.setOutputPath(job,new Path(&quot;file:///F:\\wordcount\\output&quot;));</span><br></pre></td></tr></table></figure>

<h5 id="集群运行模式"><a href="#集群运行模式" class="headerlink" title="集群运行模式"></a>集群运行模式</h5><ol>
<li>将 MapReduce 程序提交给 Yarn 集群, 分发到很多的节点上并发执行</li>
<li>处理的数据和输出结果应该位于 HDFS 文件系统</li>
<li>提交集群的实现步骤: 将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar cn.itcast.hdfs.demo1.JobMain</span><br></pre></td></tr></table></figure>

<h2 id="5-MapReduce-分区"><a href="#5-MapReduce-分区" class="headerlink" title="5. MapReduce 分区"></a>5. MapReduce 分区</h2><p>在 MapReduce 中, 通过我们指定分区, 会将同一个分区的数据发送到同一个 Reduce 当中进行处理</p>
<p>例如: 为了数据的统计, 可以把一批类似的数据发送到同一个 Reduce 当中, 在同一个 Reduce 当中统计相同类型的数据, 就可以实现类似的数据分区和统计等</p>
<p>其实就是相同类型的数据, 有共性的数据, 送到一起去处理</p>
<p>Reduce 当中默认的分区只有一个</p>
<blockquote>
<p>Step 1. 定义 Mapper</p>
</blockquote>
<p>这个 Mapper 程序不做任何逻辑, 也不对 Key-Value 做任何改变, 只是接收数据, 然后往下发送</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public class MyMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        context.write(value,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-2-定义-Reducer-逻辑"><a href="#Step-2-定义-Reducer-逻辑" class="headerlink" title="Step 2. 定义 Reducer 逻辑"></a>Step 2. 定义 Reducer 逻辑</h5><p>这个 Reducer 也不做任何处理, 将数据原封不动的输出即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public class MyReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        context.write(key,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-3-自定义-Partitioner"><a href="#Step-3-自定义-Partitioner" class="headerlink" title="Step 3. 自定义 Partitioner"></a>Step 3. 自定义 Partitioner</h5><p>主要的逻辑就在这里, 这也是这个案例的意义, 通过 Partitioner 将数据分发给不同的 Reducer</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class PartitonerOwn extends Partitioner&lt;Text,LongWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int getPartition(Text text, LongWritable longWritable, int i) &#123;</span><br><span class="line">        if(text.toString().length() &gt;=5 )&#123;</span><br><span class="line">            return  0;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            return 1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-4-Main-入口"><a href="#Step-4-Main-入口" class="headerlink" title="Step 4. Main 入口"></a>Step 4. Main 入口</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">public class PartitionMain  extends Configured implements Tool &#123;</span><br><span class="line">    public static void main(String[] args) throws  Exception&#123;</span><br><span class="line">        int run = ToolRunner.run(new Configuration(), new PartitionMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), PartitionMain.class.getSimpleName());</span><br><span class="line">        job.setJarByClass(PartitionMain.class);</span><br><span class="line">        //第一步 </span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/partitioner&quot;));</span><br><span class="line">        TextOutputFormat.setOutputPath(job,new Path(&quot;hdfs://192.168.52.250:8020/outpartition&quot;));</span><br><span class="line">        job.setMapperClass(MyMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setReducerClass(MyReducer.class);</span><br><span class="line">        /**</span><br><span class="line">         * 设置我们的分区类，以及我们的reducetask的个数，注意reduceTask的个数一定要与我们的</span><br><span class="line">         * 分区数保持一致</span><br><span class="line">         */</span><br><span class="line">        job.setPartitionerClass(MyPartitioner.class);</span><br><span class="line">        job.setNumReduceTasks(2);</span><br><span class="line">        boolean b = job.waitForCompletion(true);</span><br><span class="line">        return b?0:1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="6-MapReduce-排序和序列化"><a href="#6-MapReduce-排序和序列化" class="headerlink" title="6. MapReduce 排序和序列化"></a>6. MapReduce 排序和序列化</h2><ul>
<li>序列化 (Serialization) 是指把结构化对象转化为字节流</li>
<li>反序列化 (Deserialization) 是序列化的逆过程. 把字节流转为结构化对象. 当要在进程间传递对象或持久化对象的时候, 就需要序列化对象成字节流, 反之当要将接收到或从磁盘读取的字节流转换为对象, 就要进行反序列化</li>
<li>Java 的序列化 (Serializable) 是一个重量级序列化框架, 一个对象被序列化后, 会附带很多额外的信息 (各种校验信息, header, 继承体系等）, 不便于在网络中高效传输. 所以, Hadoop 自己开发了一套序列化机制(Writable), 精简高效. 不用像 Java 对象类一样传输多层的父子关系, 需要哪个属性就传输哪个属性值, 大大的减少网络传输的开销</li>
<li>Writable 是 Hadoop 的序列化格式, Hadoop 定义了这样一个 Writable 接口. 一个类要支持可序列化只需实现这个接口即可</li>
<li>另外 Writable 有一个子接口是 WritableComparable, WritableComparable 是既可实现序列化, 也可以对key进行比较, 我们这里可以通过自定义 Key 实现 WritableComparable 来实现我们的排序功能</li>
</ul>
<p>数据格式如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a	1</span><br><span class="line">a	9</span><br><span class="line">b	3</span><br><span class="line">a	7</span><br><span class="line">b	8</span><br><span class="line">b	10</span><br><span class="line">a	5</span><br></pre></td></tr></table></figure>

<p>要求:</p>
<ul>
<li>第一列按照字典顺序进行排列</li>
<li>第一列相同的时候, 第二列按照升序进行排列</li>
</ul>
<p>解决思路:</p>
<ul>
<li>将 Map 端输出的 <code>&lt;key,value&gt;</code> 中的 key 和 value 组合成一个新的 key (newKey), value值不变</li>
<li>这里就变成 <code>&lt;(key,value),value&gt;</code>, 在针对 newKey 排序的时候, 如果 key 相同, 就再对value进行排序</li>
</ul>
<h5 id="Step-1-自定义类型和比较器"><a href="#Step-1-自定义类型和比较器" class="headerlink" title="Step 1. 自定义类型和比较器"></a>Step 1. 自定义类型和比较器</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">public class PairWritable implements WritableComparable&lt;PairWritable&gt; &#123;</span><br><span class="line">    // 组合key,第一部分是我们第一列，第二部分是我们第二列</span><br><span class="line">    private String first;</span><br><span class="line">    private int second;</span><br><span class="line">    public PairWritable() &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    public PairWritable(String first, int second) &#123;</span><br><span class="line">        this.set(first, second);</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 方便设置字段</span><br><span class="line">     */</span><br><span class="line">    public void set(String first, int second) &#123;</span><br><span class="line">        this.first = first;</span><br><span class="line">        this.second = second;</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 反序列化</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public void readFields(DataInput input) throws IOException &#123;</span><br><span class="line">        this.first = input.readUTF();</span><br><span class="line">        this.second = input.readInt();</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 序列化</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public void write(DataOutput output) throws IOException &#123;</span><br><span class="line">        output.writeUTF(first);</span><br><span class="line">        output.writeInt(second);</span><br><span class="line">    &#125;</span><br><span class="line">    /*</span><br><span class="line">     * 重写比较器</span><br><span class="line">     */</span><br><span class="line">    public int compareTo(PairWritable o) &#123;</span><br><span class="line">        //每次比较都是调用该方法的对象与传递的参数进行比较，说白了就是第一行与第二行比较完了之后的结果与第三行比较，</span><br><span class="line">        //得出来的结果再去与第四行比较，依次类推</span><br><span class="line">        System.out.println(o.toString());</span><br><span class="line">        System.out.println(this.toString());</span><br><span class="line">        int comp = this.first.compareTo(o.first);</span><br><span class="line">        if (comp != 0) &#123;</span><br><span class="line">            return comp;</span><br><span class="line">        &#125; else &#123; // 若第一个字段相等，则比较第二个字段</span><br><span class="line">            return Integer.valueOf(this.second).compareTo(</span><br><span class="line">                    Integer.valueOf(o.getSecond()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int getSecond() &#123;</span><br><span class="line">        return second;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setSecond(int second) &#123;</span><br><span class="line">        this.second = second;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getFirst() &#123;</span><br><span class="line">        return first;</span><br><span class="line">    &#125;</span><br><span class="line">    public void setFirst(String first) &#123;</span><br><span class="line">        this.first = first;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;PairWritable&#123;&quot; +</span><br><span class="line">                &quot;first=&apos;&quot; + first + &apos;\&apos;&apos; +</span><br><span class="line">                &quot;, second=&quot; + second +</span><br><span class="line">                &apos;&#125;&apos;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-2-Mapper-1"><a href="#Step-2-Mapper-1" class="headerlink" title="Step 2. Mapper"></a>Step 2. Mapper</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public class SortMapper extends Mapper&lt;LongWritable,Text,PairWritable,IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private PairWritable mapOutKey = new PairWritable();</span><br><span class="line">    private IntWritable mapOutValue = new IntWritable();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public  void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        String lineValue = value.toString();</span><br><span class="line">        String[] strs = lineValue.split(&quot;\t&quot;);</span><br><span class="line">        //设置组合key和value ==&gt; &lt;(key,value),value&gt;</span><br><span class="line">        mapOutKey.set(strs[0], Integer.valueOf(strs[1]));</span><br><span class="line">        mapOutValue.set(Integer.valueOf(strs[1]));</span><br><span class="line">        context.write(mapOutKey, mapOutValue);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-3-Reducer-1"><a href="#Step-3-Reducer-1" class="headerlink" title="Step 3. Reducer"></a>Step 3. Reducer</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public class SortReducer extends Reducer&lt;PairWritable,IntWritable,Text,IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private Text outPutKey = new Text();</span><br><span class="line">    @Override</span><br><span class="line">    public void reduce(PairWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">//迭代输出</span><br><span class="line">        for(IntWritable value : values) &#123;</span><br><span class="line">            outPutKey.set(key.getFirst());</span><br><span class="line">            context.write(outPutKey, value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-4-Main-入口-1"><a href="#Step-4-Main-入口-1" class="headerlink" title="Step 4. Main 入口"></a>Step 4. Main 入口</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //1:创建job对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;mapreduce_sort&quot;);</span><br><span class="line"></span><br><span class="line">        //2:配置job任务(八个步骤)</span><br><span class="line">            //第一步:设置输入类和输入的路径</span><br><span class="line">            job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">            ///TextInputFormat.addInputPath(job, new Path(&quot;hdfs://node01:8020/input/sort_input&quot;));</span><br><span class="line">            TextInputFormat.addInputPath(job, new Path(&quot;file:///D:\\input\\sort_input&quot;));</span><br><span class="line"></span><br><span class="line">            //第二步: 设置Mapper类和数据类型</span><br><span class="line">            job.setMapperClass(SortMapper.class);</span><br><span class="line">            job.setMapOutputKeyClass(SortBean.class);</span><br><span class="line">            job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">            //第三，四，五，六</span><br><span class="line"></span><br><span class="line">            //第七步：设置Reducer类和类型</span><br><span class="line">            job.setReducerClass(SortReducer.class);</span><br><span class="line">            job.setOutputKeyClass(SortBean.class);</span><br><span class="line">            job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            //第八步: 设置输出类和输出的路径</span><br><span class="line">            job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">            //TextOutputFormat.setOutputPath(job, new Path(&quot;hdfs://node01:8020/out/sort_out&quot;));</span><br><span class="line">            TextOutputFormat.setOutputPath(job, new Path(&quot;file:///D:\\out\\sort_out&quot;));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //3:等待任务结束</span><br><span class="line">        boolean bl = job.waitForCompletion(true);</span><br><span class="line"></span><br><span class="line">        return bl?0:1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        //启动job任务</span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line"></span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="MapReduce-中的计数器"><a href="#MapReduce-中的计数器" class="headerlink" title="MapReduce 中的计数器"></a>MapReduce 中的计数器</h2><p>计数器是收集作业统计信息的有效手段之一，用于质量控制或应用级统计。计数器还可辅助诊断系统故障。如果需要将日志信息传输到 map 或 reduce 任务， 更好的方法通常是看能否用一个计数器值来记录某一特定事件的发生。对于大型分布式作业而言，使用计数器更为方便。除了因为获取计数器值比输出日志更方便，还有根据计数器值统计特定事件的发生次数要比分析一堆日志文件容易得多。</p>
<p>hadoop内置计数器列表</p>
<table>
<thead>
<tr>
<th align="left"><strong>MapReduce任务计数器</strong></th>
<th align="left"><strong>org.apache.hadoop.mapreduce.TaskCounter</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">文件系统计数器</td>
<td align="left">org.apache.hadoop.mapreduce.FileSystemCounter</td>
</tr>
<tr>
<td align="left">FileInputFormat计数器</td>
<td align="left">org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter</td>
</tr>
<tr>
<td align="left">FileOutputFormat计数器</td>
<td align="left">org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter</td>
</tr>
<tr>
<td align="left">作业计数器</td>
<td align="left">org.apache.hadoop.mapreduce.JobCounter</td>
</tr>
</tbody></table>
<p><strong>每次mapreduce执行完成之后，我们都会看到一些日志记录出来，其中最重要的一些日志记录如下截图</strong></p>
<p><strong>所有的这些都是MapReduce的计数器的功能，既然MapReduce当中有计数器的功能，我们如何实现自己的计数器？？？</strong></p>
<blockquote>
<p><strong>需求：以以上分区代码为案例，统计map接收到的数据记录条数</strong></p>
</blockquote>
<h5 id="第一种方式"><a href="#第一种方式" class="headerlink" title="第一种方式"></a>第一种方式</h5><p><strong>第一种方式定义计数器，通过context上下文对象可以获取我们的计数器，进行记录</strong><br><strong>通过context上下文对象，在map端使用计数器进行统计</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public class PartitionMapper  extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;&#123;</span><br><span class="line">    //map方法将K1和V1转为K2和V2</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws Exception&#123;</span><br><span class="line">        Counter counter = context.getCounter(&quot;MR_COUNT&quot;, &quot;MyRecordCounter&quot;);</span><br><span class="line">        counter.increment(1L);</span><br><span class="line">        context.write(value,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>运行程序之后就可以看到我们自定义的计数器在map阶段读取了七条数据</strong></p>
<h5 id="第二种方式"><a href="#第二种方式" class="headerlink" title="第二种方式"></a><strong>第二种方式</strong></h5><p><strong>通过enum枚举类型来定义计数器</strong><br>统计reduce端数据的输入的key有多少个</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class PartitionerReducer extends Reducer&lt;Text,NullWritable,Text,NullWritable&gt; &#123;</span><br><span class="line">   public static enum Counter&#123;</span><br><span class="line">       MY_REDUCE_INPUT_RECORDS,MY_REDUCE_INPUT_BYTES</span><br><span class="line">   &#125;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">       context.getCounter(Counter.MY_REDUCE_INPUT_RECORDS).increment(1L);</span><br><span class="line">       context.write(key, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="规约Combiner"><a href="#规约Combiner" class="headerlink" title="规约Combiner"></a>规约Combiner</h2><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>每一个 map 都可能会产生大量的本地输出，Combiner 的作用就是对 map 端的输出先做一次合并，以减少在 map 和 reduce 节点之间的数据传输量，以提高网络IO 性能，是 MapReduce 的一种优化手段之一</p>
<ul>
<li>combiner 是 MR 程序中 Mapper 和 Reducer 之外的一种组件</li>
<li>combiner 组件的父类就是 Reducer</li>
<li>combiner 和 reducer 的区别在于运行的位置<ul>
<li>Combiner 是在每一个 maptask 所在的节点运行</li>
<li>Reducer 是接收全局所有 Mapper 的输出结果</li>
</ul>
</li>
<li>combiner 的意义就是对每一个 maptask 的输出进行局部汇总，以减小网络传输量</li>
</ul>
<h5 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h5><ol>
<li>自定义一个 combiner 继承 Reducer，重写 reduce 方法</li>
<li>在 job 中设置 <code>job.setCombinerClass(CustomCombiner.class)</code></li>
</ol>
<p>combiner 能够应用的前提是不能影响最终的业务逻辑，而且，combiner 的输出 kv 应该跟 reducer 的输入 kv 类型要对应起来</p>
<h2 id="MapReduce案例-流量统计"><a href="#MapReduce案例-流量统计" class="headerlink" title="MapReduce案例-流量统计"></a>MapReduce案例-流量统计</h2><h3 id="需求一-统计求和"><a href="#需求一-统计求和" class="headerlink" title="需求一: 统计求和"></a>需求一: 统计求和</h3><p>统计每个手机号的上行数据包总和，下行数据包总和，上行总流量之和，下行总流量之和<br>分析：以手机号码作为key值，上行流量，下行流量，上行总流量，下行总流量四个字段作为value值，然后以这个key，和value作为map阶段的输出，reduce阶段的输入</p>
<h5 id="Step-1-自定义map的输出value对象FlowBean"><a href="#Step-1-自定义map的输出value对象FlowBean" class="headerlink" title="Step 1: 自定义map的输出value对象FlowBean"></a>Step 1: 自定义map的输出value对象FlowBean</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">public class FlowBean implements Writable &#123;</span><br><span class="line">    private Integer upFlow;</span><br><span class="line">    private Integer  downFlow;</span><br><span class="line">    private Integer upCountFlow;</span><br><span class="line">    private Integer downCountFlow;</span><br><span class="line">    @Override</span><br><span class="line">    public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">        out.writeInt(upFlow);</span><br><span class="line">        out.writeInt(downFlow);</span><br><span class="line">        out.writeInt(upCountFlow);</span><br><span class="line">        out.writeInt(downCountFlow);</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">        this.upFlow = in.readInt();</span><br><span class="line">        this.downFlow = in.readInt();</span><br><span class="line">        this.upCountFlow = in.readInt();</span><br><span class="line">        this.downCountFlow = in.readInt();</span><br><span class="line">    &#125;</span><br><span class="line">    public FlowBean() &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    public FlowBean(Integer upFlow, Integer downFlow, Integer upCountFlow, Integer downCountFlow) &#123;</span><br><span class="line">        this.upFlow = upFlow;</span><br><span class="line">        this.downFlow = downFlow;</span><br><span class="line">        this.upCountFlow = upCountFlow;</span><br><span class="line">        this.downCountFlow = downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public Integer getUpFlow() &#123;</span><br><span class="line">        return upFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public void setUpFlow(Integer upFlow) &#123;</span><br><span class="line">        this.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public Integer getDownFlow() &#123;</span><br><span class="line">        return downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public void setDownFlow(Integer downFlow) &#123;</span><br><span class="line">        this.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public Integer getUpCountFlow() &#123;</span><br><span class="line">        return upCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public void setUpCountFlow(Integer upCountFlow) &#123;</span><br><span class="line">        this.upCountFlow = upCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public Integer getDownCountFlow() &#123;</span><br><span class="line">        return downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public void setDownCountFlow(Integer downCountFlow) &#123;</span><br><span class="line">        this.downCountFlow = downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;FlowBean&#123;&quot; +</span><br><span class="line">                &quot;upFlow=&quot; + upFlow +</span><br><span class="line">                &quot;, downFlow=&quot; + downFlow +</span><br><span class="line">                &quot;, upCountFlow=&quot; + upCountFlow +</span><br><span class="line">                &quot;, downCountFlow=&quot; + downCountFlow +</span><br><span class="line">                &apos;&#125;&apos;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-2-定义FlowMapper类"><a href="#Step-2-定义FlowMapper类" class="headerlink" title="Step 2: 定义FlowMapper类"></a>Step 2: 定义FlowMapper类</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public class FlowCountMapper extends Mapper&lt;LongWritable,Text,Text,FlowBean&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">       //1:拆分手机号</span><br><span class="line">        String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line">        String phoneNum = split[1];</span><br><span class="line">        //2:获取四个流量字段</span><br><span class="line">        FlowBean flowBean = new FlowBean();</span><br><span class="line">        flowBean.setUpFlow(Integer.parseInt(split[6]));</span><br><span class="line">        flowBean.setDownFlow(Integer.parseInt(split[7]));</span><br><span class="line">        flowBean.setUpCountFlow(Integer.parseInt(split[8]));</span><br><span class="line">        flowBean.setDownCountFlow(Integer.parseInt(split[9]));</span><br><span class="line"></span><br><span class="line">        //3:将k2和v2写入上下文中</span><br><span class="line">        context.write(new Text(phoneNum), flowBean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-3-定义FlowReducer类"><a href="#Step-3-定义FlowReducer类" class="headerlink" title="Step 3: 定义FlowReducer类"></a>Step 3: 定义FlowReducer类</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public class FlowCountReducer extends Reducer&lt;Text,FlowBean,Text,FlowBean&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">       //封装新的FlowBean</span><br><span class="line">        FlowBean flowBean = new FlowBean();</span><br><span class="line">        Integer upFlow = 0;</span><br><span class="line">        Integer  downFlow = 0;</span><br><span class="line">        Integer upCountFlow = 0;</span><br><span class="line">        Integer downCountFlow = 0;</span><br><span class="line">        for (FlowBean value : values) &#123;</span><br><span class="line">            upFlow  += value.getUpFlow();</span><br><span class="line">            downFlow += value.getDownFlow();</span><br><span class="line">            upCountFlow += value.getUpCountFlow();</span><br><span class="line">            downCountFlow += value.getDownCountFlow();</span><br><span class="line">        &#125;</span><br><span class="line">        flowBean.setUpFlow(upFlow);</span><br><span class="line">        flowBean.setDownFlow(downFlow);</span><br><span class="line">        flowBean.setUpCountFlow(upCountFlow);</span><br><span class="line">        flowBean.setDownCountFlow(downCountFlow);</span><br><span class="line">        //将K3和V3写入上下文中</span><br><span class="line">        context.write(key, flowBean);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-4-程序main函数入口FlowMain"><a href="#Step-4-程序main函数入口FlowMain" class="headerlink" title="Step 4: 程序main函数入口FlowMain"></a>Step 4: 程序main函数入口FlowMain</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured implements Tool &#123;</span><br><span class="line"></span><br><span class="line">    //该方法用于指定一个job任务</span><br><span class="line">    @Override</span><br><span class="line">        public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //1:创建一个job任务对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;mapreduce_flowcount&quot;);</span><br><span class="line">        //如果打包运行出错，则需要加该配置</span><br><span class="line">        job.setJarByClass(JobMain.class);</span><br><span class="line">        //2:配置job任务对象(八个步骤)</span><br><span class="line"></span><br><span class="line">        //第一步:指定文件的读取方式和读取路径</span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        //TextInputFormat.addInputPath(job, new Path(&quot;hdfs://node01:8020/wordcount&quot;));</span><br><span class="line">        TextInputFormat.addInputPath(job, new Path(&quot;file:///D:\\input\\flowcount_input&quot;));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //第二步:指定Map阶段的处理方式和数据类型</span><br><span class="line">         job.setMapperClass(FlowCountMapper.class);</span><br><span class="line">         //设置Map阶段K2的类型</span><br><span class="line">          job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        //设置Map阶段V2的类型</span><br><span class="line">          job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          //第三（分区），四 （排序）</span><br><span class="line">          //第五步: 规约(Combiner)</span><br><span class="line">          //第六步 分组</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">          //第七步：指定Reduce阶段的处理方式和数据类型</span><br><span class="line">          job.setReducerClass(FlowCountReducer.class);</span><br><span class="line">          //设置K3的类型</span><br><span class="line">           job.setOutputKeyClass(Text.class);</span><br><span class="line">          //设置V3的类型</span><br><span class="line">           job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">           //第八步: 设置输出类型</span><br><span class="line">           job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">           //设置输出的路径</span><br><span class="line">           TextOutputFormat.setOutputPath(job, new Path(&quot;file:///D:\\out\\flowcount_out&quot;));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //等待任务结束</span><br><span class="line">           boolean bl = job.waitForCompletion(true);</span><br><span class="line"></span><br><span class="line">           return bl ? 0:1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        //启动job任务</span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="需求二-上行流量倒序排序（递减排序）"><a href="#需求二-上行流量倒序排序（递减排序）" class="headerlink" title="需求二: 上行流量倒序排序（递减排序）"></a>需求二: 上行流量倒序排序（递减排序）</h3><p>分析，以需求一的输出数据作为排序的输入数据，自定义FlowBean,以FlowBean为map输出的key，以手机号作为Map输出的value，因为MapReduce程序会对Map阶段输出的key进行排序</p>
<h5 id="Step-1-定义FlowBean实现WritableComparable实现比较排序"><a href="#Step-1-定义FlowBean实现WritableComparable实现比较排序" class="headerlink" title="Step 1: 定义FlowBean实现WritableComparable实现比较排序"></a>Step 1: 定义FlowBean实现WritableComparable实现比较排序</h5><p>Java 的 compareTo 方法说明:</p>
<ul>
<li>compareTo 方法用于将当前对象与方法的参数进行比较。</li>
<li>如果指定的数与参数相等返回 0。</li>
<li>如果指定的数小于参数返回 -1。</li>
<li>如果指定的数大于参数返回 1。</li>
</ul>
<p>例如：<code>o1.compareTo(o2);</code> 返回正数的话，当前对象（调用 compareTo 方法的对象 o1）要排在比较对象（compareTo 传参对象 o2）后面，返回负数的话，放在前面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123;</span><br><span class="line">    private Integer upFlow;</span><br><span class="line">    private Integer  downFlow;</span><br><span class="line">    private Integer upCountFlow;</span><br><span class="line">    private Integer downCountFlow;</span><br><span class="line">    public FlowBean() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public FlowBean(Integer upFlow, Integer downFlow, Integer upCountFlow, Integer downCountFlow) &#123;</span><br><span class="line">        this.upFlow = upFlow;</span><br><span class="line">        this.downFlow = downFlow;</span><br><span class="line">        this.upCountFlow = upCountFlow;</span><br><span class="line">        this.downCountFlow = downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">        out.writeInt(upFlow);</span><br><span class="line">        out.writeInt(downFlow);</span><br><span class="line">        out.writeInt(upCountFlow);</span><br><span class="line">        out.writeInt(downCountFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">        upFlow = in.readInt();</span><br><span class="line">        downFlow = in.readInt();</span><br><span class="line">        upCountFlow = in.readInt();</span><br><span class="line">        downCountFlow = in.readInt();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Integer getUpFlow() &#123;</span><br><span class="line">        return upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setUpFlow(Integer upFlow) &#123;</span><br><span class="line">        this.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public Integer getDownFlow() &#123;</span><br><span class="line">        return downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public void setDownFlow(Integer downFlow) &#123;</span><br><span class="line">        this.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public Integer getUpCountFlow() &#123;</span><br><span class="line">        return upCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public void setUpCountFlow(Integer upCountFlow) &#123;</span><br><span class="line">        this.upCountFlow = upCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public Integer getDownCountFlow() &#123;</span><br><span class="line">        return downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    public void setDownCountFlow(Integer downCountFlow) &#123;</span><br><span class="line">        this.downCountFlow = downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return upFlow+&quot;\t&quot;+downFlow+&quot;\t&quot;+upCountFlow+&quot;\t&quot;+downCountFlow;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public int compareTo(FlowBean o) &#123;</span><br><span class="line">        return this.upCountFlow &gt; o.upCountFlow ?-1:1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-2-定义FlowMapper"><a href="#Step-2-定义FlowMapper" class="headerlink" title="Step 2: 定义FlowMapper"></a>Step 2: 定义FlowMapper</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public class FlowCountSortMapper extends Mapper&lt;LongWritable,Text,FlowBean,Text&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        FlowBean flowBean = new FlowBean();</span><br><span class="line">        String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line"></span><br><span class="line">        //获取手机号，作为V2</span><br><span class="line">        String phoneNum = split[0];</span><br><span class="line">        //获取其他流量字段,封装flowBean，作为K2</span><br><span class="line">        flowBean.setUpFlow(Integer.parseInt(split[1]));</span><br><span class="line">        flowBean.setDownFlow(Integer.parseInt(split[2]));</span><br><span class="line">        flowBean.setUpCountFlow(Integer.parseInt(split[3]));</span><br><span class="line">        flowBean.setDownCountFlow(Integer.parseInt(split[4]));</span><br><span class="line"></span><br><span class="line">        //将K2和V2写入上下文中</span><br><span class="line">        context.write(flowBean, new Text(phoneNum));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-3-定义FlowReducer"><a href="#Step-3-定义FlowReducer" class="headerlink" title="Step 3: 定义FlowReducer"></a>Step 3: 定义FlowReducer</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public class FlowCountSortReducer extends Reducer&lt;FlowBean,Text,Text,FlowBean&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        for (Text value : values) &#123;</span><br><span class="line">            context.write(value, key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-4-程序main函数入口"><a href="#Step-4-程序main函数入口" class="headerlink" title="Step 4: 程序main函数入口"></a>Step 4: 程序main函数入口</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured  implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] strings) throws Exception &#123;</span><br><span class="line">        //创建一个任务对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;mapreduce_flowcountsort&quot;);</span><br><span class="line"></span><br><span class="line">        //打包放在集群运行时，需要做一个配置</span><br><span class="line">        job.setJarByClass(JobMain.class);</span><br><span class="line">        //第一步:设置读取文件的类: K1 和V1</span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job, new Path(&quot;hdfs://node01:8020/out/flowcount_out&quot;));</span><br><span class="line"></span><br><span class="line">        //第二步：设置Mapper类</span><br><span class="line">        job.setMapperClass(FlowCountSortMapper.class);</span><br><span class="line">        //设置Map阶段的输出类型: k2 和V2的类型</span><br><span class="line">        job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        //第三,四，五，六步采用默认方式(分区，排序，规约，分组)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //第七步 ：设置文的Reducer类</span><br><span class="line">        job.setReducerClass(FlowCountSortReducer.class);</span><br><span class="line">        //设置Reduce阶段的输出类型</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        //设置Reduce的个数</span><br><span class="line"></span><br><span class="line">        //第八步:设置输出类</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        //设置输出的路径</span><br><span class="line">        TextOutputFormat.setOutputPath(job, new Path(&quot;hdfs://node01:8020/out/flowcountsort_out&quot;));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        boolean b = job.waitForCompletion(true);</span><br><span class="line">        return b?0:1;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        //启动一个任务</span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="需求三-手机号码分区"><a href="#需求三-手机号码分区" class="headerlink" title="需求三: 手机号码分区"></a>需求三: 手机号码分区</h3><p>在需求一的基础上，继续完善，将不同的手机号分到不同的数据文件的当中去，需要自定义分区来实现，这里我们自定义来模拟分区，将以下数字开头的手机号进行分开</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">135 开头数据到一个分区文件</span><br><span class="line">136 开头数据到一个分区文件</span><br><span class="line">137 开头数据到一个分区文件</span><br><span class="line">其他分区</span><br></pre></td></tr></table></figure>

<h5 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public class FlowPartition extends Partitioner&lt;Text,FlowBean&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int getPartition(Text text, FlowBean flowBean, int i) &#123;</span><br><span class="line">        String line = text.toString();</span><br><span class="line">        if (line.startsWith(&quot;135&quot;))&#123;</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;else if(line.startsWith(&quot;136&quot;))&#123;</span><br><span class="line">            return 1;</span><br><span class="line">        &#125;else if(line.startsWith(&quot;137&quot;))&#123;</span><br><span class="line">            return 2;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            return 3;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="作业运行设置"><a href="#作业运行设置" class="headerlink" title="作业运行设置"></a>作业运行设置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(FlowPartition.class);</span><br><span class="line"> job.setNumReduceTasks(4);</span><br></pre></td></tr></table></figure>

<h5 id="修改输入输出路径-并放入集群运行"><a href="#修改输入输出路径-并放入集群运行" class="headerlink" title="修改输入输出路径, 并放入集群运行"></a>修改输入输出路径, 并放入集群运行</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TextInputFormat.addInputPath(job,new Path(&quot;hdfs://node01:8020/partition_flow/&quot;));</span><br><span class="line">TextOutputFormat.setOutputPath(job,new Path(&quot;hdfs://node01:8020/partition_out&quot;));</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/MapReduce/" data-id="cjz24s2xp0009u8u5o1y7efmk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-MapReduce2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/MapReduce2/" class="article-date">
  <time datetime="2019-08-08T03:21:13.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/MapReduce2/">MapReduce2</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="MapReduce详解"><a href="#MapReduce详解" class="headerlink" title="MapReduce详解"></a>MapReduce详解</h1><h2 id="1-MapReduce的运行机制详解"><a href="#1-MapReduce的运行机制详解" class="headerlink" title="1 .MapReduce的运行机制详解"></a>1 .MapReduce的运行机制详解</h2><p> 全流程:</p>
<p><a href="https://manzhong.github.io/images/MapReduce/1-MapReduce%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6-%E5%85%A8%E6%B5%81%E7%A8%8B.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/1-MapReduce%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6-%E5%85%A8%E6%B5%81%E7%A8%8B.jpg" alt="img"></a></p>
<h3 id="1-1-MapTask-工作机制"><a href="#1-1-MapTask-工作机制" class="headerlink" title="1.1:MapTask 工作机制"></a>1.1:MapTask 工作机制</h3><p>简单概述：inputFile通过split被逻辑切分为多个split文件，通过Record按行读取内容给map（用户自己实现的）进行处理，数据被map处理结束之后交给OutputCollector收集器，对其结果key进行分区（默认使用hash分区），然后写入buffer，每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据</p>
<h5 id="详细步骤"><a href="#详细步骤" class="headerlink" title="详细步骤"></a>详细步骤</h5><ol>
<li><p>读取数据组件 <strong>InputFormat</strong> (默认 TextInputFormat) 会通过 <code>getSplits</code> 方法对输入目录中文件进行逻辑切片规划得到 <code>block</code>, 有多少个 <code>block</code>就对应启动多少个 <code>MapTask</code>.</p>
</li>
<li><p>将输入文件切分为 <code>block</code> 之后, 由 <code>RecordReader</code> 对象 (默认是LineRecordReader) 进行<strong>读取</strong>, 以 <code>\n</code> 作为分隔符, 读取一行数据, 返回 <code>&lt;key，value&gt;</code>. Key 表示每行首字符偏移值, Value 表示这一行文本内容</p>
</li>
<li><p>读取 <code>block</code> 返回 <code>&lt;key,value&gt;</code>, <strong>进入用户自己继承的 Mapper 类中</strong>，执行用户重写的 map 函数, RecordReader 读取一行这里调用一次</p>
</li>
<li><p>Mapper 逻辑结束之后, 将 Mapper 的每条结果通过 <code>context.write</code> 进行collect数据收集. 在 collect 中, 会先对其进行分区处理，默认使用 <strong>HashPartitioner</strong></p>
<ul>
<li><blockquote>
<p>MapReduce 提供 <code>Partitioner</code> 接口, 它的作用就是根据 <code>Key</code> 或 <code>Value</code> 及 <code>Reducer</code> 的数量来决定当前的这对输出数据最终应该交由哪个 <code>Reduce task</code> 处理, 默认对 Key Hash 后再以 Reducer 数量取模. 默认的取模方式只是为了平均 Reducer 的处理能力, 如果用户自己对 Partitioner 有需求, 可以订制并设置到 Job 上</p>
</blockquote>
</li>
</ul>
</li>
<li><p>接下来, 会将数据写入内存, 内存中这片区域叫做环形缓冲区, 缓冲区的作用是批量收集 Mapper 结果, 减少磁盘 IO 的影响. 我们的 <strong>Key/Value 对以及 Partition 的结果都会被写入缓冲区</strong>. 当然, 写入之前，Key 与 Value 值都会被序列化成字节数组</p>
<ul>
<li><blockquote>
<p>环形缓冲区其实是一个数组, 数组中存放着 Key, Value 的序列化数据和 Key, Value 的元数据信息, 包括 Partition, Key 的起始位置, Value 的起始位置以及 Value 的长度. 环形结构是一个抽象概念</p>
</blockquote>
</li>
<li><blockquote>
<p>缓冲区是有大小限制, 默认是 100MB. 当 Mapper 的输出结果很多时, 就可能会撑爆内存, 所以需要在一定条件下将缓冲区中的数据临时写入磁盘, 然后重新利用这块缓冲区. 这个从内存往磁盘写数据的过程被称为 Spill, 中文可译为溢写. 这个溢写是由单独线程来完成, 不影响往缓冲区写 Mapper 结果的线程. 溢写线程启动时不应该阻止 Mapper 的结果输出, 所以整个缓冲区有个溢写的比例 <code>spill.percent</code>. 这个比例默认是 0.8, 也就是当缓冲区的数据已经达到阈值 <code>buffer size * spill percent = 100MB * 0.8 = 80MB</code>, 溢写线程启动, 锁定这 80MB 的内存, 执行溢写过程. Mapper 的输出结果还可以往剩下的 20MB 内存中写, 互不影响</p>
</blockquote>
</li>
</ul>
</li>
<li><p>当溢写线程启动后, 需要<strong>对这 80MB 空间内的 Key 做排序 (Sort)</strong>. 排序是 MapReduce 模型默认的行为, 这里的排序也是对序列化的字节做的排序</p>
<ul>
<li><blockquote>
<p>如果 Job 设置过 Combiner, 那么现在就是使用 Combiner 的时候了. 将有相同 Key 的 Key/Value 对的 Value 加起来, 减少溢写到磁盘的数据量. Combiner 会优化 MapReduce 的中间结果, 所以它在整个模型中会多次使用</p>
</blockquote>
</li>
<li><blockquote>
<p>那哪些场景才能使用 Combiner 呢? 从这里分析, Combiner 的输出是 Reducer 的输入, Combiner 绝不能改变最终的计算结果. Combiner 只应该用于那种 Reduce 的输入 Key/Value 与输出 Key/Value 类型完全一致, 且不影响最终结果的场景. 比如累加, 最大值等. Combiner 的使用一定得慎重, 如果用好, 它对 Job 执行效率有帮助, 反之会影响 Reducer 的最终结果</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>合并溢写文件</strong>, 每次溢写会在磁盘上生成一个临时文件 (写之前判断是否有 Combiner), 如果 Mapper 的输出结果真的很大, 有多次这样的溢写发生, 磁盘上相应的就会有多个临时文件存在. 当整个数据处理结束之后开始对磁盘中的临时文件进行 Merge 合并, 因为最终的文件只有一个, 写入磁盘, 并且为这个文件提供了一个索引文件, 以记录每个reduce对应数据的偏移量</p>
</li>
</ol>
<h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h5><table>
<thead>
<tr>
<th align="left">配置</th>
<th align="left">默认值</th>
<th align="left">解释</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>mapreduce.task.io.sort.mb</code></td>
<td align="left">100</td>
<td align="left">设置环型缓冲区的内存值大小</td>
</tr>
<tr>
<td align="left"><code>mapreduce.map.sort.spill.percent</code></td>
<td align="left">0.8</td>
<td align="left">设置溢写的比例</td>
</tr>
<tr>
<td align="left"><code>mapreduce.cluster.local.dir</code></td>
<td align="left"><code>${hadoop.tmp.dir}/mapred/local</code></td>
<td align="left">溢写数据目录</td>
</tr>
<tr>
<td align="left"><code>mapreduce.task.io.sort.factor</code></td>
<td align="left">10</td>
<td align="left">设置一次合并多少个溢写文件</td>
</tr>
</tbody></table>
<h3 id="1-2-ReduceTask-工作机制"><a href="#1-2-ReduceTask-工作机制" class="headerlink" title="1.2 :ReduceTask 工作机制"></a>1.2 :ReduceTask 工作机制</h3><p>Reduce 大致分为 copy、sort、reduce 三个阶段，重点在前两个阶段。copy 阶段包含一个 eventFetcher 来获取已完成的 map 列表，由 Fetcher 线程去 copy 数据，在此过程中会启动两个 merge 线程，分别为 inMemoryMerger 和 onDiskMerger，分别将内存中的数据 merge 到磁盘和将磁盘中的数据进行 merge。待数据 copy 完成之后，copy 阶段就完成了，开始进行 sort 阶段，sort 阶段主要是执行 finalMerge 操作，纯粹的 sort 阶段，完成之后就是 reduce 阶段，调用用户定义的 reduce 函数进行处理</p>
<h5 id="详细步骤-1"><a href="#详细步骤-1" class="headerlink" title="详细步骤"></a>详细步骤</h5><ol>
<li><strong>Copy阶段</strong>，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求maptask获取属于自己的文件。</li>
<li><strong>Merge阶段</strong>。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活。merge有三种形式：内存到内存；内存到磁盘；磁盘到磁盘。默认情况下第一种形式不启用。当内存中的数据量到达一定阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的文件。</li>
<li><strong>合并排序</strong>。把分散的数据合并成一个大的数据后，还会再对合并后的数据排序。</li>
<li><strong>对排序后的键值对调用reduce方法</strong>，键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对，最后把这些输出的键值对写入到HDFS文件中。</li>
</ol>
<h3 id="1-3-Shuffle-过程"><a href="#1-3-Shuffle-过程" class="headerlink" title="1.3:Shuffle 过程"></a>1.3:Shuffle 过程</h3><p>map 阶段处理的数据如何传递给 reduce 阶段，是 MapReduce 框架中最关键的一个流程，这个流程就叫 shuffle<br>shuffle: 洗牌、发牌 ——（核心机制：数据分区，排序，分组，规约，合并等过程）</p>
<p>shuffle 是 Mapreduce 的核心，它分布在 Mapreduce 的 map 阶段和 reduce 阶段。一般把从 Map 产生输出开始到 Reduce 取得数据作为输入之前的过程称作 shuffle。</p>
<ol>
<li><strong>Collect阶段</strong>：将 MapTask 的结果输出到默认大小为 100M 的环形缓冲区，保存的是 key/value，Partition 分区信息等。</li>
<li><strong>Spill阶段</strong>：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了 combiner，还会将有相同分区号和 key 的数据进行排序。</li>
<li><strong>Merge阶段</strong>：把所有溢出的临时文件进行一次合并操作，以确保一个 MapTask 最终只产生一个中间数据文件。</li>
<li><strong>Copy阶段</strong>：ReduceTask 启动 Fetcher 线程到已经完成 MapTask 的节点上复制一份属于自己的数据，这些数据默认会保存在内存的缓冲区中，当内存的缓冲区达到一定的阀值的时候，就会将数据写到磁盘之上。</li>
<li><strong>Merge阶段</strong>：在 ReduceTask 远程复制数据的同时，会在后台开启两个线程对内存到本地的数据文件进行合并操作。</li>
<li><strong>Sort阶段</strong>：在对数据进行合并的同时，会进行排序操作，由于 MapTask 阶段已经对数据进行了局部的排序，ReduceTask 只需保证 Copy 的数据的最终整体有效性即可。<br>Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快<br>缓冲区的大小可以通过参数调整, 参数：mapreduce.task.io.sort.mb 默认100M</li>
</ol>
<h2 id="2-案例-Reduce-端实现-JOIN"><a href="#2-案例-Reduce-端实现-JOIN" class="headerlink" title="2. 案例: Reduce 端实现 JOIN"></a>2. 案例: Reduce 端实现 JOIN</h2><p><a href="https://manzhong.github.io/images/MapReduce/2-Reduce%E7%AB%AFjoin%E6%93%8D%E4%BD%9C.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/2-Reduce%E7%AB%AFjoin%E6%93%8D%E4%BD%9C.bmp" alt="img"></a></p>
<p><a href="https://manzhong.github.io/images/MapReduce/3-Reduce%E7%AB%AFjoin%E6%93%8D%E4%BD%9C%E9%97%AE%E9%A2%98.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/3-Reduce%E7%AB%AFjoin%E6%93%8D%E4%BD%9C%E9%97%AE%E9%A2%98.bmp" alt="img"></a></p>
<h3 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1. 需求"></a>2.1. 需求</h3><blockquote>
<p>假如数据量巨大，两表的数据是以文件的形式存储在 HDFS 中, 需要用 MapReduce 程序来实现以下 SQL 查询运算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; &gt; select  a.id,a.date,b.name,b.category_id,b.price from t_order a left join t_product b on a.pid = b.id</span><br><span class="line">&gt; &gt;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h5 id="商品表"><a href="#商品表" class="headerlink" title="商品表"></a>商品表</h5><table>
<thead>
<tr>
<th align="left">id</th>
<th align="left">pname</th>
<th align="left">category_id</th>
<th align="left">price</th>
</tr>
</thead>
<tbody><tr>
<td align="left">P0001</td>
<td align="left">小米5</td>
<td align="left">1000</td>
<td align="left">2000</td>
</tr>
<tr>
<td align="left">P0002</td>
<td align="left">锤子T1</td>
<td align="left">1000</td>
<td align="left">3000</td>
</tr>
</tbody></table>
<h5 id="订单数据表"><a href="#订单数据表" class="headerlink" title="订单数据表"></a>订单数据表</h5><table>
<thead>
<tr>
<th align="left">id</th>
<th align="left">date</th>
<th align="left">pid</th>
<th align="left">amount</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1001</td>
<td align="left">20150710</td>
<td align="left">P0001</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left">1002</td>
<td align="left">20150710</td>
<td align="left">P0002</td>
<td align="left">3</td>
</tr>
</tbody></table>
<h3 id="2-2-实现步骤"><a href="#2-2-实现步骤" class="headerlink" title="2.2 实现步骤"></a>2.2 实现步骤</h3><p>通过将关联的条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce task，在reduce中进行数据的串联</p>
<h4 id="Step-1-定义-Mapper"><a href="#Step-1-定义-Mapper" class="headerlink" title="Step 1: 定义 Mapper"></a>Step 1: 定义 Mapper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:判断数据来自哪个文件</span><br><span class="line">        FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">        String fileName = fileSplit.getPath().getName();</span><br><span class="line">        if(fileName.equals(&quot;product.txt&quot;))&#123;</span><br><span class="line">            //数据来自商品表</span><br><span class="line">            //2:将K1和V1转为K2和V2,写入上下文中</span><br><span class="line">            String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line">            String productId = split[0];</span><br><span class="line"></span><br><span class="line">            context.write(new Text(productId), value);</span><br><span class="line"></span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            //数据来自订单表</span><br><span class="line">            //2:将K1和V1转为K2和V2,写入上下文中</span><br><span class="line">            String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line">            String productId = split[2];</span><br><span class="line"></span><br><span class="line">            context.write(new Text(productId), value);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Step-2-定义-Reducer"><a href="#Step-2-定义-Reducer" class="headerlink" title="Step 2: 定义 Reducer"></a>Step 2: 定义 Reducer</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public class ReduceJoinMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:判断数据来自哪个文件</span><br><span class="line"></span><br><span class="line">        FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">        String fileName = fileSplit.getPath().getName();</span><br><span class="line">        if(fileName.equals(&quot;product.txt&quot;))&#123;</span><br><span class="line">            //数据来自商品表</span><br><span class="line">            //2:将K1和V1转为K2和V2,写入上下文中</span><br><span class="line">            String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line">            String productId = split[0];</span><br><span class="line"></span><br><span class="line">            context.write(new Text(productId), value);</span><br><span class="line"></span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            //数据来自订单表</span><br><span class="line">            //2:将K1和V1转为K2和V2,写入上下文中</span><br><span class="line">            String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line">            String productId = split[2];</span><br><span class="line"></span><br><span class="line">            context.write(new Text(productId), value);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Step-3-定义主类"><a href="#Step-3-定义主类" class="headerlink" title="Step 3: 定义主类"></a>Step 3: 定义主类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class ReduceJoinReducer extends Reducer&lt;Text,Text,Text,Text&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">       //1:遍历集合,获取V3 (first +second)</span><br><span class="line">        String first = &quot;&quot;;</span><br><span class="line">        String second = &quot;&quot;;</span><br><span class="line">        for (Text value : values) &#123;</span><br><span class="line">            if(value.toString().startsWith(&quot;p&quot;))&#123;</span><br><span class="line">                first = value.toString();</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                second += value.toString();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        //2:将K3和V3写入上下文中</span><br><span class="line">        context.write(key, new Text(first+&quot;\t&quot;+second));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-案例-Map端实现-JOIN"><a href="#3-案例-Map端实现-JOIN" class="headerlink" title="3. 案例: Map端实现 JOIN"></a>3. 案例: Map端实现 JOIN</h2><p><a href="https://manzhong.github.io/images/MapReduce/4-Map%E7%AB%AFjoin%E6%93%8D%E4%BD%9C.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/4-Map%E7%AB%AFjoin%E6%93%8D%E4%BD%9C.bmp" alt="img"></a></p>
<h4 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h4><p> 适用于关联表中有小表的情形.</p>
<p> 使用分布式缓存,可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果，可以大大提高join操作的并发度，加快处理速度</p>
<h4 id="3-2-实现步骤"><a href="#3-2-实现步骤" class="headerlink" title="3.2 实现步骤"></a>3.2 实现步骤</h4><p>先在mapper类中预先定义好小表，进行join</p>
<p>引入实际场景中的解决方案：一次加载数据库或者用</p>
<h5 id="Step-1：定义Mapper"><a href="#Step-1：定义Mapper" class="headerlink" title="Step 1：定义Mapper"></a>Step 1：定义Mapper</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">public class MapJoinMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt;&#123;</span><br><span class="line">    private HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    //第一件事情:将分布式缓存的小表数据读取到本地Map集合(只需要做一次)</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void setup(Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:获取分布式缓存文件列表</span><br><span class="line">        URI[] cacheFiles =  context.getCacheFiles();</span><br><span class="line"></span><br><span class="line">        //2:获取指定的分布式缓存文件的文件系统(FileSystem)</span><br><span class="line">        FileSystem fileSystem = FileSystem.get(cacheFiles[0], context.getConfiguration());</span><br><span class="line"></span><br><span class="line">        //3:获取文件的输入流</span><br><span class="line">        FSDataInputStream inputStream = fileSystem.open(new Path(cacheFiles[0]));</span><br><span class="line"></span><br><span class="line">        //4:读取文件内容, 并将数据存入Map集合</span><br><span class="line">           //4.1 将字节输入流转为字符缓冲流FSDataInputStream ---&gt;BufferedReader</span><br><span class="line">        BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream));</span><br><span class="line">           //4.2 读取小表文件内容,以行位单位,并将读取的数据存入map集合</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String line = null;</span><br><span class="line">        while((line = bufferedReader.readLine()) != null)&#123;</span><br><span class="line">            String[] split = line.split(&quot;,&quot;);</span><br><span class="line"></span><br><span class="line">            map.put(split[0], line);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //5:关闭流</span><br><span class="line">        bufferedReader.close();</span><br><span class="line">        fileSystem.close();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //第二件事情:对大表的处理业务逻辑,而且要实现大表和小表的join操作</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">            //1:从行文本数据中获取商品的id: p0001 , p0002  得到了K2</span><br><span class="line">            String[] split = value.toString().split(&quot;,&quot;);</span><br><span class="line">            String productId = split[2];  //K2</span><br><span class="line"></span><br><span class="line">            //2:在Map集合中,将商品的id作为键,获取值(商品的行文本数据) ,将value和值拼接,得到V2</span><br><span class="line">            String productLine = map.get(productId);</span><br><span class="line">            String valueLine = productLine+&quot;\t&quot;+value.toString(); //V2</span><br><span class="line">            //3:将K2和V2写入上下文中</span><br><span class="line">            context.write(new Text(productId), new Text(valueLine));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Step-2：定义主类"><a href="#Step-2：定义主类" class="headerlink" title="Step 2：定义主类"></a>Step 2：定义主类</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain  extends Configured implements Tool&#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //1:获取job对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;map_join_job&quot;);</span><br><span class="line"></span><br><span class="line">        //2:设置job对象(将小表放在分布式缓存中)</span><br><span class="line">            //将小表放在分布式缓存中</span><br><span class="line">           // DistributedCache.addCacheFile(new URI(&quot;hdfs://node01:8020/cache_file/product.txt&quot;), super.getConf());</span><br><span class="line">           job.addCacheFile(new URI(&quot;hdfs://node01:8020/cache_file/product.txt&quot;));</span><br><span class="line"></span><br><span class="line">           //第一步:设置输入类和输入的路径</span><br><span class="line">            job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">            TextInputFormat.addInputPath(job, new Path(&quot;file:///D:\\input\\map_join_input&quot;));</span><br><span class="line">            //第二步:设置Mapper类和数据类型</span><br><span class="line">            job.setMapperClass(MapJoinMapper.class);</span><br><span class="line">            job.setMapOutputKeyClass(Text.class);</span><br><span class="line">            job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">            //第八步:设置输出类和输出路径</span><br><span class="line">            job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">            TextOutputFormat.setOutputPath(job, new Path(&quot;file:///D:\\out\\map_join_out&quot;));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //3:等待任务结束</span><br><span class="line">        boolean bl = job.waitForCompletion(true);</span><br><span class="line">        return bl ? 0 :1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line">        //启动job任务</span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="4-案例-求共同好友"><a href="#4-案例-求共同好友" class="headerlink" title="4. 案例:求共同好友"></a>4. 案例:求共同好友</h2><p>分析图:</p>
<p><a href="https://manzhong.github.io/images/MapReduce/5-%E6%B1%82%E5%85%B1%E5%90%8C%E5%A5%BD%E5%8F%8B.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/5-%E6%B1%82%E5%85%B1%E5%90%8C%E5%A5%BD%E5%8F%8B.bmp" alt="img"></a></p>
<h3 id="4-1-需求分析"><a href="#4-1-需求分析" class="headerlink" title="4.1 需求分析"></a>4.1 需求分析</h3><p>以下是qq的好友列表数据，冒号前是一个用户，冒号后是该用户的所有好友（数据中的好友关系是单向的）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">A:B,C,D,F,E,O</span><br><span class="line">B:A,C,E,K</span><br><span class="line">C:A,B,D,E,I </span><br><span class="line">D:A,E,F,L</span><br><span class="line">E:B,C,D,M,L</span><br><span class="line">F:A,B,C,D,E,O,M</span><br><span class="line">G:A,C,D,E,F</span><br><span class="line">H:A,C,D,E,O</span><br><span class="line">I:A,O</span><br><span class="line">J:B,O</span><br><span class="line">K:A,C,D</span><br><span class="line">L:D,E,F</span><br><span class="line">M:E,F,G</span><br><span class="line">O:A,H,I,J</span><br></pre></td></tr></table></figure>

<p>求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？</p>
<h3 id="4-2-实现步骤"><a href="#4-2-实现步骤" class="headerlink" title="4.2 实现步骤"></a>4.2 实现步骤</h3><h5 id="第一步：代码实现"><a href="#第一步：代码实现" class="headerlink" title="第一步：代码实现"></a>第一步：代码实现</h5><p><strong>Mapper类</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public class Step1Mapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">         //1:以冒号拆分行文本数据: 冒号左边就是V2</span><br><span class="line">        String[] split = value.toString().split(&quot;:&quot;);</span><br><span class="line">        String userStr = split[0];</span><br><span class="line"></span><br><span class="line">        //2:将冒号右边的字符串以逗号拆分,每个成员就是K2</span><br><span class="line">        String[] split1 = split[1].split(&quot;,&quot;);</span><br><span class="line">        for (String s : split1) &#123;</span><br><span class="line">            //3:将K2和v2写入上下文中</span><br><span class="line">            context.write(new Text(s), new Text(userStr));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Reducer类:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class Step1Reducer extends Reducer&lt;Text,Text,Text,Text&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:遍历集合,并将每一个元素拼接,得到K3</span><br><span class="line">        StringBuffer buffer = new StringBuffer();</span><br><span class="line"></span><br><span class="line">        for (Text value : values) &#123;</span><br><span class="line">            buffer.append(value.toString()).append(&quot;-&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        //2:K2就是V3</span><br><span class="line">        //3:将K3和V3写入上下文中</span><br><span class="line">        context.write(new Text(buffer.toString()), key);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>JobMain:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //1:获取Job对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;common_friends_step1_job&quot;);</span><br><span class="line"></span><br><span class="line">        //2:设置job任务</span><br><span class="line">            //第一步:设置输入类和输入路径</span><br><span class="line">            job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">            TextInputFormat.addInputPath(job, new Path(&quot;file:///D:\\input\\common_friends_step1_input&quot;));</span><br><span class="line"></span><br><span class="line">            //第二步:设置Mapper类和数据类型</span><br><span class="line">            job.setMapperClass(Step1Mapper.class);</span><br><span class="line">            job.setMapOutputKeyClass(Text.class);</span><br><span class="line">            job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">            //第三,四,五,六</span><br><span class="line"></span><br><span class="line">            //第七步:设置Reducer类和数据类型</span><br><span class="line">            job.setReducerClass(Step1Reducer.class);</span><br><span class="line">            job.setOutputKeyClass(Text.class);</span><br><span class="line">            job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">            //第八步:设置输出类和输出的路径</span><br><span class="line">            job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">            TextOutputFormat.setOutputPath(job, new Path(&quot;file:///D:\\out\\common_friends_step1_out&quot;));</span><br><span class="line"></span><br><span class="line">        //3:等待job任务结束</span><br><span class="line">        boolean bl = job.waitForCompletion(true);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return bl ? 0: 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        //启动job任务</span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line"></span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="第二步：代码实现"><a href="#第二步：代码实现" class="headerlink" title="第二步：代码实现"></a>第二步：代码实现</h5><p><strong>Mapper类</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">public class Step2Mapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123;</span><br><span class="line">    /*</span><br><span class="line">     K1           V1</span><br><span class="line"></span><br><span class="line">     0            A-F-C-J-E-	B</span><br><span class="line">    ----------------------------------</span><br><span class="line"></span><br><span class="line">     K2             V2</span><br><span class="line">     A-C            B</span><br><span class="line">     A-E            B</span><br><span class="line">     A-F            B</span><br><span class="line">     C-E            B</span><br><span class="line"></span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:拆分行文本数据,结果的第二部分可以得到V2</span><br><span class="line">        String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line">        String   friendStr =split[1];</span><br><span class="line"></span><br><span class="line">        //2:继续以&apos;-&apos;为分隔符拆分行文本数据第一部分,得到数组</span><br><span class="line">        String[] userArray = split[0].split(&quot;-&quot;);</span><br><span class="line"></span><br><span class="line">        //3:对数组做一个排序</span><br><span class="line">        Arrays.sort(userArray);</span><br><span class="line"></span><br><span class="line">        //4:对数组中的元素进行两两组合,得到K2</span><br><span class="line">        /*</span><br><span class="line">          A-E-C -----&gt;  A  C  E</span><br><span class="line"></span><br><span class="line">          A  C  E</span><br><span class="line">            A  C  E</span><br><span class="line"></span><br><span class="line">         */</span><br><span class="line">        for (int i = 0; i &lt;userArray.length -1 ; i++) &#123;</span><br><span class="line">            for (int j = i+1; j  &lt; userArray.length ; j++) &#123;</span><br><span class="line">                //5:将K2和V2写入上下文中</span><br><span class="line">                context.write(new Text(userArray[i] +&quot;-&quot;+userArray[j]), new Text(friendStr));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>Reducer类:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class Step2Reducer extends Reducer&lt;Text,Text,Text,Text&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:原来的K2就是K3</span><br><span class="line">        //2:将集合进行遍历,将集合中的元素拼接,得到V3</span><br><span class="line">        StringBuffer buffer = new StringBuffer();</span><br><span class="line">        for (Text value : values) &#123;</span><br><span class="line">            buffer.append(value.toString()).append(&quot;-&quot;);</span><br><span class="line">            </span><br><span class="line">        &#125;</span><br><span class="line">        //3:将K3和V3写入上下文中</span><br><span class="line">        context.write(key, new Text(buffer.toString()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>JobMain:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //1:获取Job对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;common_friends_step2_job&quot;);</span><br><span class="line"></span><br><span class="line">        //2:设置job任务</span><br><span class="line">            //第一步:设置输入类和输入路径</span><br><span class="line">            job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">            TextInputFormat.addInputPath(job, new Path(&quot;file:///D:\\out\\common_friends_step1_out&quot;));</span><br><span class="line"></span><br><span class="line">            //第二步:设置Mapper类和数据类型</span><br><span class="line">            job.setMapperClass(Step2Mapper.class);</span><br><span class="line">            job.setMapOutputKeyClass(Text.class);</span><br><span class="line">            job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">            //第三,四,五,六</span><br><span class="line"></span><br><span class="line">            //第七步:设置Reducer类和数据类型</span><br><span class="line">            job.setReducerClass(Step2Reducer.class);</span><br><span class="line">            job.setOutputKeyClass(Text.class);</span><br><span class="line">            job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">            //第八步:设置输出类和输出的路径</span><br><span class="line">            job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">            TextOutputFormat.setOutputPath(job, new Path(&quot;file:///D:\\out\\common_friends_step2_out&quot;));</span><br><span class="line"></span><br><span class="line">        //3:等待job任务结束</span><br><span class="line">        boolean bl = job.waitForCompletion(true);</span><br><span class="line">        return bl ? 0: 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line">        //启动job任务</span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="1-自定义InputFormat合并小文件"><a href="#1-自定义InputFormat合并小文件" class="headerlink" title="1. 自定义InputFormat合并小文件"></a>1. 自定义InputFormat合并小文件</h2><p><a href="https://manzhong.github.io/images/MapReduce/1-%E8%87%AA%E5%AE%9A%E4%B9%89InputFormat.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/1-%E8%87%AA%E5%AE%9A%E4%B9%89InputFormat.bmp" alt="img"></a></p>
<h3 id="1-1-需求"><a href="#1-1-需求" class="headerlink" title="1.1 需求"></a><strong>1.1 需求</strong></h3><p>无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案</p>
<h3 id="1-2-分析"><a href="#1-2-分析" class="headerlink" title="1.2 分析"></a><strong>1.2 分析</strong></h3><p>小文件的优化无非以下几种方式：</p>
<p>1、 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS</p>
<p>2、 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并</p>
<p>3、 在mapreduce处理时，可采用combineInputFormat提高效率</p>
<h3 id="1-3-实现"><a href="#1-3-实现" class="headerlink" title="1.3 实现"></a><strong>1.3 实现</strong></h3><p>本节实现的是上述第二种方式</p>
<p>程序的核心机制：</p>
<p>自定义一个InputFormat</p>
<p>改写RecordReader，实现一次读取一个完整文件封装为KV</p>
<p>在输出时使用SequenceFileOutPutFormat输出合并文件</p>
<p>代码如下：</p>
<h4 id="自定义InputFromat"><a href="#自定义InputFromat" class="headerlink" title="自定义InputFromat"></a><strong>自定义InputFromat</strong></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public class MyInputFormat extends FileInputFormat&lt;NullWritable,BytesWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:创建自定义RecordReader对象</span><br><span class="line">        MyRecordReader myRecordReader = new MyRecordReader();</span><br><span class="line">        //2:将inputSplit和context对象传给MyRecordReader</span><br><span class="line">        myRecordReader.initialize(inputSplit, taskAttemptContext);</span><br><span class="line">        return myRecordReader;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     设置文件是否可以被切割</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    protected boolean isSplitable(JobContext context, Path filename) &#123;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="自定义RecordReader"><a href="#自定义RecordReader" class="headerlink" title="自定义RecordReader"></a><strong>自定义</strong>RecordReader</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">public class MyRecordReader extends RecordReader&lt;NullWritable,BytesWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">    private Configuration configuration = null;</span><br><span class="line">    private  FileSplit fileSplit = null;</span><br><span class="line">    private boolean processed = false;</span><br><span class="line">    private BytesWritable bytesWritable = new BytesWritable();</span><br><span class="line">    private  FileSystem fileSystem = null;</span><br><span class="line">    private  FSDataInputStream inputStream = null;</span><br><span class="line">    //进行初始化工作</span><br><span class="line">    @Override</span><br><span class="line">    public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123;</span><br><span class="line">        //获取文件的切片</span><br><span class="line">          fileSplit= (FileSplit)inputSplit;</span><br><span class="line"></span><br><span class="line">        //获取Configuration对象</span><br><span class="line">         configuration = taskAttemptContext.getConfiguration();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //该方法用于获取K1和V1</span><br><span class="line">    /*</span><br><span class="line">     K1: NullWritable</span><br><span class="line">     V1: BytesWritable</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public boolean nextKeyValue() throws IOException, InterruptedException &#123;</span><br><span class="line">        if(!processed)&#123;</span><br><span class="line">            //1:获取源文件的字节输入流</span><br><span class="line">            //1.1 获取源文件的文件系统 (FileSystem)</span><br><span class="line">             fileSystem = FileSystem.get(configuration);</span><br><span class="line">            //1.2 通过FileSystem获取文件字节输入流</span><br><span class="line">             inputStream = fileSystem.open(fileSplit.getPath());</span><br><span class="line"></span><br><span class="line">            //2:读取源文件数据到普通的字节数组(byte[])</span><br><span class="line">            byte[] bytes = new byte[(int) fileSplit.getLength()];</span><br><span class="line">            IOUtils.readFully(inputStream, bytes, 0, (int)fileSplit.getLength());</span><br><span class="line"></span><br><span class="line">            //3:将字节数组中数据封装到BytesWritable ,得到v1</span><br><span class="line"></span><br><span class="line">            bytesWritable.set(bytes, 0, (int)fileSplit.getLength());</span><br><span class="line"></span><br><span class="line">            processed = true;</span><br><span class="line"></span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //返回K1</span><br><span class="line">    @Override</span><br><span class="line">    public NullWritable getCurrentKey() throws IOException, InterruptedException &#123;</span><br><span class="line">        return NullWritable.get();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //返回V1</span><br><span class="line">    @Override</span><br><span class="line">    public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123;</span><br><span class="line">        return bytesWritable;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //获取文件读取的进度</span><br><span class="line">    @Override</span><br><span class="line">    public float getProgress() throws IOException, InterruptedException &#123;</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //进行资源释放</span><br><span class="line">    @Override</span><br><span class="line">    public void close() throws IOException &#123;</span><br><span class="line">        inputStream.close();</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Mapper类"><a href="#Mapper类" class="headerlink" title="Mapper类:"></a>Mapper类:</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public class SequenceFileMapper extends Mapper&lt;NullWritable,BytesWritable,Text,BytesWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:获取文件的名字,作为K2</span><br><span class="line">        FileSplit fileSplit = (FileSplit) context.getInputSplit();</span><br><span class="line">        String fileName = fileSplit.getPath().getName();</span><br><span class="line"></span><br><span class="line">        //2:将K2和V2写入上下文中</span><br><span class="line">        context.write(new Text(fileName), value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="主类"><a href="#主类" class="headerlink" title="主类:"></a>主类:</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //1:获取job对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;sequence_file_job&quot;);</span><br><span class="line"></span><br><span class="line">        //2:设置job任务</span><br><span class="line">            //第一步:设置输入类和输入的路径</span><br><span class="line">            job.setInputFormatClass(MyInputFormat.class);</span><br><span class="line">            MyInputFormat.addInputPath(job, new Path(&quot;file:///D:\\input\\myInputformat_input&quot;));</span><br><span class="line"></span><br><span class="line">            //第二步:设置Mapper类和数据类型</span><br><span class="line">            job.setMapperClass(SequenceFileMapper.class);</span><br><span class="line">            job.setMapOutputKeyClass(Text.class);</span><br><span class="line">            job.setMapOutputValueClass(BytesWritable.class);</span><br><span class="line"></span><br><span class="line">            //第七步: 不需要设置Reducer类,但是必须设置数据类型</span><br><span class="line">            job.setOutputKeyClass(Text.class);</span><br><span class="line">            job.setOutputValueClass(BytesWritable.class);</span><br><span class="line"></span><br><span class="line">            //第八步:设置输出类和输出的路径</span><br><span class="line">            job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br><span class="line">            SequenceFileOutputFormat.setOutputPath(job, new Path(&quot;file:///D:\\out\\myinputformat_out&quot;));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //3:等待job任务执行结束</span><br><span class="line">        boolean bl = job.waitForCompletion(true);</span><br><span class="line">        return bl ? 0 : 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line"></span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2-自定义outputFormat"><a href="#2-自定义outputFormat" class="headerlink" title="2. 自定义outputFormat"></a>2. 自定义outputFormat</h2><p><a href="https://manzhong.github.io/images/MapReduce/2-%E8%87%AA%E5%AE%9A%E4%B9%89OutputFormat.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/2-%E8%87%AA%E5%AE%9A%E4%B9%89OutputFormat.bmp" alt="img"></a></p>
<h3 id="2-1-需求-1"><a href="#2-1-需求-1" class="headerlink" title="2.1 需求"></a><strong>2.1</strong> <strong>需求</strong></h3><p>现在有一些订单的评论数据，需求，将订单的好评与差评进行区分开来，将最终的数据分开到不同的文件夹下面去，数据内容参见资料文件夹，其中数据第九个字段表示好评，中评，差评。0：好评，1：中评，2：差评</p>
<h3 id="2-2-分析"><a href="#2-2-分析" class="headerlink" title="2.2 分析"></a><strong>2.2 分析</strong></h3><p>程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现</p>
<h3 id="2-3-实现"><a href="#2-3-实现" class="headerlink" title="2.3 实现"></a><strong>2.3 实现</strong></h3><p>实现要点：</p>
<p>1、 在mapreduce中访问外部资源</p>
<p>2、 自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write()</p>
<h4 id="第一步：自定义MyOutputFormat"><a href="#第一步：自定义MyOutputFormat" class="headerlink" title="第一步：自定义MyOutputFormat"></a><strong>第一步</strong>：自定义MyOutputFormat</h4><p><strong>MyOutputFormat类:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class MyOutputFormat extends FileOutputFormat&lt;Text,NullWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:获取目标文件的输出流(两个)</span><br><span class="line">        FileSystem fileSystem = FileSystem.get(taskAttemptContext.getConfiguration());</span><br><span class="line">        FSDataOutputStream goodCommentsOutputStream = fileSystem.create(new Path(&quot;file:///D:\\out\\good_comments\\good_comments.txt&quot;));</span><br><span class="line">        FSDataOutputStream badCommentsOutputStream = fileSystem.create(new Path(&quot;file:///D:\\out\\bad_comments\\bad_comments.txt&quot;));</span><br><span class="line"></span><br><span class="line">        //2:将输出流传给MyRecordWriter</span><br><span class="line">        MyRecordWriter myRecordWriter = new MyRecordWriter(goodCommentsOutputStream,badCommentsOutputStream);</span><br><span class="line"></span><br><span class="line">        return myRecordWriter;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>MyRecordReader类:</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">public class MyRecordWriter extends RecordWriter&lt;Text,NullWritable&gt; &#123;</span><br><span class="line">    private FSDataOutputStream goodCommentsOutputStream;</span><br><span class="line">    private FSDataOutputStream badCommentsOutputStream;</span><br><span class="line"></span><br><span class="line">    public MyRecordWriter() &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public MyRecordWriter(FSDataOutputStream goodCommentsOutputStream, FSDataOutputStream badCommentsOutputStream) &#123;</span><br><span class="line">        this.goodCommentsOutputStream = goodCommentsOutputStream;</span><br><span class="line">        this.badCommentsOutputStream = badCommentsOutputStream;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     *</span><br><span class="line">     * @param text  行文本内容</span><br><span class="line">     * @param nullWritable</span><br><span class="line">     * @throws IOException</span><br><span class="line">     * @throws InterruptedException</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public void write(Text text, NullWritable nullWritable) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:从行文本数据中获取第9个字段</span><br><span class="line">        String[] split = text.toString().split(&quot;\t&quot;);</span><br><span class="line">        String numStr = split[9];</span><br><span class="line"></span><br><span class="line">        //2:根据字段的值,判断评论的类型,然后将对应的数据写入不同的文件夹文件中</span><br><span class="line">        if(Integer.parseInt(numStr) &lt;= 1)&#123;</span><br><span class="line">            //好评或者中评</span><br><span class="line">            goodCommentsOutputStream.write(text.toString().getBytes());</span><br><span class="line">            goodCommentsOutputStream.write(&quot;\r\n&quot;.getBytes());</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            //差评</span><br><span class="line">            badCommentsOutputStream.write(text.toString().getBytes());</span><br><span class="line">            badCommentsOutputStream.write(&quot;\r\n&quot;.getBytes());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123;</span><br><span class="line">        IOUtils.closeStream(goodCommentsOutputStream);</span><br><span class="line">        IOUtils.closeStream(badCommentsOutputStream);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第二步：自定义Mapper类"><a href="#第二步：自定义Mapper类" class="headerlink" title="第二步：自定义Mapper类"></a><strong>第二步</strong>：自定义Mapper类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public class MyOutputFormatMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第三步-主类JobMain"><a href="#第三步-主类JobMain" class="headerlink" title="第三步:主类JobMain"></a>第三步:主类JobMain</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //1:获取job对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;myoutputformat_job&quot;);</span><br><span class="line"></span><br><span class="line">        //2:设置job任务</span><br><span class="line">            //第一步:设置输入类和输入的路径</span><br><span class="line">            job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">            TextInputFormat.addInputPath(job, new Path(&quot;file:///D:\\input\\myoutputformat_input&quot;));</span><br><span class="line"></span><br><span class="line">            //第二步:设置Mapper类和数据类型</span><br><span class="line">            job.setMapperClass(MyOutputFormatMapper.class);</span><br><span class="line">            job.setMapOutputKeyClass(Text.class);</span><br><span class="line">            job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">            //第八步:设置输出类和输出的路径</span><br><span class="line">            job.setOutputFormatClass(MyOutputFormat.class);</span><br><span class="line">            MyOutputFormat.setOutputPath(job, new Path(&quot;file:///D:\\out\\myoutputformat_out&quot;));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        //3:等待任务结束</span><br><span class="line">        boolean bl = job.waitForCompletion(true);</span><br><span class="line">        return bl ? 0 : 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-自定义分组求取topN"><a href="#3-自定义分组求取topN" class="headerlink" title="3. 自定义分组求取topN"></a>3. 自定义分组求取topN</h2><p><a href="https://manzhong.github.io/images/MapReduce/3-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E7%BB%84%E6%B1%82TopN.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/MapReduce/3-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E7%BB%84%E6%B1%82TopN.bmp" alt="img"></a></p>
<p>分组是mapreduce当中reduce端的一个功能组件，主要的作用是决定哪些数据作为一组，调用一次reduce的逻辑，默认是每个不同的key，作为多个不同的组，每个组调用一次reduce逻辑，我们可以自定义分组实现不同的key作为同一个组，调用一次reduce逻辑</p>
<h3 id="3-1-需求"><a href="#3-1-需求" class="headerlink" title="3.1 需求"></a><strong>3.1 需求</strong></h3><p>有如下订单数据</p>
<table>
<thead>
<tr>
<th align="left">订单id</th>
<th align="left">商品id</th>
<th align="left">成交金额</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Order_0000001</td>
<td align="left">Pdt_01</td>
<td align="left">222.8</td>
</tr>
<tr>
<td align="left">Order_0000001</td>
<td align="left">Pdt_05</td>
<td align="left">25.8</td>
</tr>
<tr>
<td align="left">Order_0000002</td>
<td align="left">Pdt_03</td>
<td align="left">522.8</td>
</tr>
<tr>
<td align="left">Order_0000002</td>
<td align="left">Pdt_04</td>
<td align="left">122.4</td>
</tr>
<tr>
<td align="left">Order_0000002</td>
<td align="left">Pdt_05</td>
<td align="left">722.4</td>
</tr>
<tr>
<td align="left">Order_0000003</td>
<td align="left">Pdt_01</td>
<td align="left">222.8</td>
</tr>
</tbody></table>
<p>现在需要求出每一个订单中成交金额最大的一笔交易</p>
<h3 id="3-2-分析"><a href="#3-2-分析" class="headerlink" title="3.2 分析"></a><strong>3.2 分析</strong></h3><p>1、利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce</p>
<p>2、在reduce端利用分组将订单id相同的kv聚合成组，然后取第一个即是最大值</p>
<h3 id="3-3-实现"><a href="#3-3-实现" class="headerlink" title="3.3 实现"></a><strong>3.3 实现</strong></h3><h4 id="第一步-定义OrderBean"><a href="#第一步-定义OrderBean" class="headerlink" title="第一步:定义OrderBean"></a><strong>第一步:</strong>定义OrderBean</h4><p>定义一个OrderBean，里面定义两个字段，第一个字段是我们的orderId，第二个字段是我们的金额（注意金额一定要使用Double或者DoubleWritable类型，否则没法按照金额顺序排序）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">public class OrderBean  implements WritableComparable&lt;OrderBean&gt;&#123;</span><br><span class="line">    private  String orderId;</span><br><span class="line">    private  Double price;</span><br><span class="line"></span><br><span class="line">    public String getOrderId() &#123;</span><br><span class="line">        return orderId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setOrderId(String orderId) &#123;</span><br><span class="line">        this.orderId = orderId;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public Double getPrice() &#123;</span><br><span class="line">        return price;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setPrice(Double price) &#123;</span><br><span class="line">        this.price = price;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return  orderId + &quot;\t&quot; + price;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //指定排序规则</span><br><span class="line">    @Override</span><br><span class="line">    public int compareTo(OrderBean orderBean) &#123;</span><br><span class="line">        //先比较订单ID,如果订单ID一致,则排序订单金额(降序)</span><br><span class="line">        int i = this.orderId.compareTo(orderBean.orderId);</span><br><span class="line">        if(i == 0)&#123;</span><br><span class="line">            i = this.price.compareTo(orderBean.price) * -1;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //实现对象的序列化</span><br><span class="line">    @Override</span><br><span class="line">    public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">         out.writeUTF(orderId);</span><br><span class="line">         out.writeDouble(price);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //实现对象反序列化</span><br><span class="line">    @Override</span><br><span class="line">    public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">        this.orderId = in.readUTF();</span><br><span class="line">        this.price  = in.readDouble();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第二步-定义Mapper类"><a href="#第二步-定义Mapper类" class="headerlink" title="第二步: 定义Mapper类"></a>第二步: 定义Mapper类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public class GroupMapper extends Mapper&lt;LongWritable,Text,OrderBean,Text&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        //1:拆分行文本数据,得到订单的ID,订单的金额</span><br><span class="line">        String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line"></span><br><span class="line">        //2:封装OrderBean,得到K2</span><br><span class="line">        OrderBean orderBean = new OrderBean();</span><br><span class="line">        orderBean.setOrderId(split[0]);</span><br><span class="line">        orderBean.setPrice(Double.valueOf(split[2]));</span><br><span class="line"></span><br><span class="line">        //3:将K2和V2写入上下文中</span><br><span class="line">        context.write(orderBean, value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第三步-自定义分区"><a href="#第三步-自定义分区" class="headerlink" title="第三步:自定义分区"></a><strong>第三步:</strong>自定义分区</h4><p>自定义分区，按照订单id进行分区，把所有订单id相同的数据，都发送到同一个reduce中去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public class OrderPartition extends Partitioner&lt;OrderBean,Text&gt; &#123;</span><br><span class="line">    //分区规则: 根据订单的ID实现分区</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     *</span><br><span class="line">     * @param orderBean K2</span><br><span class="line">     * @param text  V2</span><br><span class="line">     * @param i  ReduceTask个数</span><br><span class="line">     * @return 返回分区的编号</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public int getPartition(OrderBean orderBean, Text text, int i) &#123;</span><br><span class="line">        return (orderBean.getOrderId().hashCode() &amp; 2147483647) % i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第四步-自定义分组"><a href="#第四步-自定义分组" class="headerlink" title="第四步:自定义分组"></a><strong>第四步:</strong>自定义分组</h4><p>按照我们自己的逻辑进行分组，通过比较相同的订单id，将相同的订单id放到一个组里面去，进过分组之后当中的数据，已经全部是排好序的数据，我们只需要取前topN即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 1: 继承WriteableComparator</span><br><span class="line">public class OrderGroupComparator extends WritableComparator &#123;</span><br><span class="line">    // 2: 调用父类的有参构造</span><br><span class="line">    public OrderGroupComparator() &#123;</span><br><span class="line">        super(OrderBean.class,true);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //3: 指定分组的规则(重写方法)</span><br><span class="line">    @Override</span><br><span class="line">    public int compare(WritableComparable a, WritableComparable b) &#123;</span><br><span class="line">        //3.1 对形参做强制类型转换</span><br><span class="line">        OrderBean first = (OrderBean)a;</span><br><span class="line">        OrderBean second = (OrderBean)b;</span><br><span class="line"></span><br><span class="line">        //3.2 指定分组规则</span><br><span class="line">        return first.getOrderId().compareTo(second.getOrderId());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第五步-定义Reducer类"><a href="#第五步-定义Reducer类" class="headerlink" title="第五步:定义Reducer类"></a>第五步:定义Reducer类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class GroupReducer extends Reducer&lt;OrderBean,Text,Text,NullWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(OrderBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        int i = 0;</span><br><span class="line">        //获取集合中的前N条数据</span><br><span class="line">        for (Text value : values) &#123;</span><br><span class="line">            context.write(value, NullWritable.get());</span><br><span class="line">            i++;</span><br><span class="line">            if(i &gt;= 1)&#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="第六步-程序main函数入口"><a href="#第六步-程序main函数入口" class="headerlink" title="第六步:程序main函数入口"></a><strong>第六步:</strong>程序main函数入口</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public class JobMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //1:获取Job对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;mygroup_job&quot;);</span><br><span class="line"></span><br><span class="line">        //2:设置job任务</span><br><span class="line">            //第一步:设置输入类和输入路径</span><br><span class="line">            job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">            TextInputFormat.addInputPath(job, new Path(&quot;file:///D:\\input\\mygroup_input&quot;));</span><br><span class="line"></span><br><span class="line">            //第二步:设置Mapper类和数据类型</span><br><span class="line">            job.setMapperClass(GroupMapper.class);</span><br><span class="line">            job.setMapOutputKeyClass(OrderBean.class);</span><br><span class="line">            job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">            //第三,四,五,六</span><br><span class="line">            //设置分区</span><br><span class="line">            job.setPartitionerClass(OrderPartition.class);</span><br><span class="line">            //设置分组</span><br><span class="line">            job.setGroupingComparatorClass(OrderGroupComparator.class);</span><br><span class="line"></span><br><span class="line">            //第七步:设置Reducer类和数据类型</span><br><span class="line">            job.setReducerClass(GroupReducer.class);</span><br><span class="line">            job.setOutputKeyClass(Text.class);</span><br><span class="line">            job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">            //第八步:设置输出类和输出的路径</span><br><span class="line">            job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">            TextOutputFormat.setOutputPath(job, new Path(&quot;file:///D:\\out\\mygroup_out&quot;));</span><br><span class="line"></span><br><span class="line">        //3:等待job任务结束</span><br><span class="line">        boolean bl = job.waitForCompletion(true);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return bl ? 0: 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = new Configuration();</span><br><span class="line"></span><br><span class="line">        //启动job任务</span><br><span class="line">        int run = ToolRunner.run(configuration, new JobMain(), args);</span><br><span class="line"></span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/MapReduce2/" data-id="cjz24s2xv000cu8u5o3ambsz4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Yarn-资源调度" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/08/Yarn-资源调度/" class="article-date">
  <time datetime="2019-08-08T03:20:02.000Z" itemprop="datePublished">2019-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/08/Yarn-资源调度/">Yarn-资源调度</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Yarn资源调度详解"><a href="#Yarn资源调度详解" class="headerlink" title="Yarn资源调度详解"></a>Yarn资源调度详解</h1><h3 id="1-yarn的介绍："><a href="#1-yarn的介绍：" class="headerlink" title="1.yarn的介绍："></a>1.<strong>yarn</strong>的介绍：</h3><p> yarn是hadoop集群当中的资源管理系统模块，从hadoop2.0开始引入yarn模块,yarn可为各类计算框架提供资源的管理和调度,主要用于管理集群当中的资源（主要是服务器的各种硬件资源，包括CPU，内存，磁盘，网络IO等）以及调度运行在yarn上面的各种任务。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn核心出发点是为了分离资源管理与作业监控，实现分离的做法是拥有一个全局的资源管理（ResourceManager，RM），以及每个应用程序对应一个的应用管理器（ApplicationMaster，AM）</span><br></pre></td></tr></table></figure>

<p> 总结一句话就是说：yarn主要就是为了调度资源，管理任务等</p>
<p>其调度分为两个层级来说：</p>
<ul>
<li><p>一级调度管理：</p>
<p> 计算资源管理(CPU,内存，网络IO，磁盘)</p>
</li>
<li><p>二级调度管理：</p>
<p> 任务内部的计算模型管理 (AppMaster的任务精细化管理)</p>
</li>
</ul>
<p><strong>yarn的官网文档说明：</strong></p>
<p><a href="http://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-site/YARN.html</a></p>
<p><strong>yarn集群的监控管理界面：</strong></p>
<p><a href="http://node01:8088/cluster" target="_blank" rel="noopener">http://node01:8088/cluster</a></p>
<p><strong>jobHistoryServer查看界面：</strong></p>
<p><a href="http://node01:19888/jobhistory" target="_blank" rel="noopener">http://node01:19888/jobhistory</a></p>
<h3 id="2-Yarn的主要组件介绍与作用"><a href="#2-Yarn的主要组件介绍与作用" class="headerlink" title="2.Yarn的主要组件介绍与作用"></a>2.<strong>Yarn的</strong>主要组件介绍与作用</h3><p><code>YARN总体上是Master/Slave结构</code>，主要由ResourceManager、NodeManager、 ApplicationMaster和Container等几个组件构成。</p>
<ul>
<li><p>ResourceManager(RM)</p>
<p>负责处理客户端请求,对各NM上的资源进行统一管理和调度。给ApplicationMaster分配空闲的Container 运行并监控其运行状态。主要由两个组件构成：调度器和应用程序管理器：</p>
<ol>
<li><strong>调度器(Scheduler)</strong>：调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container。Shceduler不负责监控或者跟踪应用程序的状态。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。</li>
<li><strong>应用程序管理器(Applications Manager)</strong>：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster 、监控ApplicationMaster运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。</li>
</ol>
</li>
<li><p><strong>NodeManager (NM)</strong><br>NodeManager 是每个节点上的资源和任务管理器。它会定时地向ResourceManager汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自ApplicationMaster 的Container 启动/停止等请求。</p>
</li>
<li><p><strong>ApplicationMaster (AM)</strong>：<br>用户提交的应用程序均包含一个*<em>ApplicationMaster *</em>，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。</p>
</li>
<li><p><strong>Container</strong>：<br>Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当ApplicationMaster向ResourceManager申请资源时，ResourceManager为ApplicationMaster 返回的资源便是用Container 表示的。</p>
</li>
</ul>
<h3 id="3-yarn的架构和工作流程"><a href="#3-yarn的架构和工作流程" class="headerlink" title="3.yarn的架构和工作流程"></a>3.<strong>yarn</strong>的架构和工作流程</h3><p><a href="https://manzhong.github.io/images/yarn/4-yarn%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/yarn/4-yarn%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="img"></a></p>
<h3 id="4-yarn的调度器"><a href="#4-yarn的调度器" class="headerlink" title="4.yarn的调度器"></a>4.yarn的调度器</h3><p>yarn我们都知道主要是用于做资源调度，任务分配等功能的，那么在hadoop当中，究竟使用什么算法来进行任务调度就需要我们关注了，hadoop支持好几种任务的调度方式，不同的场景需要使用不同的任务调度器.</p>
<h5 id="第一种调度器：FIFO-Scheduler（队列调度）"><a href="#第一种调度器：FIFO-Scheduler（队列调度）" class="headerlink" title="第一种调度器：FIFO Scheduler（队列调度）"></a><code>第一种调度器：FIFO Scheduler（队列调度）</code></h5><p>把任务按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的任务进行分配资源，待最头上任务需求满足后再给下一个分配，以此类推。</p>
<p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的任务可能会占用所有集群资源，这就导致其它任务被阻塞。</p>
<h5 id="第二种调度器：Capacity-Scheduler（容量调度器，apache版本默认使用的调度器）"><a href="#第二种调度器：Capacity-Scheduler（容量调度器，apache版本默认使用的调度器）" class="headerlink" title="第二种调度器：Capacity Scheduler（容量调度器，apache版本默认使用的调度器）"></a><code>第二种调度器：Capacity Scheduler（容量调度器，apache版本默认使用的调度器）</code></h5><p>Capacity 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。</p>
<h5 id="第三种调度器：Fair-Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）"><a href="#第三种调度器：Fair-Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）" class="headerlink" title="第三种调度器：Fair Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）"></a><code>第三种调度器：Fair Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）</code></h5><p>Fair调度器的设计目标是为所有的应用分配公平的资源（对公平的定义可以通过参数来设置）。公平调度在也可以在多个队列间工作。举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共享</p>
<p>使用哪种调度器取决于yarn-site.xml当中的</p>
<p><code>yarn.resourcemanager.scheduler.class</code> 这个属性的配置</p>
<h3 id="5-关于yarn常用参数设置"><a href="#5-关于yarn常用参数设置" class="headerlink" title="5.关于yarn常用参数设置"></a>5.<strong>关于</strong>yarn常用参数设置</h3><h4 id="设置container分配最小内存"><a href="#设置container分配最小内存" class="headerlink" title="设置container分配最小内存"></a>设置container分配<strong>最小内存</strong></h4><p>yarn.scheduler.minimum-allocation-mb 1024 给应用程序container分配的最小内存</p>
<h4 id="设置container分配最大内存"><a href="#设置container分配最大内存" class="headerlink" title="设置container分配最大内存"></a>设置container分配<strong>最大内存</strong></h4><p>yarn.scheduler.maximum-allocation-mb 8192 给应用程序container分配的最大内存</p>
<h4 id="设置每个container的最小虚拟内核个数"><a href="#设置每个container的最小虚拟内核个数" class="headerlink" title="设置每个container的最小虚拟内核个数"></a>设置每个<strong>container的</strong>最小<strong>虚拟内核个数</strong></h4><p>yarn.scheduler.minimum-allocation-vcores 1 每个container默认给分配的最小的虚拟内核个数</p>
<h4 id="设置每个container的最大虚拟内核个数"><a href="#设置每个container的最大虚拟内核个数" class="headerlink" title="设置每个container的最大虚拟内核个数"></a>设置每个container的最大虚拟内核个数</h4><p>yarn.scheduler.maximum-allocation-vcores 32 每个container可以分配的最大的虚拟内核的个数</p>
<h4 id="设置NodeManager可以分配的内存大小"><a href="#设置NodeManager可以分配的内存大小" class="headerlink" title="设置NodeManager可以分配的内存大小"></a>设置NodeManager可以分配的内存大小</h4><p>yarn.nodemanager.resource.memory-mb 8192 nodemanager可以分配的最大内存大小，默认8192Mb</p>
<h4 id="定义每台机器的内存使用大小"><a href="#定义每台机器的内存使用大小" class="headerlink" title="定义每台机器的内存使用大小"></a>定义每台机器的内存使用大小</h4><p>yarn.nodemanager.resource.memory-mb 8192</p>
<h4 id="定义交换区空间可以使用的大小"><a href="#定义交换区空间可以使用的大小" class="headerlink" title="定义交换区空间可以使用的大小"></a>定义交换区空间可以使用的大小</h4><p>交换区空间就是讲一块硬盘拿出来做内存使用,这里指定的是nodemanager的2.1倍</p>
<p>yarn.nodemanager.vmem-pmem-ratio 2.1</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/08/08/Yarn-资源调度/" data-id="cjz24s2wr0003u8u5x1px3vvc" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/08/08/Linux根目录权限修复方法/">Linux根目录权限修复方法</a>
          </li>
        
          <li>
            <a href="/2019/08/08/Linux常用命令/">Linux常用命令</a>
          </li>
        
          <li>
            <a href="/2019/08/08/Linux开发环境/">Linux开发环境</a>
          </li>
        
          <li>
            <a href="/2019/08/08/shell/">shell</a>
          </li>
        
          <li>
            <a href="/2019/08/08/Zookeeper/">Zookeeper</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>