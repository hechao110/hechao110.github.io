<!DOCTYPE html><html lang="hc-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一只沙皮狗的悲伤"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.4"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.4"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>Hdfs | 一只沙皮狗的悲伤</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Hdfs</h1><a id="logo" href="/.">一只沙皮狗的悲伤</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Inicio</i></a><a href="/archives/"><i class="fa fa-archive"> Archivo</i></a><a href="/about/"><i class="fa fa-user"> Acerca de</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Búsqueda"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Hdfs</h1><div class="post-meta"><a href="/2019/08/08/Hdfs/#comments" class="comment-count"></a><p><span class="date">Aug 08, 2019</span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>Golpes</i></i></span></p></div><div class="post-content"><h1 id="Hadoop的核心-Hdfs"><a href="#Hadoop的核心-Hdfs" class="headerlink" title="Hadoop的核心 Hdfs"></a>Hadoop的核心 Hdfs</h1><h2 id="1-HDFS概述"><a href="#1-HDFS概述" class="headerlink" title="1. HDFS概述"></a>1. HDFS概述</h2><h3 id="1-1-介绍"><a href="#1-1-介绍" class="headerlink" title="1.1 介绍"></a>1.1 介绍</h3><p>在现代的企业环境中，单机容量往往无法存储大量数据，需要跨机器存储。统一管理分布在集群上的文件系统称为<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">分布式文件系统</a> 。</p>
<p> <a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">HDFS</a>（Hadoop Distributed File System）是 Apache Hadoop 项目的一个子项目. Hadoop 非常适于存储大型数据 (比如 TB 和 PB), 其就是使用 HDFS 作为存储系统. HDFS 使用多台计算机存储文件, 并且提供统一的访问接口, 像是访问一个普通文件系统一样使用分布式文件系统.</p>
<h3 id="1-2-历史"><a href="#1-2-历史" class="headerlink" title="1.2 历史"></a>1.2 历史</h3><ol>
<li><a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">Doug Cutting</a> 在做 Lucene 的时候, 需要编写一个爬虫服务, 这个爬虫写的并不顺利, 遇到了一些问题, 诸如: 如何存储大规模的数据, 如何保证集群的可伸缩性, 如何动态容错等</li>
<li>2013年的时候, Google 发布了三篇论文, 被称作为三驾马车, 其中有一篇叫做 GFS, 是描述了 Google 内部的一个叫做 <a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">GFS</a> 的分布式大规模文件系统, 具有强大的可伸缩性和容错性</li>
<li>Doug Cutting 后来根据 GFS 的论文, 创造了一个新的文件系统, 叫做 HDFS</li>
</ol>
<h2 id="2-HDFS应用场景"><a href="#2-HDFS应用场景" class="headerlink" title="2. HDFS应用场景"></a>2. HDFS应用场景</h2><h3 id="2-1-适合的应用场景"><a href="#2-1-适合的应用场景" class="headerlink" title="2.1 适合的应用场景"></a>2.1 适合的应用场景</h3><ul>
<li>存储非常大的文件：这里非常大指的是几百M、G、或者TB级别，需要<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">高吞吐量</a>，对<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">延时没有要求</a>。</li>
<li>采用流式的数据访问方式: 即<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">一次写入、多次读取</a>，数据集经常从数据源生成或者拷贝一次，然后在其上做很多分析工作 。</li>
<li>运行于商业硬件上: Hadoop不需要特别贵的机器，可运行于普通廉价机器，可以处<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">节约成本</a></li>
<li>需要高<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">容错性</a></li>
<li>为数据存储提供所需的<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">扩展能力</a></li>
</ul>
<h3 id="2-2-不适合的应用场景"><a href="#2-2-不适合的应用场景" class="headerlink" title="2.2 不适合的应用场景"></a>2.2 不适合的应用场景</h3><p>1） 低延时的数据访问<br>对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的,因此可能<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">牺牲延时</a></p>
<p>2）大量小文件<br>文件的元数据保存在<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">NameNode的内存中</a>， 整个文件系统的文件数量会受限于NameNode的内存大小。<br>经验而言，一个文件/目录/文件块一般占有150字节的元数据内存空间。如果有100万个文件，每个文件占用1个文件块，则需要大约300M的内存。因此十亿级别的文件数量在现有商用机器上难以支持。</p>
<p>3）多方读写，需要任意的文件修改<br>HDFS采用追加（append-only）的方式写入数据。<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">不支持文件任意offset的修改</a>。不支持多个写入器（writer）</p>
<h2 id="3-HDFS-的架构"><a href="#3-HDFS-的架构" class="headerlink" title="3. HDFS 的架构"></a>3. HDFS 的架构</h2><p>HDFS是一个<code>主/从（Mater/Slave）体系结构</code>，</p>
<p>HDFS由四部分组成，<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">HDFS Client</a>、<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">NameNod</a><a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">e</a>、<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">DataNode</a>和<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">Secondary NameNode</a>。</p>
<p>　**1、Client：就是客户端。</p>
<ul>
<li>文件切分。文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储。</li>
<li>与 NameNode 交互，获取文件的位置信息。</li>
<li>与 DataNode 交互，读取或者写入数据。</li>
<li>Client 提供一些命令来管理 和访问HDFS，比如启动或者关闭HDFS。</li>
</ul>
<p>　　<strong>2、NameNode：就是 master，它是一个主管、管理者。</strong></p>
<ul>
<li>管理 HDFS 的名称空间</li>
<li>管理数据块（Block）映射信息</li>
<li>配置副本策略</li>
<li>处理客户端读写请求。</li>
</ul>
<p>　　<strong>3、DataNode：就是Slave。NameNode 下达命令，DataNode 执行实际的操作。</strong></p>
<ul>
<li>存储实际的数据块。</li>
<li>执行数据块的读/写操作。</li>
</ul>
<p>　　<strong>4、Secondary NameNode：并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。</strong></p>
<ul>
<li>辅助 NameNode，分担其工作量。</li>
<li>定期合并 fsimage和fsedits，并推送给NameNode。</li>
<li>在紧急情况下，可辅助恢复 NameNode。</li>
</ul>
<h2 id="4-NameNode和DataNode"><a href="#4-NameNode和DataNode" class="headerlink" title="4:NameNode和DataNode"></a>4:NameNode和DataNode</h2><p>###4.1 NameNode作用</p>
<ul>
<li><p>NameNode在内存中保存着整个文件系统的<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">名称</a><a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">空间</a>和文件数据块的<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">地址映射</a></p>
</li>
<li><p>整个HDFS可存储的文件数受限于<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">NameNode的内存大小</a></p>
<p><code>1、NameNode元数据信息</code><br>文件名，文件目录结构，文件属性(生成时间，副本数，权限)每个文件的块列表。<br>以及列表中的块与块所在的DataNode之间的地址映射关系<br>在内存中加载文件系统中每个文件和每个数据块的引用关系(文件、block、datanode之间的映射信息)<br>数据会定期保存到本地磁盘（fsImage文件和edits文件）</p>
</li>
</ul>
<p><code>2、NameNode文件操作</code><br>NameNode负责文件元数据的操作<br>DataNode负责处理文件内容的读写请求，数据流不经过NameNode，会询问它跟那个DataNode联系</p>
<p><code>3、NameNode副本</code><br>文件数据块到底存放到哪些DataNode上，是由NameNode决定的，NN根据全局情况做出放置副本的决定</p>
<p><code>4、NameNode心跳机制</code><br>全权管理数据块的复制，周期性的接受心跳和块的状态报告信息（包含该DataNode上所有数据块的列表）<br>若接受到心跳信息，NameNode认为DataNode工作正常，如果在10分钟后还接受到不到DN的心跳，那么NameNode认为DataNode已经宕机 ,这时候NN准备要把DN上的数据块进行重新的复制。 块的状态报告包含了一个DN上所有数据块的列表，blocks report 每个1小时发送一次.</p>
<h3 id="4-2-DataNode作用"><a href="#4-2-DataNode作用" class="headerlink" title="4.2 DataNode作用"></a>4.2 DataNode作用</h3><p>提供真实文件数据的存储服务。</p>
<p>1、Data Node以数据块的形式存储HDFS文件</p>
<p>2、Data Node 响应HDFS 客户端读写请求</p>
<p>3、Data Node 周期性向NameNode汇报心跳信息</p>
<p>4、Data Node 周期性向NameNode汇报数据块信息</p>
<p>5、Data Node 周期性向NameNode汇报缓存数据块信息</p>
<h2 id="5-HDFS的副本机制和机架感知"><a href="#5-HDFS的副本机制和机架感知" class="headerlink" title="5:HDFS的副本机制和机架感知"></a>5:HDFS的副本机制和机架感知</h2><h3 id="5-1-HDFS-文件副本机制"><a href="#5-1-HDFS-文件副本机制" class="headerlink" title="5.1 HDFS 文件副本机制"></a>5.1 HDFS 文件副本机制</h3><p>所有的文件都是以 block 块的方式存放在 HDFS 文件系统当中,作用如下</p>
<ol>
<li>一个文件有可能大于集群中任意一个磁盘，引入块机制,可以很好的解决这个问题</li>
<li>使用块作为文件存储的逻辑单位可以简化存储子系统</li>
<li>块非常适合用于数据备份进而提供数据容错能力</li>
</ol>
<p>在 Hadoop1 当中, 文件的 block 块默认大小是 64M, hadoop2 当中, 文件的 block 块大小默认是 128M, block 块的大小可以通过 hdfs-site.xml 当中的配置文件进行指定</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.block.size&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;块大小 以字节为单位&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="5-2-机架感知"><a href="#5-2-机架感知" class="headerlink" title="5.2 机架感知"></a>5.2 机架感知</h3><p>HDFS分布式文件系统的内部有一个副本存放策略：以默认的副本数=3为例：</p>
<p>1、第一个副本块存本机</p>
<p>2、第二个副本块存跟本机同机架内的其他服务器节点</p>
<p>3、第三个副本块存不同机架的一个服务器节点上</p>
<h2 id="6、hdfs的命令行使用"><a href="#6、hdfs的命令行使用" class="headerlink" title="6、hdfs的命令行使用"></a>6、hdfs的命令行使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">ls</span><br><span class="line">格式：  hdfs dfs -ls  URI</span><br><span class="line">作用：类似于Linux的ls命令，显示文件列表</span><br><span class="line">hdfs  dfs   -ls  /</span><br><span class="line">lsr</span><br><span class="line">格式  :   hdfs  dfs -lsr URI</span><br><span class="line">作用  : 在整个目录下递归执行ls, 与UNIX中的ls-R类似</span><br><span class="line">hdfs  dfs   -lsr  /</span><br><span class="line">mkdir</span><br><span class="line">格式 ： hdfs  dfs [-p] -mkdir &lt;paths&gt;</span><br><span class="line">作用 :   以&lt;paths&gt;中的URI作为参数，创建目录。使用-p参数可以递归创建目录</span><br><span class="line">put</span><br><span class="line">格式   ： hdfs dfs -put &lt;localsrc &gt;  ... &lt;dst&gt;</span><br><span class="line">作用 ：  将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标文件系统中（&lt;dst&gt;对应的路径）。也可以从标准输入中读取输入，写入目标文件系统中</span><br><span class="line">hdfs dfs -put  /rooot/a.txt  /dir1</span><br><span class="line">moveFromLocal</span><br><span class="line">格式： hdfs  dfs -moveFromLocal  &lt;localsrc&gt;   &lt;dst&gt;</span><br><span class="line">作用:   和put命令类似，但是源文件localsrc拷贝之后自身被删除</span><br><span class="line">hdfs  dfs -moveFromLocal  /root/install.log  /</span><br><span class="line">moveToLocal</span><br><span class="line">未实现</span><br><span class="line">get</span><br><span class="line">格式   hdfs dfs  -get [-ignorecrc ]  [-crc]  &lt;src&gt; &lt;localdst&gt;</span><br><span class="line"></span><br><span class="line">作用：将文件拷贝到本地文件系统。 CRC 校验失败的文件通过-ignorecrc选项拷贝。 文件和CRC校验和可以通过-CRC选项拷贝</span><br><span class="line">hdfs dfs  -get   /install.log  /export/servers</span><br><span class="line">mv</span><br><span class="line">格式  ： hdfs  dfs -mv URI   &lt;dest&gt;</span><br><span class="line">作用： 将hdfs上的文件从原路径移动到目标路径（移动之后文件删除），该命令不能夸文件系统</span><br><span class="line">hdfs  dfs  -mv  /dir1/a.txt   /dir2</span><br><span class="line">rm</span><br><span class="line">格式： hdfs dfs -rm [-r] 【-skipTrash】 URI 【URI 。。。】</span><br><span class="line">作用：   删除参数指定的文件，参数可以有多个。   此命令只删除文件和非空目录。</span><br><span class="line">如果指定-skipTrash选项，那么在回收站可用的情况下，该选项将跳过回收站而直接删除文件；</span><br><span class="line">否则，在回收站可用时，在HDFS Shell 中执行此命令，会将文件暂时放到回收站中。</span><br><span class="line">hdfs  dfs  -rm  -r  /dir1</span><br><span class="line">cp</span><br><span class="line">格式:     hdfs  dfs  -cp URI [URI ...] &lt;dest&gt;</span><br><span class="line">作用：    将文件拷贝到目标路径中。如果&lt;dest&gt;  为目录的话，可以将多个文件拷贝到该目录下。</span><br><span class="line">-f</span><br><span class="line">选项将覆盖目标，如果它已经存在。</span><br><span class="line">-p</span><br><span class="line">选项将保留文件属性（时间戳、所有权、许可、ACL、XAttr）。</span><br><span class="line">hdfs dfs -cp /dir1/a.txt  /dir2/b.txt</span><br><span class="line">cat</span><br><span class="line">hdfs dfs  -cat  URI [uri  ...]</span><br><span class="line">作用：将参数所指示的文件内容输出到stdout</span><br><span class="line">hdfs dfs  -cat /install.log</span><br><span class="line">chmod</span><br><span class="line">格式:      hdfs   dfs  -chmod  [-R]  URI[URI  ...]</span><br><span class="line">作用：    改变文件权限。如果使用  -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。</span><br><span class="line">hdfs dfs -chmod -R 777 /install.log</span><br><span class="line">chown</span><br><span class="line">格式:      hdfs   dfs  -chmod  [-R]  URI[URI  ...]</span><br><span class="line">作用：    改变文件的所属用户和用户组。如果使用  -R 选项，则对整个目录有效递归执行。使用这一命令的用户必须是文件的所属用户，或者超级用户。</span><br><span class="line">hdfs  dfs  -chown  -R hadoop:hadoop  /install.log</span><br><span class="line">appendToFile</span><br><span class="line">格式: hdfs dfs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt;</span><br><span class="line">作用: 追加一个或者多个文件到hdfs指定文件中.也可以从命令行读取输入.</span><br><span class="line"> hdfs dfs -appendToFile  a.xml b.xml  /big.xml</span><br></pre></td></tr></table></figure>

<h2 id="7、hdfs的高级使用命令"><a href="#7、hdfs的高级使用命令" class="headerlink" title="7、hdfs的高级使用命令"></a>7、hdfs的高级使用命令</h2><h3 id="7-1、HDFS文件限额配置"><a href="#7-1、HDFS文件限额配置" class="headerlink" title="7. 1、HDFS文件限额配置"></a>7. 1、HDFS文件限额配置</h3><p> 在多人共用HDFS的环境下，配置设置非常重要。特别是在Hadoop处理大量资料的环境，如果没有配额管理，很容易把所有的空间用完造成别人无法存取。Hdfs的配额设定是针对目录而不是针对账号，可以 让每个账号仅操作某一个目录，然后对目录设置配置。</p>
<p> hdfs文件的限额配置允许我们以文件个数，或者文件大小来限制我们在某个目录下上传的文件数量或者文件内容总量，以便达到我们类似百度网盘网盘等限制每个用户允许上传的最大的文件的量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -count -q -h /user/root/dir1  #查看配额信息</span><br></pre></td></tr></table></figure>

<p>所谓的空间限额</p>
<h4 id="7-1-1、数量限额"><a href="#7-1-1、数量限额" class="headerlink" title="7.1.1、数量限额"></a>7.1.1、数量限额</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs  -mkdir -p /user/root/dir    #创建hdfs文件夹</span><br><span class="line">hdfs dfsadmin -setQuota 2  dir      # 给该文件夹下面设置最多上传两个文件，发现只能上传一个文件</span><br><span class="line">hdfs dfsadmin -clrQuota /user/root/dir  # 清除文件数量限制</span><br></pre></td></tr></table></figure>

<h4 id="7-1-2、空间大小限额"><a href="#7-1-2、空间大小限额" class="headerlink" title="7.1.2、空间大小限额"></a>7.1.2、空间<strong>大小限额</strong></h4><p>在设置空间配额时，设置的空间至少是block_size * 3大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -setSpaceQuota 4k /user/root/dir   # 限制空间大小4KB</span><br><span class="line">hdfs dfs -put  /root/a.txt  /user/root/dir</span><br></pre></td></tr></table></figure>

<p>生成任意大小文件的命令:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dd if=/dev/zero of=1.txt  bs=1M count=2     #生成2M的文件</span><br></pre></td></tr></table></figure>

<p>清除空间配额限制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -clrSpaceQuota /user/root/dir</span><br></pre></td></tr></table></figure>

<h3 id="7-2、hdfs的安全模式"><a href="#7-2、hdfs的安全模式" class="headerlink" title="7.2、hdfs的安全模式"></a>7.2、hdfs的安全模式</h3><p>安全模式是hadoop的一种<a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">保护机制</a>，用于保证集群中的数据块的安全性。当集群启动的时候，会首先进入安全模式。当系统处于安全模式时会检查数据块的完整性。</p>
<p>假设我们设置的副本数（即参数dfs.replication）是3，那么在datanode上就应该有3个副本存在，假设只存在2个副本，那么比例就是2/3=0.666。hdfs默认的副本率0.999。我们的副本率0.666明显小于0.999，因此系统会自动的复制副本到其他dataNode，使得副本率不小于0.999。如果系统中有5个副本，超过我们设定的3个副本，那么系统也会删除多于的2个副本。</p>
<p><a href="https://manzhong.github.io/2017/07/05/Hdfs/" target="_blank" rel="noopener">在安全模式状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求</a>。在，当整个系统达到安全标准时，HDFS自动离开安全模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">安全模式操作命令</span><br><span class="line">hdfs  dfsadmin  -safemode  get #查看安全模式状态</span><br><span class="line">hdfs  dfsadmin  -safemode  enter #进入安全模式</span><br><span class="line">hdfs  dfsadmin  -safemode  leave #离开安全模式</span><br></pre></td></tr></table></figure>

<h2 id="8-HDFS基准测试"><a href="#8-HDFS基准测试" class="headerlink" title="8. HDFS基准测试"></a>8. HDFS基准测试</h2><p>实际生产环境当中，hadoop的环境搭建完成之后，第一件事情就是进行压力测试，测试我们的集群的读取和写入速度，测试我们的网络带宽是否足够等一些基准测试</p>
<h3 id="8-1-测试写入速度"><a href="#8-1-测试写入速度" class="headerlink" title="8.1 测试写入速度"></a>8.1 测试写入速度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">向HDFS文件系统中写入数据,10个文件,每个文件10MB,文件存放到/benchmarks/TestDFSIO中</span><br><span class="line">hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar  TestDFSIO -write -nrFiles 10  -fileSize 10MB</span><br></pre></td></tr></table></figure>

<p>完成之后查看写入速度结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -text  /benchmarks/TestDFSIO/io_write/part-00000</span><br></pre></td></tr></table></figure>

<h3 id="8-2-测试读取速度"><a href="#8-2-测试读取速度" class="headerlink" title="8.2 测试读取速度"></a>8.2 测试读取速度</h3><p>测试hdfs的读取文件性能</p>
<p>在HDFS文件系统中读入10个文件,每个文件10M</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar  TestDFSIO -read -nrFiles 10 -fileSize 10MB</span><br></pre></td></tr></table></figure>

<p>查看读取果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -text /benchmarks/TestDFSIO/io_read/part-00000</span><br></pre></td></tr></table></figure>

<h3 id="8-3-清除测试数据"><a href="#8-3-清除测试数据" class="headerlink" title="8.3 清除测试数据"></a>8.3 清除测试数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /export/servers/hadoop-2.7.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.5.jar   TestDFSIO -clean</span><br></pre></td></tr></table></figure>

<h2 id="9-HDFS-文件写入过程"><a href="#9-HDFS-文件写入过程" class="headerlink" title="9.HDFS 文件写入过程"></a>9.HDFS 文件写入过程</h2><p><a href="https://manzhong.github.io/images/hdfs/w.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/w.bmp" alt="img"></a></p>
<ol>
<li>Client 发起文件上传请求, 通过 RPC 与 NameNode 建立通讯, NameNode 检查目标文件是否已存在, 父目录是否存在, 返回是否可以上传</li>
<li>Client 请求第一个 block 该传输到哪些 DataNode 服务器上</li>
<li>NameNode 根据配置文件中指定的备份数量及机架感知原理进行文件分配, 返回可用的 DataNode 的地址如: A, B, C<ul>
<li>Hadoop 在设计时考虑到数据的安全与高效, 数据文件默认在 HDFS 上存放三份, 存储策略为本地一份, 同机架内其它某一节点上一份, 不同机架的某一节点上一份。</li>
</ul>
</li>
<li>Client 请求 3 台 DataNode 中的一台 A 上传数据（本质上是一个 RPC 调用，建立 pipeline ）, A 收到请求会继续调用 B, 然后 B 调用 C, 将整个 pipeline 建立完成, 后逐级返回 client</li>
<li>Client 开始往 A 上传第一个 block（先从磁盘读取数据放到一个本地内存缓存）, 以 packet 为单位（默认64K）, A 收到一个 packet 就会传给 B, B 传给 C. A 每传一个 packet 会放入一个应答队列等待应答</li>
<li>数据被分割成一个个 packet 数据包在 pipeline 上依次传输, 在 pipeline 反方向上, 逐个发送 ack（命令正确应答）, 最终由 pipeline 中第一个 DataNode 节点 A 将 pipelineack 发送给 Client</li>
<li>当一个 block 传输完成之后, Client 再次请求 NameNode 上传第二个 block 到服务 1</li>
</ol>
<h2 id="10-HDFS-文件读取过程"><a href="#10-HDFS-文件读取过程" class="headerlink" title="10.HDFS 文件读取过程"></a>10.HDFS 文件读取过程</h2><p><a href="https://manzhong.github.io/images/hdfs/r.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/r.bmp" alt="img"></a></p>
<ol>
<li>Client向NameNode发起RPC请求，来确定请求文件block所在的位置；</li>
<li>NameNode会视情况返回文件的部分或者全部block列表，对于每个block，NameNode 都会返回含有该 block 副本的 DataNode 地址； 这些返回的 DN 地址，会按照集群拓扑结构得出 DataNode 与客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离 Client 近的排靠前；心跳机制中超时汇报的 DN 状态为 STALE，这样的排靠后；</li>
<li>Client 选取排序靠前的 DataNode 来读取 block，如果客户端本身就是DataNode,那么将从本地直接获取数据(短路读取特性)；</li>
<li>底层上本质是建立 Socket Stream（FSDataInputStream），重复的调用父类 DataInputStream 的 read 方法，直到这个块上的数据读取完毕；</li>
<li>当读完列表的 block 后，若文件读取还没有结束，客户端会继续向NameNode 获取下一批的 block 列表；</li>
<li>读取完一个 block 都会进行 checksum 验证，如果读取 DataNode 时出现错误，客户端会通知 NameNode，然后再从下一个拥有该 block 副本的DataNode 继续读。</li>
<li>read 方法是并行的读取 block 信息，不是一块一块的读取；NameNode 只是返回Client请求包含块的DataNode地址，并不是返回请求块的数据；</li>
<li>最终读取来所有的 block 会合并成一个完整的最终文件。</li>
</ol>
<h2 id="11-HDFS-的元数据辅助管理"><a href="#11-HDFS-的元数据辅助管理" class="headerlink" title="11.HDFS 的元数据辅助管理"></a>11.HDFS 的元数据辅助管理</h2><p>当 Hadoop 的集群当中, NameNode的所有元数据信息都保存在了 FsImage 与 Eidts 文件当中, 这两个文件就记录了所有的数据的元数据信息, 元数据信息的保存目录配置在了 <code>hdfs-site.xml</code> 当中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;    </span><br><span class="line">    &lt;value&gt;</span><br><span class="line">        file:///export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas,          	        		file:///export/servers/hadoop-2.7.5/hadoopDatas/namenodeDatas2</span><br><span class="line">    &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;file:///export/servers/hadoop-2.7.5/hadoopDatas/nn/edits&lt;/value</span><br><span class="line">&lt;/property&gt;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="11-1-FsImage-和-Edits-详解"><a href="#11-1-FsImage-和-Edits-详解" class="headerlink" title="11.1 FsImage 和 Edits 详解"></a>11.1 FsImage 和 Edits 详解</h4><ul>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edits</span><br></pre></td></tr></table></figure>

<ul>
<li><code>edits</code> 存放了客户端最近一段时间的操作日志</li>
<li>客户端对 HDFS 进行写文件时会首先被记录在 <code>edits</code> 文件中</li>
<li><code>edits</code> 修改时元数据也会更新</li>
</ul>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fsimage</span><br></pre></td></tr></table></figure>

<ul>
<li>NameNode 中关于元数据的镜像, 一般称为检查点, <code>fsimage</code> 存放了一份比较完整的元数据信息</li>
<li>因为 <code>fsimage</code> 是 NameNode 的完整的镜像, 如果每次都加载到内存生成树状拓扑结构，这是非常耗内存和CPU, 所以一般开始时对 NameNode 的操作都放在 edits 中</li>
<li><code>fsimage</code> 内容包含了 NameNode 管理下的所有 DataNode 文件及文件 block 及 block 所在的 DataNode 的元数据信息.</li>
<li>随着 <code>edits</code> 内容增大, 就需要在一定时间点和 <code>fsimage</code> 合并</li>
</ul>
</li>
</ul>
<h4 id="11-2-fsimage-中的文件信息查看"><a href="#11-2-fsimage-中的文件信息查看" class="headerlink" title="11.2 fsimage 中的文件信息查看"></a>11.2 fsimage 中的文件信息查看</h4><p>使用命令 <code>hdfs oiv</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas</span><br><span class="line">hdfs oiv -i fsimage_0000000000000000864 -p XML -o hello.xml</span><br></pre></td></tr></table></figure>

<h4 id="11-3-edits-中的文件信息查看"><a href="#11-3-edits-中的文件信息查看" class="headerlink" title="11.3. edits 中的文件信息查看"></a>11.3. edits 中的文件信息查看</h4><p>使用命令 <code>hdfs oev</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop2.7.5/hadoopDatas/namenodeDatas</span><br><span class="line">hdfs oev -i  edits_0000000000000000865-0000000000000000866 -p XML -o myedit.xml</span><br></pre></td></tr></table></figure>

<h4 id="11-4-SecondaryNameNode-如何辅助管理-fsimage-与-edits-文件"><a href="#11-4-SecondaryNameNode-如何辅助管理-fsimage-与-edits-文件" class="headerlink" title="11.4 SecondaryNameNode 如何辅助管理 fsimage 与 edits 文件?"></a>11.4 SecondaryNameNode 如何辅助管理 fsimage 与 edits 文件?</h4><ul>
<li><p>SecondaryNameNode 定期合并 fsimage 和 edits, 把 edits 控制在一个范围内</p>
</li>
<li><p>配置 SecondaryNameNode</p>
<ul>
<li><p>SecondaryNameNode 在 <code>conf/masters</code> 中指定</p>
</li>
<li><p>在 masters 指定的机器上, 修改 <code>hdfs-site.xml</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.http.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;host:50070&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改 <code>core-site.xml</code>, 这一步不做配置保持默认也可以</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 多久记录一次 HDFS 镜像, 默认 1小时 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.checkpoint.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3600&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 一次记录多大, 默认 64M --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.checkpoint.size&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;67108864&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p><a href="https://manzhong.github.io/images/hdfs/s.bmp" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/s.bmp" alt="img"></a></p>
<ol>
<li>SecondaryNameNode 通知 NameNode 切换 editlog</li>
<li>SecondaryNameNode 从 NameNode 中获得 fsimage 和 editlog(通过http方式)</li>
<li>SecondaryNameNode 将 fsimage 载入内存, 然后开始合并 editlog, 合并之后成为新的 fsimage</li>
<li>SecondaryNameNode 将新的 fsimage 发回给 NameNode</li>
<li>NameNode 用新的 fsimage 替换旧的 fsimage</li>
</ol>
<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><ul>
<li>完成合并的是 SecondaryNameNode, 会请求 NameNode 停止使用 edits, 暂时将新写操作放入一个新的文件中 <code>edits.new</code></li>
<li>SecondaryNameNode 从 NameNode 中通过 Http GET 获得 edits, 因为要和 fsimage 合并, 所以也是通过 Http Get 的方式把 fsimage 加载到内存, 然后逐一执行具体对文件系统的操作, 与 fsimage 合并, 生成新的 fsimage, 然后通过 Http POST 的方式把 fsimage 发送给 NameNode. NameNode 从 SecondaryNameNode 获得了 fsimage 后会把原有的 fsimage 替换为新的 fsimage, 把 edits.new 变成 edits. 同时会更新 fstime</li>
<li>Hadoop 进入安全模式时需要管理员使用 dfsadmin 的 save namespace 来创建新的检查点</li>
<li>SecondaryNameNode 在合并 edits 和 fsimage 时需要消耗的内存和 NameNode 差不多, 所以一般把 NameNode 和 SecondaryNameNode 放在不同的机器上</li>
</ul>
<h2 id="1-HDFS-的-API-操作"><a href="#1-HDFS-的-API-操作" class="headerlink" title="1:HDFS 的 API 操作"></a>1:HDFS 的 API 操作</h2><h3 id="1-1-配置Windows下Hadoop环境"><a href="#1-1-配置Windows下Hadoop环境" class="headerlink" title="1.1 配置Windows下Hadoop环境"></a>1.1 配置Windows下Hadoop环境</h3><p>在windows系统需要配置hadoop运行环境，否则直接运行代码会出现以下问题:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">缺少winutils.exe</span><br><span class="line">Could not locate executable null \bin\winutils.exe in the hadoop binaries</span><br><span class="line">缺少hadoop.dll</span><br><span class="line">Unable to load native-hadoop library for your platform… using builtin-Java classes where applicable</span><br></pre></td></tr></table></figure>

<p>步骤:</p>
<p>第一步：将hadoop2.7.5文件夹拷贝到一个没有中文没有空格的路径下面</p>
<p>第二步：在windows上面配置hadoop的环境变量： HADOOP_HOME，并将%HADOOP_HOME%\bin添加到path中</p>
<p>第三步：把hadoop2.7.5文件夹中bin目录下的hadoop.dll文件放到系统盘: C:\Windows\System32 目录</p>
<p>第四步：关闭windows重启</p>
<h3 id="1-2-导入-Maven-依赖"><a href="#1-2-导入-Maven-依赖" class="headerlink" title="1.2 导入 Maven 依赖"></a>1.2 导入 Maven 依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">	 &lt;dependencies&gt;</span><br><span class="line">    	&lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">   &lt;/dependency&gt;</span><br><span class="line">      &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">      &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">      &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;RELEASE&lt;/version&gt;</span><br><span class="line">      &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line">  &lt;build&gt;</span><br><span class="line">      &lt;plugins&gt;</span><br><span class="line">          &lt;plugin&gt;</span><br><span class="line">              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">              &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">              &lt;version&gt;3.1&lt;/version&gt;</span><br><span class="line">              &lt;configuration&gt;</span><br><span class="line">                  &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                  &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                  &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">                  &lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span><br><span class="line">              &lt;/configuration&gt;</span><br><span class="line">          &lt;/plugin&gt;</span><br><span class="line">          &lt;plugin&gt;</span><br><span class="line">              &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">              &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">              &lt;version&gt;2.4.3&lt;/version&gt;</span><br><span class="line">              &lt;executions&gt;</span><br><span class="line">                  &lt;execution&gt;</span><br><span class="line">                      &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                      &lt;goals&gt;</span><br><span class="line">                          &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                      &lt;/goals&gt;</span><br><span class="line">                      &lt;configuration&gt;</span><br><span class="line">                          &lt;minimizeJar&gt;true&lt;/minimizeJar&gt;</span><br><span class="line">                      &lt;/configuration&gt;</span><br><span class="line">                  &lt;/execution&gt;</span><br><span class="line">              &lt;/executions&gt;</span><br><span class="line">          &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">      &lt;/plugins&gt;</span><br><span class="line">  &lt;/build&gt;</span><br></pre></td></tr></table></figure>

<h3 id="1-3-使用url方式访问数据（了解）"><a href="#1-3-使用url方式访问数据（了解）" class="headerlink" title="1.3 使用url方式访问数据（了解）"></a>1.3 使用url方式访问数据（了解）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void demo1()throws  Exception&#123;</span><br><span class="line">    //第一步：注册hdfs 的url</span><br><span class="line">    URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());</span><br><span class="line"></span><br><span class="line">    //获取文件输入流</span><br><span class="line">    InputStream inputStream  = new URL(&quot;hdfs://node01:8020/a.txt&quot;).openStream();</span><br><span class="line">    //获取文件输出流</span><br><span class="line">    FileOutputStream outputStream = new FileOutputStream(new File(&quot;D:\\hello.txt&quot;));</span><br><span class="line"></span><br><span class="line">    //实现文件的拷贝</span><br><span class="line">    IOUtils.copy(inputStream, outputStream);</span><br><span class="line"></span><br><span class="line">    //关闭流</span><br><span class="line">    IOUtils.closeQuietly(inputStream);</span><br><span class="line">    IOUtils.closeQuietly(outputStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="1-4-使用文件系统方式访问数据（掌握）"><a href="#1-4-使用文件系统方式访问数据（掌握）" class="headerlink" title="1.4 使用文件系统方式访问数据（掌握）"></a>1.4 <strong>使用</strong>文件系统方式访问数据（掌握）</h3><h4 id="1-4-1-涉及的主要类"><a href="#1-4-1-涉及的主要类" class="headerlink" title="1.4.1 涉及的主要类"></a>1.4.1 涉及的主要类</h4><p>在 Java 中操作 HDFS, 主要涉及以下 Class:</p>
<ul>
<li><p><code>Configuration</code></p>
<ul>
<li>该类的对象封转了客户端或者服务器的配置</li>
</ul>
</li>
<li><p><code>FileSystem</code></p>
<ul>
<li><p>该类的对象是一个文件系统对象, 可以用该对象的一些方法来对文件进行操作, 通过 FileSystem 的静态方法 get 获得该对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FileSystem fs = FileSystem.get(conf)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>get</code> 方法从 <code>conf</code> 中的一个参数 <code>fs.defaultFS</code> 的配置值判断具体是什么类型的文件系统</li>
<li>如果我们的代码中没有指定 <code>fs.defaultFS</code>, 并且工程 ClassPath 下也没有给定相应的配置, <code>conf</code> 中的默认值就来自于 Hadoop 的 Jar 包中的 <code>core-default.xml</code></li>
<li>默认值为 <code>file:///</code>, 则获取的不是一个 DistributedFileSystem 的实例, 而是一个本地文件系统的客户端对象</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-4-2-获取-FileSystem-的几种方式"><a href="#1-4-2-获取-FileSystem-的几种方式" class="headerlink" title="1.4.2 获取 FileSystem 的几种方式"></a>1.4.2 获取 FileSystem 的几种方式</h4><ul>
<li>第一种方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileSystem1() throws IOException &#123;</span><br><span class="line">    Configuration configuration = new Configuration();</span><br><span class="line">    //指定我们使用的文件系统类型:</span><br><span class="line">    configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://node01:8020/&quot;);</span><br><span class="line"></span><br><span class="line">    //获取指定的文件系统</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(configuration);</span><br><span class="line">    System.out.println(fileSystem.toString());</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第二种方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileSystem2() throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    System.out.println(&quot;fileSystem:&quot;+fileSystem);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第三种方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileSystem3() throws  Exception&#123;</span><br><span class="line">    Configuration configuration = new Configuration();</span><br><span class="line">    configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://node01:8020&quot;);</span><br><span class="line">    FileSystem fileSystem = FileSystem.newInstance(configuration);</span><br><span class="line">    System.out.println(fileSystem.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第四种方式</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//@Test</span><br><span class="line">public void getFileSystem4() throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.newInstance(new URI(&quot;hdfs://node01:8020&quot;) ,new Configuration());</span><br><span class="line">    System.out.println(fileSystem.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-3-遍历-HDFS-中所有文件"><a href="#1-4-3-遍历-HDFS-中所有文件" class="headerlink" title="1.4.3 遍历 HDFS 中所有文件"></a>1.4.3 遍历 HDFS 中所有文件</h4><ul>
<li>使用 API 遍历</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void listMyFiles()throws Exception&#123;</span><br><span class="line">    //获取fileSystem类</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    //获取RemoteIterator 得到所有的文件或者文件夹，第一个参数指定遍历的路径，第二个参数表示是否要递归遍历</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; locatedFileStatusRemoteIterator = fileSystem.listFiles(new Path(&quot;/&quot;), true);</span><br><span class="line">    while (locatedFileStatusRemoteIterator.hasNext())&#123;</span><br><span class="line">        LocatedFileStatus next = locatedFileStatusRemoteIterator.next();</span><br><span class="line">        System.out.println(next.getPath().toString());</span><br><span class="line">    &#125;</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-4-HDFS-上创建文件夹"><a href="#1-4-4-HDFS-上创建文件夹" class="headerlink" title="1.4.4 HDFS 上创建文件夹"></a>1.4.4 HDFS 上创建文件夹</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void mkdirs() throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    boolean mkdirs = fileSystem.mkdirs(new Path(&quot;/hello/mydir/test&quot;));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-4-下载文件"><a href="#1-4-4-下载文件" class="headerlink" title="1.4.4 下载文件"></a>1.4.4 下载文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileToLocal()throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(new Path(&quot;/timer.txt&quot;));</span><br><span class="line">    FileOutputStream  outputStream = new FileOutputStream(new File(&quot;e:\\timer.txt&quot;));</span><br><span class="line">    IOUtils.copy(inputStream,outputStream );</span><br><span class="line">    IOUtils.closeQuietly(inputStream);</span><br><span class="line">    IOUtils.closeQuietly(outputStream);</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-5-HDFS-文件上传"><a href="#1-4-5-HDFS-文件上传" class="headerlink" title="1.4.5 HDFS 文件上传"></a>1.4.5 HDFS 文件上传</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void putData() throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration());</span><br><span class="line">    fileSystem.copyFromLocalFile(new Path(&quot;file:///c:\\install.log&quot;),new Path(&quot;/hello/mydir/test&quot;));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-6-hdfs访问权限控制"><a href="#1-4-6-hdfs访问权限控制" class="headerlink" title="1.4.6 hdfs访问权限控制"></a>1.4.6 hdfs访问权限控制</h4><ol>
<li>停止hdfs集群，在node01机器上执行以下命令</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5</span><br><span class="line">sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>

<ol>
<li>修改node01机器上的hdfs-site.xml当中的配置文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5/etc/hadoop</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.permissions.enabled&lt;/name&gt;</span><br><span class="line">	&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol>
<li>修改完成之后配置文件发送到其他机器上面去</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp hdfs-site.xml node02:$PWD</span><br><span class="line">scp hdfs-site.xml node03:$PWD</span><br></pre></td></tr></table></figure>

<ol>
<li>重启hdfs集群</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5</span><br><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>

<ol>
<li>随意上传一些文件到我们hadoop集群当中准备测试使用</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hadoop-2.7.5/etc/hadoop</span><br><span class="line">hdfs dfs -mkdir /config</span><br><span class="line">hdfs dfs -put *.xml /config</span><br><span class="line">hdfs dfs -chmod 600 /config/core-site.xml</span><br></pre></td></tr></table></figure>

<ol>
<li>使用代码准备下载文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getConfig()throws  Exception&#123;</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://node01:8020&quot;), new Configuration(),&quot;hadoop&quot;);</span><br><span class="line">    fileSystem.copyToLocalFile(new Path(&quot;/config/core-site.xml&quot;),new Path(&quot;file:///c:/core-site.xml&quot;));</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-7-小文件合并"><a href="#1-4-7-小文件合并" class="headerlink" title="1.4.7 小文件合并"></a>1.4.7 小文件合并</h4><p>由于 Hadoop 擅长存储大文件，因为大文件的元数据信息比较少，如果 Hadoop 集群当中有大量的小文件，那么每个小文件都需要维护一份元数据信息，会大大的增加集群管理元数据的内存压力，所以在实际工作当中，如果有必要一定要将小文件合并成大文件进行一起处理</p>
<p>在我们的 HDFS 的 Shell 命令模式下，可以通过命令行将很多的 hdfs 文件合并成一个大文件下载到本地</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">hdfs dfs -getmerge /config/*.xml ./hello.xml</span><br></pre></td></tr></table></figure>

<p>既然可以在下载的时候将这些小文件合并成一个大文件一起下载，那么肯定就可以在上传的时候将小文件合并到一个大文件里面去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void mergeFile() throws  Exception&#123;</span><br><span class="line">    //获取分布式文件系统</span><br><span class="line">    FileSystem fileSystem = FileSystem.get(new URI(&quot;hdfs://192.168.52.250:8020&quot;), new Configuration(),&quot;root&quot;);</span><br><span class="line">    FSDataOutputStream outputStream = fileSystem.create(new Path(&quot;/bigfile.txt&quot;));</span><br><span class="line">    //获取本地文件系统</span><br><span class="line">    LocalFileSystem local = FileSystem.getLocal(new Configuration());</span><br><span class="line">    //通过本地文件系统获取文件列表，为一个集合</span><br><span class="line">    FileStatus[] fileStatuses = local.listStatus(new Path(&quot;file:///E:\\input&quot;));</span><br><span class="line">    for (FileStatus fileStatus : fileStatuses) &#123;</span><br><span class="line">        FSDataInputStream inputStream = local.open(fileStatus.getPath());</span><br><span class="line">       IOUtils.copy(inputStream,outputStream);</span><br><span class="line">        IOUtils.closeQuietly(inputStream);</span><br><span class="line">    &#125;</span><br><span class="line">    IOUtils.closeQuietly(outputStream);</span><br><span class="line">    local.close();</span><br><span class="line">    fileSystem.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="2：HDFS的高可用机制"><a href="#2：HDFS的高可用机制" class="headerlink" title="2：HDFS的高可用机制"></a>2：HDFS的高可用机制</h2><h3 id="2-1-HDFS高可用介绍"><a href="#2-1-HDFS高可用介绍" class="headerlink" title="2.1 HDFS高可用介绍"></a>2.1 HDFS高可用介绍</h3><p>在Hadoop 中，NameNode 所处的位置是非常重要的，整个HDFS文件系统的元数据信息都由NameNode 来管理，NameNode的可用性直接决定了Hadoop 的可用性，一旦NameNode进程不能工作了，就会影响整个集群的正常使用。</p>
<p>在典型的HA集群中，两台独立的机器被配置为NameNode。在工作集群中，NameNode机器中的一个处于Active状态，另一个处于Standby状态。Active NameNode负责群集中的所有客户端操作，而Standby充当从服务器。Standby机器保持足够的状态以提供快速故障切换（如果需要）。</p>
<h3 id="2-2-组件介绍"><a href="#2-2-组件介绍" class="headerlink" title="2.2 组件介绍"></a>2.2 组件介绍</h3><p><a href="https://manzhong.github.io/images/hdfs/ha.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/ha.jpg" alt="img"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ZKFailoverController</span><br></pre></td></tr></table></figure>

<p>是基于Zookeeper的故障转移控制器，它负责控制NameNode的主备切换，ZKFailoverController会监测NameNode的健康状态，当发现Active NameNode出现异常时会通过Zookeeper进行一次新的选举，完成Active和Standby状态的切换</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HealthMonitor</span><br></pre></td></tr></table></figure>

<p>周期性调用NameNode的HAServiceProtocol RPC接口（monitorHealth 和 getServiceStatus），监控NameNode的健康状态并向ZKFailoverController反馈</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ActiveStandbyElector</span><br></pre></td></tr></table></figure>

<p>接收ZKFC的选举请求，通过Zookeeper自动完成主备选举，选举完成后回调ZKFailoverController的主备切换方法对NameNode进行Active和Standby状态的切换.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataNode</span><br></pre></td></tr></table></figure>

<p>NameNode包含了HDFS的元数据信息和数据块信息（blockmap），其中数据块信息通过DataNode主动向Active NameNode和Standby NameNode上报</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">共享存储系统</span><br></pre></td></tr></table></figure>

<p>共享存储系统负责存储HDFS的元数据（EditsLog），Active NameNode（写入）和 Standby NameNode（读取）通过共享存储系统实现元数据同步，在主备切换过程中，新的Active NameNode必须确保元数据同步完成才能对外提供服务</p>
<h2 id="3-Hadoop的联邦机制-Federation"><a href="#3-Hadoop的联邦机制-Federation" class="headerlink" title="3: Hadoop的联邦机制(Federation)"></a>3: Hadoop的联邦机制(Federation)</h2><p><a href="https://manzhong.github.io/images/hdfs/lb.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/hdfs/lb.jpg" alt="img"></a></p>
<h3 id="3-1背景概述"><a href="#3-1背景概述" class="headerlink" title="3.1背景概述"></a>3.1<strong>背景概述</strong></h3><p>单NameNode的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NameNode进程使用的内存可能会达到上百G，NameNode成为了性能的瓶颈。因而提出了namenode水平扩展方案– Federation。</p>
<p>Federation中文意思为联邦,联盟，是NameNode的Federation,也就是会有多个NameNode。多个NameNode的情况意味着有多个namespace(命名空间)，区别于HA模式下的多NameNode，它们是拥有着同一个namespace。既然说到了NameNode的命名空间的概念,</p>
<p>我们可以很明显地看出现有的HDFS数据管理,数据存储2层分层的结构.也就是说,所有关于存储数据的信息和管理是放在NameNode这边,而真实数据的存储则是在各个DataNode下.而这些隶属于同一个NameNode所管理的数据都是在同一个命名空间下的.而一个namespace对应一个block pool。Block Pool是同一个namespace下的block的集合.当然这是我们最常见的单个namespace的情况,也就是一个NameNode管理集群中所有元数据信息的时候.如果我们遇到了之前提到的NameNode内存使用过高的问题,这时候怎么办?元数据空间依然还是在不断增大,一味调高NameNode的jvm大小绝对不是一个持久的办法.这时候就诞生了HDFS Federation的机制.</p>
<h3 id="3-2-Federation架构设计"><a href="#3-2-Federation架构设计" class="headerlink" title="3.2 Federation架构设计"></a>3.2 <strong>Federation架构设计</strong></h3><p>HDFS Federation是解决namenode内存瓶颈问题的水平横向扩展方案。</p>
<p>Federation意味着在集群中将会有多个namenode/namespace。这些namenode之间是联合的，也就是说，他们之间相互独立且不需要互相协调，各自分工，管理自己的区域。分布式的datanode被用作通用的数据块存储存储设备。每个datanode要向集群中所有的namenode注册，且周期性地向所有namenode发送心跳和块报告，并执行来自所有namenode的命令。</p>
<p>Federation一个典型的例子就是上面提到的NameNode内存过高问题,我们完全可以将上面部分大的文件目录移到另外一个NameNode上做管理.<strong>更重要的一点在于,这些NameNode是共享集群中所有的DataNode的,它们还是在同一个集群内的**</strong>。**</p>
<p>这时候在DataNode上就不仅仅存储一个Block Pool下的数据了,而是多个(在DataNode的datadir所在目录里面查看BP-xx.xx.xx.xx打头的目录)。</p>
<p><strong>概括起来：</strong></p>
<p>多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务。</p>
<p>每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储。</p>
<p>DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况。</p>
<p><strong>HDFS Federation不足</strong></p>
<p>HDFS Federation并没有完全解决单点故障问题。虽然namenode/namespace存在多个，但是从单个namenode/namespace看，仍然存在单点故障：如果某个namenode挂掉了，其管理的相应的文件便不可以访问。Federation中每个namenode仍然像之前HDFS上实现一样，配有一个secondary namenode，以便主namenode挂掉一下，用于还原元数据信息。</p>
<p>所以一般集群规模真的很大的时候，会采用HA+Federation的部署方案。也就是每个联合的namenodes都是ha的。</p>
<p>总结:</p>
<p>主备:解决了单点故障,形成高可用.</p>
<p>联邦: 解决了namenode的内存问题.</p>
<p>namenode的可用性决定了Hadoop的可用性</p>
</div><div class="post-copyright"><blockquote><p>Autor original: hechao</p><p>Enlace original: <a href="http://yoursite.com/2019/08/08/Hdfs/">http://yoursite.com/2019/08/08/Hdfs/</a></p><p>Declaración de copyright: Indique la fuente de la reimpresión (debe conservar la firma del autor y el enlace)</p></blockquote></div><div class="tags"></div><div class="post-share"><div class="social-share"><span>Cuota:</span></div></div><div class="post-nav"><a href="/2019/08/08/Zookeeper/" class="pre">Zookeeper</a><a href="/2019/08/08/Hadoop/" class="next">Hadoop</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">Contenidos</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop的核心-Hdfs"><span class="toc-text">Hadoop的核心 Hdfs</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-HDFS概述"><span class="toc-text">1. HDFS概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-介绍"><span class="toc-text">1.1 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-历史"><span class="toc-text">1.2 历史</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-HDFS应用场景"><span class="toc-text">2. HDFS应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-适合的应用场景"><span class="toc-text">2.1 适合的应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-不适合的应用场景"><span class="toc-text">2.2 不适合的应用场景</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-HDFS-的架构"><span class="toc-text">3. HDFS 的架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-NameNode和DataNode"><span class="toc-text">4:NameNode和DataNode</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-DataNode作用"><span class="toc-text">4.2 DataNode作用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-HDFS的副本机制和机架感知"><span class="toc-text">5:HDFS的副本机制和机架感知</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-HDFS-文件副本机制"><span class="toc-text">5.1 HDFS 文件副本机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-机架感知"><span class="toc-text">5.2 机架感知</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6、hdfs的命令行使用"><span class="toc-text">6、hdfs的命令行使用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7、hdfs的高级使用命令"><span class="toc-text">7、hdfs的高级使用命令</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1、HDFS文件限额配置"><span class="toc-text">7. 1、HDFS文件限额配置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#7-1-1、数量限额"><span class="toc-text">7.1.1、数量限额</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-1-2、空间大小限额"><span class="toc-text">7.1.2、空间大小限额</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2、hdfs的安全模式"><span class="toc-text">7.2、hdfs的安全模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-HDFS基准测试"><span class="toc-text">8. HDFS基准测试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-测试写入速度"><span class="toc-text">8.1 测试写入速度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-测试读取速度"><span class="toc-text">8.2 测试读取速度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-清除测试数据"><span class="toc-text">8.3 清除测试数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-HDFS-文件写入过程"><span class="toc-text">9.HDFS 文件写入过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-HDFS-文件读取过程"><span class="toc-text">10.HDFS 文件读取过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-HDFS-的元数据辅助管理"><span class="toc-text">11.HDFS 的元数据辅助管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#11-1-FsImage-和-Edits-详解"><span class="toc-text">11.1 FsImage 和 Edits 详解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-2-fsimage-中的文件信息查看"><span class="toc-text">11.2 fsimage 中的文件信息查看</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-3-edits-中的文件信息查看"><span class="toc-text">11.3. edits 中的文件信息查看</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-4-SecondaryNameNode-如何辅助管理-fsimage-与-edits-文件"><span class="toc-text">11.4 SecondaryNameNode 如何辅助管理 fsimage 与 edits 文件?</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#特点"><span class="toc-text">特点</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-HDFS-的-API-操作"><span class="toc-text">1:HDFS 的 API 操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-配置Windows下Hadoop环境"><span class="toc-text">1.1 配置Windows下Hadoop环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-导入-Maven-依赖"><span class="toc-text">1.2 导入 Maven 依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-使用url方式访问数据（了解）"><span class="toc-text">1.3 使用url方式访问数据（了解）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-使用文件系统方式访问数据（掌握）"><span class="toc-text">1.4 使用文件系统方式访问数据（掌握）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-1-涉及的主要类"><span class="toc-text">1.4.1 涉及的主要类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-2-获取-FileSystem-的几种方式"><span class="toc-text">1.4.2 获取 FileSystem 的几种方式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-3-遍历-HDFS-中所有文件"><span class="toc-text">1.4.3 遍历 HDFS 中所有文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-4-HDFS-上创建文件夹"><span class="toc-text">1.4.4 HDFS 上创建文件夹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-4-下载文件"><span class="toc-text">1.4.4 下载文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-5-HDFS-文件上传"><span class="toc-text">1.4.5 HDFS 文件上传</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-6-hdfs访问权限控制"><span class="toc-text">1.4.6 hdfs访问权限控制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-7-小文件合并"><span class="toc-text">1.4.7 小文件合并</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2：HDFS的高可用机制"><span class="toc-text">2：HDFS的高可用机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-HDFS高可用介绍"><span class="toc-text">2.1 HDFS高可用介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-组件介绍"><span class="toc-text">2.2 组件介绍</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Hadoop的联邦机制-Federation"><span class="toc-text">3: Hadoop的联邦机制(Federation)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1背景概述"><span class="toc-text">3.1背景概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Federation架构设计"><span class="toc-text">3.2 Federation架构设计</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> Recientes</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Hbase/">Hbase</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Hbase增强/">Hbase增强</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Storm/">Storm</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Scala入门/">Scala入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Scala进阶1/">Scala进阶1</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Scala进阶2/">Scala进阶2</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Scala高级/">Scala高级</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/ElSearh/">ElSearh</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Spark入门/">Spark入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/SparkRDD/">SparkRDD</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> Etiquetas</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> Archivo</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Mapa del sitio</a> |  <a href="/atom.xml">suscribirse a este sitio</a> |  <a href="/about/">Contacta al blogger</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">hechao.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.4"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.4" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>