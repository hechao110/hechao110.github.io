<!DOCTYPE html><html lang="hc-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一只沙皮狗的悲伤"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.4"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.4"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>SparkRDD | 一只沙皮狗的悲伤</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">SparkRDD</h1><a id="logo" href="/.">一只沙皮狗的悲伤</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Suche"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">SparkRDD</h1><div class="post-meta"><a href="/2019/08/08/SparkRDD/#comments" class="comment-count"></a><p><span class="date">Aug 08, 2019</span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>Schlägt</i></i></span></p></div><div class="post-content"><h1 id="一-深入RDDS"><a href="#一-深入RDDS" class="headerlink" title="一 深入RDDS"></a>一 深入RDDS</h1><p>先来个小demo</p>
<h2 id="1-需求"><a href="#1-需求" class="headerlink" title="1 需求"></a>1 需求</h2><p>在访问日志中,统计独立IP数量,TOP10</p>
<h2 id="2-明确数据结构"><a href="#2-明确数据结构" class="headerlink" title="2 明确数据结构"></a>2 明确数据结构</h2><p>IP,时间戳,http method,URL……</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1/Nov/2017:00:00:15 +0000] &quot;GET /axis2/services/WebFilteringService/getCategoryByUrl?app=chrome_antiporn&amp;ver=0.19.7.1&amp;url=https%3A//securepubads.g.doubleclick.net/static/3p_cookie.html&amp;cat=business-and-economy HTTP/1.1&quot; 200 133 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&quot;</span><br><span class="line">//这是一条数据</span><br></pre></td></tr></table></figure>

<h2 id="3-明确编码步骤"><a href="#3-明确编码步骤" class="headerlink" title="3 明确编码步骤"></a>3 明确编码步骤</h2><p>1 取出IP,生成只有IP的数据集</p>
<p>2 简单清洗</p>
<p>3 统计IP出现次数</p>
<p>4 排序 按照IP出现次数</p>
<p>5 取出前十</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.commons.lang3.StringUtils</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.junit.Test</span><br><span class="line">class AccessLogsCount &#123;</span><br><span class="line"></span><br><span class="line">  @Test</span><br><span class="line">  def logs: Unit = &#123;</span><br><span class="line">    //1 创建SparkContext</span><br><span class="line">     val log_count = new SparkConf().setMaster(&quot;local[3]&quot;).setAppName(&quot;log_count&quot;)</span><br><span class="line">     val context:SparkContext = new SparkContext(log_count)</span><br><span class="line">    //2 获取文件 一次获取一行</span><br><span class="line">    val unit = context.textFile(&quot;G:\\develop\\bigdatas\\BigData\\day25SparkRdds\\data\\access_log_sample.txt&quot;)</span><br><span class="line">    //3 读出IP  并赋予值为1</span><br><span class="line">    val rdd1 = unit.map(item =&gt; (item.split(&quot; &quot;)(0),1))</span><br><span class="line">    //4 简单清洗</span><br><span class="line">       //4.1 去除空的数据</span><br><span class="line">    val rdd2 = rdd1.filter(item =&gt; StringUtils.isNotEmpty(item._1))</span><br><span class="line">       //4.2 去除非法数据</span><br><span class="line">       //4.3 根据业务规整数据</span><br><span class="line">    //5 根据IP出现次数进行聚合</span><br><span class="line">    val rdd3 = rdd2.reduceByKey((curr,ag) =&gt; curr+ag)</span><br><span class="line"></span><br><span class="line">    //6 根据IP出现次数进行排序</span><br><span class="line">    val result = rdd3.sortBy(item =&gt; item._2,ascending = false)</span><br><span class="line">    //7 取出结果</span><br><span class="line">    result.take(10).foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="4-解析"><a href="#4-解析" class="headerlink" title="4 解析"></a>4 解析</h2><p><strong>1假设要针对整个网站的历史数据进行处理, 量有 1T, 如何处理?</strong></p>
<p>因为单机运行瓶颈太多内存,磁盘,CPU等</p>
<p>放在集群中, 利用集群多台计算机来并行处理</p>
<p><strong>2如何放在集群中运行?</strong></p>
<p>简单来讲, 并行计算就是同时使用多个计算资源解决一个问题, 有如下四个要点</p>
<ul>
<li>要解决的问题必须可以分解为多个可以并发计算的部分</li>
<li>每个部分要可以在不同处理器上被同时执行</li>
<li><strong>需要一个共享内存的机制</strong></li>
<li>需要一个总体上的协作机制来进行调度</li>
</ul>
<p><strong>3 如果放在集群中的话, 可能要对整个计算任务进行分解, 如何分解?</strong></p>
<p><a href="https://manzhong.github.io/images/spark/sfj.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/sfj.png" alt="img"></a></p>
<p>概述</p>
<ul>
<li>对于 HDFS 中的文件, 是分为不同的 Block 的</li>
<li>在进行计算的时候, 就可以按照 Block 来划分, 每一个 Block 对应一个不同的计算单元</li>
</ul>
<p>扩展</p>
<ul>
<li><code>RDD</code> 并没有真实的存放数据, 数据是从 HDFS 中读取的, 在计算的过程中读取即可</li>
<li><code>RDD</code> 至少是需要可以 <strong>分片</strong> 的, 因为HDFS中的文件就是分片的, <code>RDD</code> 分片的意义在于表示对源数据集每个分片的计算, <code>RDD</code> 可以分片也意味着 <strong>可以并行计算</strong></li>
</ul>
<p><strong>4 *移动数据不如移动计算是一个基础的优化, 如何做到?</strong></p>
<p><a href="https://manzhong.github.io/images/spark/yj.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/yj.png" alt="img"></a></p>
<p>每一个计算单元需要记录其存储单元的位置, 尽量调度过去</p>
<p><strong>5 在集群中运行, 需要很多节点之间配合, 出错的概率也更高, 出错了怎么办?</strong></p>
<p><a href="https://manzhong.github.io/images/spark/rc.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/rc.png" alt="img"></a></p>
<p>RDD1 → RDD2 → RDD3 这个过程中, RDD2 出错了, 有两种办法可以解决</p>
<ol>
<li>缓存 RDD2 的数据, 直接恢复 RDD2, 类似 HDFS 的备份机制</li>
<li>记录 RDD2 的依赖关系, 通过其父级的 RDD 来恢复 RDD2, 这种方式会少很多数据的交互和保存</li>
</ol>
<p>如何通过父级 RDD 来恢复?</p>
<ol>
<li>记录 RDD2 的父亲是 RDD1</li>
<li>记录 RDD2 的计算函数, 例如记录 <code>RDD2 = RDD1.map(…)</code>, <code>map(…)</code> 就是计算函数</li>
<li>当 RDD2 计算出错的时候, 可以通过父级 RDD 和计算函数来恢复 RDD2</li>
</ol>
<p><strong>6 假如任务特别复杂, 流程特别长, 有很多 RDD 之间有依赖关系, 如何优化?</strong></p>
<p><a href="https://manzhong.github.io/images/spark/yh.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/yh.png" alt="img"></a></p>
<p>上面提到了可以使用依赖关系来进行容错, 但是如果依赖关系特别长的时候, 这种方式其实也比较低效, 这个时候就应该使用另外一种方式, 也就是记录数据集的状态</p>
<ul>
<li>在 Spark 中有两个手段可以做到</li>
<li>缓存Checkpoint</li>
</ul>
<h2 id="5-rdd出现解决了什么问题"><a href="#5-rdd出现解决了什么问题" class="headerlink" title="5 rdd出现解决了什么问题"></a>5 rdd出现解决了什么问题</h2><p><strong>在 RDD 出现之前, 当时 MapReduce 是比较主流的, 而 MapReduce 如何执行迭代计算的任务呢?</strong></p>
<p><a href="https://manzhong.github.io/images/spark/mr.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/mr.png" alt="img"></a></p>
<p>多个 MapReduce 任务之间没有基于内存的数据共享方式, 只能通过磁盘来进行共享</p>
<p>这种方式明显比较低效</p>
<p><strong>rdd 执行迭代计算任务</strong></p>
<p><a href="https://manzhong.github.io/images/spark/rddd.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/rddd.png" alt="img"></a></p>
<p>在 Spark 中, 其实最终 Job3 从逻辑上的计算过程是: <code>Job3 = (Job1.map).filter</code>, 整个过程是共享内存的, 而不需要将中间结果存放在可靠的分布式文件系统中</p>
<p>这种方式可以在保证容错的前提下, 提供更多的灵活, 更快的执行速度, RDD 在执行迭代型任务时候的表现可以通过下面代码体现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 线性回归</span><br><span class="line">val points = sc.textFile(...)</span><br><span class="line">	.map(...)</span><br><span class="line">	.persist(...)</span><br><span class="line">val w = randomValue</span><br><span class="line">for (i &lt;- 1 to 10000) &#123;</span><br><span class="line">    val gradient = points.map(p =&gt; p.x * (1 / (1 + exp(-p.y * (w dot p.x))) - 1) * p.y)</span><br><span class="line">    	.reduce(_ + _)</span><br><span class="line">    w -= gradient</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="6-rdd的特点"><a href="#6-rdd的特点" class="headerlink" title="6 rdd的特点"></a>6 rdd的特点</h2><p><strong>RDD 不仅是数据集, 也是编程模型</strong></p>
<p>RDD 即是一种数据结构, 同时也提供了上层 API, 同时 RDD 的 API 和 Scala 中对集合运算的 API 非常类似, 同样也都是各种算子</p>
<p><strong>RDD 的算子大致分为两类:</strong></p>
<ul>
<li>Transformation 转换操作, 例如 <code>map</code> <code>flatMap</code> <code>filter</code> 等</li>
<li>Action 动作操作, 例如 <code>reduce</code> <code>collect</code> <code>show</code> 等</li>
</ul>
<p>执行 RDD 的时候, 在执行到转换操作的时候, 并不会立刻执行, 直到遇见了 Action 操作, 才会触发真正的执行, 这个特点叫做 <strong>惰性求值</strong></p>
<h3 id="rdd可以分区"><a href="#rdd可以分区" class="headerlink" title="rdd可以分区"></a>rdd可以分区</h3><p><a href="https://manzhong.github.io/images/spark/rddfq.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/rddfq.png" alt="img"></a></p>
<p>RDD 是一个分布式计算框架, 所以, 一定是要能够进行分区计算的, 只有分区了, 才能利用集群的并行计算能力</p>
<p>同时, RDD 不需要始终被具体化, 也就是说: RDD 中可以没有数据, 只要有足够的信息知道自己是从谁计算得来的就可以, 这是一种非常高效的容错方式</p>
<h3 id="rdd是只读的"><a href="#rdd是只读的" class="headerlink" title="rdd是只读的"></a>rdd是只读的</h3><p>RDD 是只读的, 不允许任何形式的修改. 虽说不能因为 RDD 和 HDFS 是只读的, 就认为分布式存储系统必须设计为只读的. 但是设计为只读的, 会显著降低问题的复杂度, 因为 RDD 需要可以容错, 可以惰性求值, 可以移动计算, 所以很难支持修改.</p>
<ul>
<li>RDD2 中可能没有数据, 只是保留了依赖关系和计算函数, 那修改啥?</li>
<li>如果因为支持修改, 而必须保存数据的话, 怎么容错?</li>
<li>如果允许修改, 如何定位要修改的那一行? RDD 的转换是粗粒度的, 也就是说, RDD 并不感知具体每一行在哪.</li>
</ul>
<h3 id="rdd是可以容错的"><a href="#rdd是可以容错的" class="headerlink" title="rdd是可以容错的"></a>rdd是可以容错的</h3><p>RDD 的容错有两种方式</p>
<ul>
<li>保存 RDD 之间的依赖关系, 以及计算函数, 出现错误重新计算</li>
<li>直接将 RDD 的数据存放在外部存储系统, 出现错误直接读取, Checkpoint</li>
</ul>
<h3 id="弹性分布式数据集"><a href="#弹性分布式数据集" class="headerlink" title="弹性分布式数据集"></a>弹性分布式数据集</h3><p>分布式</p>
<p>RDD 支持分区, 可以运行在集群中</p>
<p>弹性</p>
<ul>
<li>RDD 支持高效的容错</li>
<li>RDD 中的数据即可以缓存在内存中, 也可以缓存在磁盘中, 也可以缓存在外部存储中</li>
</ul>
<p>数据集</p>
<ul>
<li>RDD 可以不保存具体数据, 只保留创建自己的必备信息, 例如依赖和计算函数</li>
<li>RDD 也可以缓存起来, 相当于存储具体数据</li>
</ul>
<p><strong>总结 rdd的五大属性</strong></p>
<p>首先整理一下上面所提到的 RDD 所要实现的功能:</p>
<ol>
<li>RDD 有分区</li>
<li>RDD 要可以通过依赖关系和计算函数进行容错</li>
<li>RDD 要针对数据本地性进行优化</li>
<li>RDD 支持 MapReduce 形式的计算, 所以要能够对数据进行 Shuffled</li>
</ol>
<p>对于 RDD 来说, 其中应该有什么内容呢? 如果站在 RDD 设计者的角度上, 这个类中, 至少需要什么属性?</p>
<ul>
<li><code>Partition List</code> 分片列表, 记录 RDD 的分片, 可以在创建 RDD 的时候指定分区数目, 也可以通过算子来生成新的 RDD 从而改变分区数目</li>
<li><code>Compute Function</code> 为了实现容错, 需要记录 RDD 之间转换所执行的计算函数</li>
<li><code>RDD Dependencies</code> RDD 之间的依赖关系, 要在 RDD 中记录其上级 RDD 是谁, 从而实现容错和计算</li>
<li><code>Partitioner</code> 为了执行 Shuffled 操作, 必须要有一个函数用来计算数据应该发往哪个分区</li>
<li><code>Preferred Location</code> 优先位置, 为了实现数据本地性操作, 从而移动计算而不是移动存储, 需要记录每个 RDD 分区最好应该放置在什么位置</li>
</ul>
<h1 id="二-rdd算子"><a href="#二-rdd算子" class="headerlink" title="二 rdd算子"></a>二 rdd算子</h1><h2 id="1-分类"><a href="#1-分类" class="headerlink" title="1 分类"></a>1 分类</h2><p>RDD 中的算子从功能上分为两大类</p>
<ol>
<li>Transformation(转换) 它会在一个已经存在的 RDD 上创建一个新的 RDD, 将旧的 RDD 的数据转换为另外一种形式后放入新的 RDD(map,flatMap等) <strong>转换算子的本质就是生成各种rdd,让rdd之间具有联系,只是生成rdd链条</strong>,<strong>执行到转换的时候,并不会真的执行整个程序,而是在Action被调用后,程序才可以执行</strong></li>
<li>Action(动作) 执行各个分区的计算任务, 将的到的结果返回到 Driver 中 <strong>执行操作</strong></li>
</ol>
<p>RDD 中可以存放各种类型的数据, 那么对于不同类型的数据, RDD 又可以分为三类</p>
<ul>
<li>针对基础类型(例如 String)处理的普通算子</li>
<li>针对 <code>Key-Value</code> 数据处理的 <code>byKey</code> 算子 (reduceByKey等)</li>
<li>针对数字类型数据处理的计算算子</li>
</ul>
<h2 id="2-特点"><a href="#2-特点" class="headerlink" title="2 特点"></a>2 特点</h2><ul>
<li>Spark 中所有的 Transformations 是 Lazy(惰性) 的, 它们不会立即执行获得结果. 相反, 它们只会记录在数据集上要应用的操作. 只有当需要返回结果给 Driver 时, 才会执行这些操作, 通过 DAGScheduler 和 TaskScheduler 分发到集群中运行, 这个特性叫做 <strong>惰性求值</strong></li>
<li>默认情况下, 每一个 Action 运行的时候, 其所关联的所有 Transformation RDD 都会重新计算, 但是也可以使用 <code>presist</code> 方法将 RDD 持久化到磁盘或者内存中. 这个时候为了下次可以更快的访问, 会把数据保存到集群上.</li>
</ul>
<h2 id="3-TransFormation算子"><a href="#3-TransFormation算子" class="headerlink" title="3 TransFormation算子"></a>3 TransFormation算子</h2><h3 id="1-map"><a href="#1-map" class="headerlink" title="1 map"></a>1 map</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(Seq(1, 2, 3))</span><br><span class="line">  .map( num =&gt; num * 10 )</span><br><span class="line">  .collect()</span><br></pre></td></tr></table></figure>

<p>map(T ⇒ U)</p>
<p>作用</p>
<ul>
<li>把 RDD 中的数据 一对一 的转为另一种形式</li>
</ul>
<p>签名</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def map[U: ClassTag](f: T ⇒ U): RDD[U]</span><br></pre></td></tr></table></figure>

<p>参数</p>
<ul>
<li><code>f</code> → Map 算子是 <code>原RDD → 新RDD</code> 的过程, 传入函数的参数是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据</li>
</ul>
<p>注意点</p>
<ul>
<li>Map 是一对一, 如果函数是 <code>String → Array[String]</code> 则新的 RDD 中每条数据就是一个数组</li>
</ul>
<h3 id="2-flatMap-T-⇒-List-U"><a href="#2-flatMap-T-⇒-List-U" class="headerlink" title="2 flatMap(T ⇒ List[U])"></a>2 flatMap(T ⇒ List[U])</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(Seq(&quot;Hello lily&quot;, &quot;Hello lucy&quot;, &quot;Hello tim&quot;))</span><br><span class="line">  .flatMap( line =&gt; line.split(&quot; &quot;) )</span><br><span class="line">  .collect()</span><br></pre></td></tr></table></figure>

<p>作用</p>
<ul>
<li>FlatMap 算子和 Map 算子类似, 但是 FlatMap 是一对多</li>
</ul>
<p>调用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def flatMap[U: ClassTag](f: T ⇒ List[U]): RDD[U]</span><br></pre></td></tr></table></figure>

<p>参数</p>
<ul>
<li><code>f</code> → 参数是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据, 需要注意的是返回值是一个集合, 集合中的数据会被展平后再放入新的 RDD</li>
</ul>
<p>注意点</p>
<ul>
<li>flatMap 其实是两个操作, 是 <code>map + flatten</code>, 也就是先转换, 后把转换而来的 List 展开</li>
<li>Spark 中并没有直接展平 RDD 中数组的算子, 可以使用 <code>flatMap</code> 做这件事</li>
</ul>
<h3 id="3mapPartitions-List-T-⇒-List-U"><a href="#3mapPartitions-List-T-⇒-List-U" class="headerlink" title="3mapPartitions(List[T] ⇒ List[U])"></a>3mapPartitions(List[T] ⇒ List[U])</h3><p><strong>DD[T] ⇒ RDD[U]</strong> 和 map 类似, 但是针对整个分区的数据转换</p>
<p>map是针对一条数据,mapPartitions是针对一个分区的数据转换</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">context.parallelize(Seq(1,2,3,4),2)  //2 为分区数</span><br><span class="line">      .mapPartitions(item =&gt; &#123;</span><br><span class="line">        item.foreach(iter =&gt; print(iter))</span><br><span class="line">        item</span><br><span class="line">      &#125;)</span><br><span class="line">      .collect()</span><br></pre></td></tr></table></figure>

<p><strong>注意</strong> mapPartitions()的返回值和传入参数都是集合类型 因为是一个分区的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">context.parallelize(Seq(1,2,3,4),2)  //2 为分区数</span><br><span class="line">      .mapPartitions(iter =&gt;&#123;</span><br><span class="line">        //iter 是scala中的集合类型</span><br><span class="line">        iter.map(item =&gt; item*10)  //返回一个集合</span><br><span class="line">      &#125;)</span><br><span class="line">      .collect()</span><br><span class="line">      .foreach(print(_))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-mapPartitionsWithIndex"><a href="#4-mapPartitionsWithIndex" class="headerlink" title="4 mapPartitionsWithIndex"></a>4 mapPartitionsWithIndex</h3><p>和 mapPartitions 类似, 只是在函数中增加了分区的 Index</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> context.parallelize(Seq(1,0,55,555,999),2)</span><br><span class="line">      .mapPartitionsWithIndex((in,iter) =&gt;&#123;</span><br><span class="line">        println(&quot;index&quot;+in)</span><br><span class="line">        iter.map(item =&gt; println(item))</span><br><span class="line">      &#125;)</span><br><span class="line">      .collect()</span><br><span class="line">//两个结果一样  顺序可能不同</span><br><span class="line">context.parallelize(Seq(1,0,55,555,999),2)</span><br><span class="line">      .mapPartitionsWithIndex((in,iter) =&gt;&#123;</span><br><span class="line">        println(&quot;index&quot;+in)</span><br><span class="line">        iter.foreach(item =&gt; println(item))</span><br><span class="line">        iter</span><br><span class="line">      &#125;)</span><br><span class="line">      .collect()</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">index0</span><br><span class="line">1</span><br><span class="line">0</span><br><span class="line">index1</span><br><span class="line">55</span><br><span class="line">555</span><br><span class="line">999</span><br><span class="line">有可能出现</span><br><span class="line">index0</span><br><span class="line">index1</span><br><span class="line">1</span><br><span class="line">55</span><br><span class="line">0</span><br><span class="line">555</span><br><span class="line">999</span><br><span class="line">context.parallelize(Seq(1,2,3,4),2)</span><br><span class="line">      .mapPartitionsWithIndex((in,iter) =&gt;&#123;</span><br><span class="line">        println(&quot;index&quot;+in)</span><br><span class="line">        iter.foreach(item =&gt; println(item))</span><br><span class="line">        iter</span><br><span class="line">      &#125;)</span><br><span class="line">      .collect()</span><br><span class="line">      </span><br><span class="line">  context.parallelize(Seq(1,2,3,4),2)  //Seq(1,0,55,555,999)</span><br><span class="line">      .mapPartitionsWithIndex((in,iter) =&gt;&#123;</span><br><span class="line">        println(&quot;index&quot;+in)</span><br><span class="line">        iter.map(item =&gt; println(item))</span><br><span class="line">      &#125;)</span><br><span class="line">      .collect()</span><br><span class="line">index0</span><br><span class="line">index1</span><br><span class="line">3</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">4</span><br></pre></td></tr></table></figure>

<p><strong>map,mapPartitions和mapPartitionsWithIndex区别</strong></p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">参数</th>
<th align="left">返回值</th>
<th align="left">参数个数</th>
</tr>
</thead>
<tbody><tr>
<td align="left">map</td>
<td align="left">单条数据</td>
<td align="left">单条数据</td>
<td align="left">一个</td>
</tr>
<tr>
<td align="left">mapPartitions</td>
<td align="left">集合(一个分区所有数据)</td>
<td align="left">集合</td>
<td align="left">一个</td>
</tr>
<tr>
<td align="left">mapPartitionsWithIndex</td>
<td align="left">集合(一个分区所有数据)和分区数</td>
<td align="left">集合</td>
<td align="left">两个</td>
</tr>
</tbody></table>
<h3 id="5-filter"><a href="#5-filter" class="headerlink" title="5 filter"></a>5 filter</h3><p>作用</p>
<ul>
<li><code>Filter</code> 算子的主要作用是过滤掉不需要的内容</li>
</ul>
<p>接收函数,参数为每一个元素,如果函数返回为true当前元素会被加入新的数据集,如果为false,会过滤掉该元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">context.parallelize(Seq(1,0,55,555,999,88),2)</span><br><span class="line">      .filter(item =&gt; item % 2 ==0)</span><br><span class="line">      .collect()</span><br><span class="line">      .foreach(println(_))</span><br></pre></td></tr></table></figure>

<h3 id="6-sample-withReplacement-fraction-seed"><a href="#6-sample-withReplacement-fraction-seed" class="headerlink" title="6 sample(withReplacement, fraction, seed)"></a>6 sample(withReplacement, fraction, seed)</h3><p>作用</p>
<ul>
<li>Sample 算子可以从一个数据集中抽样出来一部分, 常用作于减小数据集以保证运行速度, 并且尽可能少规律的损失</li>
</ul>
<p>参数</p>
<ul>
<li>Sample 接受第一个参数为<code>withReplacement</code>, 意为是否取样以后是否还放回原数据集供下次使用, 简单的说, 如果这个参数的值为 true, 则抽样出来的数据集中可能会有重复 若为false 则不会有重复的值</li>
<li>Sample 接受第二个参数为<code>fraction</code>, 意为抽样的比例 double类型</li>
<li>Sample 接受第三个参数为<code>seed</code>, 随机数种子, 用于 Sample 内部随机生成下标, 一般不指定, 使用默认值</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val value = context.parallelize(Seq(1,2,3,4,5,6),2)  //1,2,3,4,5,6  1,0,55,555,999,88</span><br><span class="line">      val unit = value.sample(false,0.5)</span><br><span class="line">    unit.collect().foreach(println(_))</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6  </span><br><span class="line">或者</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4   结果不一定</span><br></pre></td></tr></table></figure>

<h3 id="7-mapValues"><a href="#7-mapValues" class="headerlink" title="7 mapValues"></a>7 mapValues</h3><p>作用</p>
<ul>
<li>MapValues 只能作用于 Key-Value 型数据, 和 Map 类似, 也是使用函数按照转换数据, 不同点是 MapValues 只转换 Key-Value 中的 Value</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">context.parallelize(Seq((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4),(&quot;e&quot;,5)),2)</span><br><span class="line">      .mapValues(item =&gt; item*10)   //item 指代(&quot;a&quot;,k) 中的k  若数据不为k-values格式 则这个方法调不到</span><br><span class="line">      .collect()</span><br><span class="line">      .foreach(println(_))</span><br></pre></td></tr></table></figure>

<h3 id="8-差集-交集和并集"><a href="#8-差集-交集和并集" class="headerlink" title="8 差集,交集和并集"></a>8 差集,交集和并集</h3><p><strong>union(other)</strong> 并集 所有元素都会集合,包括重复的元素</p>
<p>*<em>intersection(other) *</em>交集</p>
<p>Intersection 算子是一个集合操作, 用于求得 左侧集合 和 右侧集合 的交集, 换句话说, 就是左侧集合和右侧集合都有的元素, 并生成一个新的 RDD</p>
<p><strong>subtract(other, numPartitions)</strong>差集, 可以设置分区数 a中有 b中没有的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//交集 并集  差集</span><br><span class="line"> @Test</span><br><span class="line"> def union(): Unit =&#123;</span><br><span class="line">   val a1 = context.parallelize(Seq(1,2,3,4,5,6))</span><br><span class="line">   val a2 = context.parallelize(Seq(4,5,6,7,8,9))</span><br><span class="line">   //并集</span><br><span class="line">  /* a1.union(a2)</span><br><span class="line">     .collect()</span><br><span class="line">     .foreach(println(_))*/</span><br><span class="line">   //交集</span><br><span class="line">   /*a1.intersection(a2)</span><br><span class="line">     .collect()</span><br><span class="line">     .foreach(println(_))*/</span><br><span class="line">   //差集</span><br><span class="line">   a1.subtract(a2)</span><br><span class="line">     .collect()</span><br><span class="line">     .foreach(println(_))//213</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h3 id="9-distinct-numPartitions"><a href="#9-distinct-numPartitions" class="headerlink" title="9 distinct(numPartitions)"></a>9 distinct(numPartitions)</h3><p>作用</p>
<ul>
<li>Distinct 算子用于去重</li>
</ul>
<p>注意点</p>
<ul>
<li>Distinct 是一个需要 Shuffled 的操作</li>
<li>本质上 Distinct 就是一个 reductByKey, 把重复的合并为一个</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(Seq(1, 1, 2, 2, 3))</span><br><span class="line">  .distinct()</span><br><span class="line">  .collect()</span><br></pre></td></tr></table></figure>

<h3 id="10groupByKey与reduceByKey"><a href="#10groupByKey与reduceByKey" class="headerlink" title="10groupByKey与reduceByKey"></a>10groupByKey与reduceByKey</h3><p><strong>reduceByKey:</strong></p>
<p>reduceByKey((V, V) ⇒ V, numPartition)</p>
<p>作用</p>
<ul>
<li>首先按照 Key 分组生成一个 Tuple, 然后针对每个组执行 <code>reduce</code> 算子</li>
</ul>
<p>调用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]</span><br></pre></td></tr></table></figure>

<p>参数</p>
<ul>
<li>func → 执行数据处理的函数, 传入两个参数, 一个是当前值, 一个是局部汇总, 这个函数需要有一个输出, 输出就是这个 Key 的汇总结果</li>
</ul>
<p>注意点</p>
<ul>
<li>ReduceByKey 只能作用于 Key-Value 型数据, Key-Value 型数据在当前语境中特指 Tuple2</li>
<li>ReduceByKey 是一个需要 Shuffled 的操作</li>
<li>和其它的 Shuffled 相比, ReduceByKey是高效的, 因为类似 MapReduce 的, 在 Map 端有一个 Cominer, 这样 I/O 的数据便会减少</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>groupByKey</strong></p>
<p>作用</p>
<ul>
<li>GroupByKey 算子的主要作用是按照 Key 分组, 和 ReduceByKey 有点类似, 但是 GroupByKey 并不求聚合, 只是列举 Key 对应的所有 Value</li>
</ul>
<p>注意点</p>
<ul>
<li>GroupByKey 是一个 Shuffled</li>
<li>GroupByKey 和 ReduceByKey 不同, 因为需要列举 Key 对应的所有数据, 所以无法在 Map 端做 Combine, 所以 GroupByKey 的性能并没有 ReduceByKey 好</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>总结</strong></p>
<p>reduceByKey:在map端做Combiner 了可对结果做 聚合</p>
<p>groupByKey: 在map端不做聚合,也不做Combiner 结果格式: (k,(values,value,….))</p>
<p>###11combineByKey()</p>
<p>作用</p>
<ul>
<li>对数据集按照 Key 进行聚合</li>
</ul>
<p>调用</p>
<ul>
<li><code>combineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner], [mapSideCombiner], [serializer])</code></li>
</ul>
<p>参数</p>
<ul>
<li><code>createCombiner</code> 将 Value 进行初步转换</li>
<li><code>mergeValue</code> 在每个分区把上一步转换的结果聚合</li>
<li><code>mergeCombiners</code> 在所有分区上把每个分区的聚合结果聚合</li>
<li><code>partitioner</code> 可选, 分区函数</li>
<li><code>mapSideCombiner</code> 可选, 是否在 Map 端 Combine</li>
<li><code>serializer</code> 序列化器</li>
</ul>
<p>注意点</p>
<ul>
<li><code>combineByKey</code> 的要点就是三个函数的意义要理解</li>
<li><code>groupByKey</code>, <code>reduceByKey</code> 的底层都是 <code>combineByKey</code></li>
</ul>
<p>例子:</p>
<p><a href="https://manzhong.github.io/images/spark/cbk.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/cbk.png" alt="img"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">  def combineByKeyDemo(): Unit =&#123;</span><br><span class="line">    //创建数据</span><br><span class="line">    val rdd = context.parallelize(Seq(</span><br><span class="line">      (&quot;zhangsan&quot;, 99.0),</span><br><span class="line">      (&quot;zhangsan&quot;, 96.0),</span><br><span class="line">      (&quot;lisi&quot;, 97.0),</span><br><span class="line">      (&quot;lisi&quot;, 98.0),</span><br><span class="line">      (&quot;zhangsan&quot;, 97.0))</span><br><span class="line">    )</span><br><span class="line">    //处理数据</span><br><span class="line">    val rdd3 = rdd.combineByKey(</span><br><span class="line">      //结果   (&quot;zhangsan&quot;, (99.0,1))</span><br><span class="line">      createCombiner = (item: Double) =&gt; (item, 1),</span><br><span class="line">      //结果  (&quot;zhangsan&quot;, (99.0+96.0,1+1))  分区上的聚合</span><br><span class="line">      mergeValue = (c: (Double, Int), newi: Double) =&gt; ((c._1 + newi), c._2 + 1),</span><br><span class="line">      //结果   把分区上的结果再次聚合 形成最终结果</span><br><span class="line">      mergeCombiners = (cuur: (Double, Int), agg: (Double, Int)) =&gt; ((cuur._1 + agg._1), (cuur._2 + agg._2))</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line">    rdd3.collect().foreach(println(_))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="12-foldByKey-zeroValue-V-V-⇒-V"><a href="#12-foldByKey-zeroValue-V-V-⇒-V" class="headerlink" title="12 foldByKey(zeroValue)((V, V) ⇒ V)"></a>12 foldByKey(zeroValue)((V, V) ⇒ V)</h3><p>作用</p>
<ul>
<li>和 ReduceByKey 是一样的, 都是按照 Key 做分组去求聚合, 但是 FoldByKey 的不同点在于可以指定初始值</li>
</ul>
<p>调用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foldByKey(zeroValue)(func)</span><br></pre></td></tr></table></figure>

<p>参数</p>
<ul>
<li><code>zeroValue</code> 初始值</li>
<li><code>func</code> seqOp 和 combOp 相同, 都是这个参数</li>
</ul>
<p>注意点</p>
<ul>
<li>FoldByKey 是 AggregateByKey 的简化版本, seqOp 和 combOp 是同一个函数</li>
<li>FoldByKey 指定的初始值作用于每一个 Value</li>
<li>scala 中的foldleft或者foldRight 区别是这个值不会作用于每一个value</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">  def foldByKeyDemo(): Unit =&#123;</span><br><span class="line">    context.parallelize(Seq((&quot;a&quot;,1),(&quot;a&quot;,2),(&quot;c&quot;,3),(&quot;d&quot;,4),(&quot;e&quot;,5)))</span><br><span class="line">      .foldByKey(10)((curr,agg) =&gt; curr+agg)    //每一个参数都会+10</span><br><span class="line">      .collect()</span><br><span class="line">      .foreach(println(_))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(d,14)</span><br><span class="line">(e,15)</span><br><span class="line">(a,13)</span><br><span class="line">(c,13)</span><br></pre></td></tr></table></figure>

<h3 id="13-aggregateByKey"><a href="#13-aggregateByKey" class="headerlink" title="13 aggregateByKey()"></a>13 aggregateByKey()</h3><ul>
<li><p>作用</p>
<p>聚合所有 Key 相同的 Value, 换句话说, 按照 Key 聚合 Value</p>
</li>
<li><p>调用</p>
<p><code>rdd.aggregateByKey(zeroValue)(seqOp, combOp)</code></p>
</li>
<li><p>参数</p>
<p><code>zeroValue</code> 初始值<code>seqOp</code> 转换每一个值的函数<code>comboOp</code> 将转换过的值聚合的函数</p>
</li>
</ul>
<p>注意点 <strong>* 为什么需要两个函数?</strong> aggregateByKey 运行将一个<code>RDD[(K, V)]</code>聚合为<code>RDD[(K, U)]</code>, 如果要做到这件事的话, 就需要先对数据做一次转换, 将每条数据从<code>V</code>转为<code>U</code>, <code>seqOp</code>就是干这件事的 ** 当<code>seqOp</code>的事情结束以后, <code>comboOp</code>把其结果聚合</p>
<ul>
<li>和 reduceByKey 的区别::<ul>
<li>aggregateByKey 最终聚合结果的类型和传入的初始值类型保持一致</li>
<li>reduceByKey 在集合中选取第一个值作为初始值, 并且聚合过的数据类型不能改变</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">* 需求 求所有商品打八折  后 手机和电脑的总价</span><br><span class="line">*</span><br><span class="line">* aggregateByKey(zeroValue)(seqOp,comOp)</span><br><span class="line">*zeroValue  指定初始值</span><br><span class="line">* seqOp: 作用于每一个元素,根据初始值进行计算</span><br><span class="line">*comOp:  将sepOp的结果进行聚合</span><br><span class="line">* */</span><br><span class="line">  @Test</span><br><span class="line">  def agg(): Unit =&#123;</span><br><span class="line">     context.parallelize(Seq((&quot;手机&quot;, 10.0), (&quot;手机&quot;, 15.0), (&quot;电脑&quot;, 20.0)))</span><br><span class="line">      .aggregateByKey(0.8)((zeroValue,item)=&gt; zeroValue*item,(curr,aggr)=&gt; curr+aggr)</span><br><span class="line">      .collect()</span><br><span class="line">      .foreach(println(_))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>适合针对每个数据先处理,后聚合</strong></p>
<h3 id="14-JOIN"><a href="#14-JOIN" class="headerlink" title="14 JOIN"></a>14 JOIN</h3><p>作用</p>
<ul>
<li>将两个 RDD 按照相同的 Key 进行连接</li>
</ul>
<p>调用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">join(other, [partitioner or numPartitions])</span><br></pre></td></tr></table></figure>

<p>参数</p>
<ul>
<li><code>other</code> 其它 RDD</li>
<li><code>partitioner or numPartitions</code> 可选, 可以通过传递分区函数或者分区数量来改变分区</li>
</ul>
<p>注意点</p>
<ul>
<li>Join 有点类似于 SQL 中的内连接, 只会再结果中包含能够连接到的 Key</li>
<li>Join 的结果是一个笛卡尔积形式, 例如<code>&quot;a&quot;, 1), (&quot;a&quot;, 2</code>和<code>&quot;a&quot;, 10), (&quot;a&quot;, 11</code>的 Join 结果集是 <code>&quot;a&quot;, 1, 10), (&quot;a&quot;, 1, 11), (&quot;a&quot;, 2, 10), (&quot;a&quot;, 2, 11</code></li>
</ul>
<p><a href="https://manzhong.github.io/images/spark/join.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/join.png" alt="img"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//join</span><br><span class="line">  //同key 时才参与join</span><br><span class="line">  @Test</span><br><span class="line">  def joinDemo(): Unit =&#123;</span><br><span class="line">    val rdd1 = context.parallelize(Seq((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;b&quot;, 1)))</span><br><span class="line">    val rdd2 = context.parallelize(Seq((&quot;a&quot;, 10), (&quot;a&quot;, 11), (&quot;a&quot;, 12)))</span><br><span class="line">    rdd1.join(rdd2).collect().foreach(println(_))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="15-sortBy和sortByKey"><a href="#15-sortBy和sortByKey" class="headerlink" title="15 sortBy和sortByKey"></a>15 sortBy和sortByKey</h3><p>作用</p>
<ul>
<li>排序相关相关的算子有两个, 一个是<code>sortBy</code>, 另外一个是<code>sortByKey</code></li>
</ul>
<p>调用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sortBy(func, ascending, numPartitions)</span><br></pre></td></tr></table></figure>

<p>参数</p>
<ul>
<li><code>func</code>通过这个函数返回要排序的字段</li>
<li><code>ascending</code>是否升序</li>
<li><code>numPartitions</code>分区数</li>
</ul>
<p>注意点</p>
<ul>
<li>普通的 RDD 没有<code>sortByKey</code>, 只有 Key-Value 的 RDD 才有</li>
<li><code>sortBy</code>可以指定按照哪个字段来排序, <code>sortByKey</code>直接按照 Key 来排序</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">//sortBy 和sortByKey</span><br><span class="line">  /*sortBy可以作用所有的类型的数据,sortByKey只能识别kv类型数据</span><br><span class="line">  sortBy可以按照任意部分来排序  sortByKey 只能按照k进行排序</span><br><span class="line">  *</span><br><span class="line">  * */</span><br><span class="line">  @Test</span><br><span class="line">  def sort(): Unit =&#123;</span><br><span class="line">    val rdd1 = context.parallelize(Seq(1,5,2,8,3))</span><br><span class="line">    val rdd2 = context.parallelize(Seq((&quot;a&quot;, 3), (&quot;b&quot;, 2), (&quot;c&quot;, 8)))</span><br><span class="line">    rdd1.sortBy(it =&gt; it)</span><br><span class="line">    rdd2.sortBy(it =&gt; it._2).collect().foreach(println(_))</span><br><span class="line">    rdd2.sortByKey()  //按照key 进行排序</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="16-repartitioin-和-coalesce"><a href="#16-repartitioin-和-coalesce" class="headerlink" title="16 repartitioin 和 coalesce"></a>16 <code>repartitioin</code> 和 <code>coalesce</code></h3><p>作用 重分区</p>
<ul>
<li>一般涉及到分区操作的算子常见的有两个, <code>repartitioin</code> 和 <code>coalesce</code>, 两个算子都可以调大或者调小分区数量</li>
</ul>
<p>调用</p>
<ul>
<li><code>repartitioin(numPartitions)</code></li>
<li><code>coalesce(numPartitions, shuffle)</code></li>
<li>若coalesce想调大分区 则必须设置shuffle为true</li>
</ul>
<p>参数</p>
<ul>
<li><code>numPartitions</code> 新的分区数</li>
<li><code>shuffle</code> 是否 shuffle, 如果新的分区数量比原分区数大, 必须 Shuffled, 否则重分区无效</li>
</ul>
<p>注意点</p>
<ul>
<li><code>repartition</code> 和 <code>coalesce</code> 的不同就在于 <code>coalesce</code> 可以控制是否 Shuffle</li>
<li><code>repartition</code> 是一个 Shuffled 操作</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line"> def re(): Unit =&#123;</span><br><span class="line">   val rdd1 = context.parallelize(Seq(1,5,2,8,3),2)</span><br><span class="line">   println(rdd1.repartition(5).partitions.size)   //默认shuffle为true</span><br><span class="line"></span><br><span class="line">   println(rdd1.coalesce(1))//调小可以  相对于初始分区数</span><br><span class="line">   println(rdd1.coalesce(5, shuffle = true))//调大指定shuffle为true</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p><strong>repartitionAndSortWithinPartitions</strong></p>
<p>重新分区的同时升序排序, 在partitioner中排序, 比先重分区再排序要效率高, 建议使用在需要分区后再排序的场景使用</p>
<p><strong>cartesian(other)</strong></p>
<p><strong>(RDD[T], RDD[U]) ⇒ RDD[(T, U)]</strong> 生成两个 RDD 的笛卡尔积</p>
<p><strong>cogroup(other, numPartitions)</strong></p>
<p>作用</p>
<ul>
<li>多个 RDD 协同分组, 将多个 RDD 中 Key 相同的 Value 分组</li>
</ul>
<p>调用</p>
<ul>
<li><code>cogroup(rdd1, rdd2, rdd3, [partitioner or numPartitions])</code></li>
</ul>
<p>参数</p>
<ul>
<li><code>rdd…</code> 最多可以传三个 RDD 进去, 加上调用者, 可以为四个 RDD 协同分组</li>
<li><code>partitioner or numPartitions</code> 可选, 可以通过传递分区函数或者分区数来改变分区</li>
</ul>
<p>注意点</p>
<ul>
<li><p>对 RDD1, RDD2, RDD3 进行 cogroup, 结果中就一定会有三个 List, 如果没有 Value 则是空 List, 这一点类似于 SQL 的全连接, 返回所有结果, 即使没有关联上</p>
</li>
<li><p>CoGroup 是一个需要 Shuffled 的操作</p>
<p><a href="https://manzhong.github.io/images/spark/ccc.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/ccc.png" alt="img"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(Seq((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;a&quot;, 5), (&quot;b&quot;, 2), (&quot;b&quot;, 6), (&quot;c&quot;, 3), (&quot;d&quot;, 2)))</span><br><span class="line">val rdd2 = sc.parallelize(Seq((&quot;a&quot;, 10), (&quot;b&quot;, 1), (&quot;d&quot;, 3)))</span><br><span class="line">val rdd3 = sc.parallelize(Seq((&quot;b&quot;, 10), (&quot;a&quot;, 1)))</span><br><span class="line"></span><br><span class="line">val result1 = rdd1.cogroup(rdd2).collect()</span><br><span class="line">val result2 = rdd1.cogroup(rdd2, rdd3).collect()</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">执行结果:</span><br><span class="line">Array(</span><br><span class="line">  (d,(CompactBuffer(2),CompactBuffer(3))),</span><br><span class="line">  (a,(CompactBuffer(1, 2, 5),CompactBuffer(10))),</span><br><span class="line">  (b,(CompactBuffer(2, 6),CompactBuffer(1))),</span><br><span class="line">  (c,(CompactBuffer(3),CompactBuffer()))</span><br><span class="line">)</span><br><span class="line"> */</span><br><span class="line">println(result1)</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">执行结果:</span><br><span class="line">Array(</span><br><span class="line">  (d,(CompactBuffer(2),CompactBuffer(3),CompactBuffer())),</span><br><span class="line">  (a,(CompactBuffer(1, 2, 5),CompactBuffer(10),CompactBuffer(1))),</span><br><span class="line">  (b,(CompactBuffer(2, 6),CompactBuffer(1),Co...</span><br><span class="line"> */</span><br><span class="line">println(result2)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p><strong>总结</strong></p>
<p>所有的转换操作的算子都是惰性的,在执行的时候并不会真的去调度运行,求得结果,而是只是生成了对应的RDD,只有在Action时,才会真的运行求得结果</p>
<p><a href="https://manzhong.github.io/images/spark/tz.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/tz.jpg" alt="img"></a></p>
<h2 id="2-Action-算子"><a href="#2-Action-算子" class="headerlink" title="2 Action 算子"></a>2 Action 算子</h2><h3 id="1-reduce-T-T-⇒-U-不是一个shuffle操作"><a href="#1-reduce-T-T-⇒-U-不是一个shuffle操作" class="headerlink" title="1 reduce( (T, T) ⇒ U ) 不是一个shuffle操作"></a>1 reduce( (T, T) ⇒ U ) 不是一个shuffle操作</h3><p>作用</p>
<ul>
<li>对整个结果集规约, 最终生成一条数据, 是整个数据集的汇总</li>
</ul>
<p>调用</p>
<ul>
<li><code>reduce( (currValue[T], agg[T]) ⇒ T )</code></li>
</ul>
<p>注意点</p>
<ul>
<li>reduce 和 reduceByKey 是完全不同的, reduce 是一个 action, 并不是 Shuffled 操作</li>
<li>本质上 reduce 就是现在每个 partition 上求值, 最终把每个 partition 的结果再汇总</li>
<li>例如 一个RDD里面有一万条数据,大部分key相同,有十个key 不同,则reduceByKey会生成10条数据,但reduce只会生成1个结果</li>
<li>reduceByKey 是根据k分组,再把每组聚合 是针对kv数据进行计算</li>
<li>reduce 是针对一整个数据集来进行聚合 是针对任意类型进行计算</li>
</ul>
<p>shuffle操作: 分为mapper和reduce,mapper将数据放入partition的函数计算求得分往哪个reduce,后分到对应的reduce中</p>
<p>reduce操作: 并没有mapper和reduce,因为reduce算子会作用于rdd中的每一个分区,然后在分区上求得局部结果,最终汇总求得最终结果</p>
<p>rdd中的五大属性: Partitioner 在shuffle过程中使用,<strong>partition只有KV型数据的RDD才有</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class ActionDemo &#123;</span><br><span class="line"></span><br><span class="line">  //定义变量</span><br><span class="line">  private val transformations: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;transformations&quot;)</span><br><span class="line">  private val context = new SparkContext(transformations)</span><br><span class="line">    @Test</span><br><span class="line">  def reduceDemo(): Unit =&#123;</span><br><span class="line">      val unit = context.parallelize(Seq((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3)))</span><br><span class="line">      //curr为一条数据(&quot;a&quot;,1)  reduce 整体上的结果,只有一个</span><br><span class="line">      val tuple:(String, Int) = unit.reduce((curr, agg) =&gt; (&quot;总价&quot;,curr._2+agg._2))</span><br><span class="line">      println(tuple)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-collect"><a href="#2-collect" class="headerlink" title="2 collect()"></a>2 collect()</h3><p>以数组的形式返回数据集中所有元素</p>
<h3 id="3-foreach-T-⇒-…"><a href="#3-foreach-T-⇒-…" class="headerlink" title="3 foreach( T ⇒ … )"></a>3 foreach( T ⇒ … )</h3><p>遍历每一个元素</p>
<h3 id="4-count-和countByKey"><a href="#4-count-和countByKey" class="headerlink" title="4 count() 和countByKey()"></a>4 count() 和countByKey()</h3><p>count()返回元素个数</p>
<p>countByKey()</p>
<p>作用</p>
<ul>
<li>求得整个数据集中 Key 以及对应 Key 出现的次数</li>
</ul>
<p>注意点</p>
<ul>
<li>返回结果为 <code>Map(key → count)</code></li>
<li><strong>常在解决数据倾斜问题时使用, 查看倾斜的 Key</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">  * 每次调用Action 都会生成一个job,job会运行获取结果(形成大量log日志)</span><br><span class="line">  *</span><br><span class="line">  * countByKey的结果是Map(k -&gt; k的count)</span><br><span class="line">  * Map(b -&gt; 2, a -&gt; 2, c -&gt; 1)</span><br><span class="line">  * */</span><br><span class="line">  @Test</span><br><span class="line">  def count(): Unit =&#123;</span><br><span class="line">    val unit = context.parallelize(Seq((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3),(&quot;a&quot;,2),(&quot;b&quot;,4)))</span><br><span class="line">    println(unit.count())</span><br><span class="line"></span><br><span class="line">    println(unit.countByKey())</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-take-first-和takeSample"><a href="#5-take-first-和takeSample" class="headerlink" title="5 take first 和takeSample"></a>5 take first 和takeSample</h3><p>take 返回前 N 个元素</p>
<p>first 返回第一个元素</p>
<p>takeSample(withReplacement, fract) 类似于 sample, 区别在这是一个Action, 直接返回结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">take 和takeSample   一个直接获取  一个采样获取</span><br><span class="line"></span><br><span class="line">first :  会在所有分区获取数据,相对来说速度比较慢,但first只是获取第一个元素,所以first只会处理第一分区的数据</span><br><span class="line">所以速度快,无序处理所有数据</span><br><span class="line">@Test</span><br><span class="line">  def takeSam(): Unit =&#123;</span><br><span class="line">    val unit = context.parallelize(Seq(1,2,3,4,5,6))</span><br><span class="line">    unit.takeSample(false,3).foreach(println(_))</span><br><span class="line">    println(unit.first())</span><br><span class="line">    unit.take(3).foreach(println(_))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>总结</strong></p>
<p>RDD 的算子大部分都会生成一些专用的 RDD</p>
<ul>
<li><code>map</code>, <code>flatMap</code>, <code>filter</code> 等算子会生成 <code>MapPartitionsRDD</code></li>
<li><code>coalesce</code>, <code>repartition</code> 等算子会生成 <code>CoalescedRDD</code></li>
</ul>
<p>常见的 RDD 有两种类型</p>
<ul>
<li>转换型的 RDD, Transformation</li>
<li>动作型的 RDD, Action</li>
</ul>
<p>常见的 Transformation 类型的 RDD</p>
<ul>
<li>map</li>
<li>flatMap</li>
<li>filter</li>
<li>groupBy</li>
<li>reduceByKey</li>
</ul>
<p>常见的 Action 类型的 RDD</p>
<ul>
<li>collect</li>
<li>countByKey</li>
<li>reduce</li>
</ul>
<h2 id="3-rdd对不同类型数据的支持"><a href="#3-rdd对不同类型数据的支持" class="headerlink" title="3 rdd对不同类型数据的支持"></a>3 rdd对不同类型数据的支持</h2><p>RDD 对 Key-Value 类型的数据是有专门支持的,,对数字类型也有专门支持</p>
<p>一般要处理的类型有三种</p>
<ul>
<li>字符串</li>
<li>键值对</li>
<li>数字型</li>
</ul>
<p>RDD 的算子设计对这三类不同的数据分别都有支持</p>
<ul>
<li>对于以字符串为代表的基本数据类型是比较基础的一些的操作, 诸如 map, flatMap, filter 等基础的算子</li>
<li>对于键值对类型的数据, 有额外的支持, 诸如 reduceByKey, groupByKey 等 byKey 的算子</li>
<li>同样对于数字型的数据也有额外的支持, 诸如 max, min 等</li>
</ul>
<p><strong>RDD 对键值对数据的额外支持</strong></p>
<p>键值型数据本质上就是一个二元元组, 键值对类型的 RDD 表示为 <code>RDD[(K, V)]</code></p>
<p>RDD 对键值对的额外支持是通过隐式支持来完成的, 一个 <code>RDD[(K, V)]</code>, 可以被隐式转换为一个 <code>PairRDDFunctions</code> 对象, 从而调用其中的方法.</p>
<p>源码:</p>
<p><a href="https://manzhong.github.io/images/spark/spkv.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/spkv.jpg" alt="img"></a></p>
<p><strong>既然对键值对的支持是通过</strong> <code>PairRDDFunctions</code> <strong>提供的, 那么从</strong> <code>PairRDDFunctions</code> <strong>中就可以看到这些支持有什么</strong></p>
<table>
<thead>
<tr>
<th align="left">类别</th>
<th align="left">算子</th>
</tr>
</thead>
<tbody><tr>
<td align="left">聚合操作</td>
<td align="left">reduceByKey foldByKey combineByKey</td>
</tr>
<tr>
<td align="left">分组操作</td>
<td align="left">cogroup groupByKey</td>
</tr>
<tr>
<td align="left">连接操作</td>
<td align="left">join leftOuterJoin rightOuterJoin</td>
</tr>
<tr>
<td align="left">排序操作</td>
<td align="left">sortBy sortByKey</td>
</tr>
<tr>
<td align="left">Action</td>
<td align="left">countByKey take collect</td>
</tr>
</tbody></table>
<p><strong>RDD 对数字型数据的额外支持</strong></p>
<p>对于数字型数据的额外支持基本上<strong>都是 Action 操作, 而不是转换操作</strong></p>
<table>
<thead>
<tr>
<th align="left">算子</th>
<th align="left">含义</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>count</code></td>
<td align="left">个数</td>
</tr>
<tr>
<td align="left"><code>mean</code></td>
<td align="left">均值</td>
</tr>
<tr>
<td align="left"><code>sum</code></td>
<td align="left">求和</td>
</tr>
<tr>
<td align="left"><code>max</code></td>
<td align="left">最大值</td>
</tr>
<tr>
<td align="left"><code>min</code></td>
<td align="left">最小值</td>
</tr>
<tr>
<td align="left"><code>variance</code></td>
<td align="left">方差</td>
</tr>
<tr>
<td align="left"><code>sampleVariance</code></td>
<td align="left">从采样中计算方差</td>
</tr>
<tr>
<td align="left"><code>stdev</code></td>
<td align="left">标准差</td>
</tr>
<tr>
<td align="left"><code>sampleStdev</code></td>
<td align="left">采样的标准差</td>
</tr>
</tbody></table>
<p>###4 使用算子 来个小demo</p>
<p>需求;</p>
<p>以年月为基础,统计北京东四地区的PM值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">package com.nicai.demo</span><br><span class="line"></span><br><span class="line">import org.apache.commons.lang3.StringUtils</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.junit.Test</span><br><span class="line">//需求  以年月为基础,统计北京东四地区的PM值</span><br><span class="line">class PmCount &#123;</span><br><span class="line">  private val conf: SparkConf = new SparkConf().setMaster(&quot;local[5]&quot;).setAppName(&quot;Bj-pm-count&quot;)</span><br><span class="line">  private val context = new SparkContext(conf)</span><br><span class="line">  @Test</span><br><span class="line">  def pmCount(): Unit =&#123;</span><br><span class="line"></span><br><span class="line">    //读取数据 一次一行</span><br><span class="line">   var data: RDD[String] = context.textFile(&quot;G:\\develop\\data\\BeijingPM20100101_20151231_noheader.csv&quot;)</span><br><span class="line">    //清洗数据</span><br><span class="line">    //以年月为key ,pm 值为value</span><br><span class="line">    data.map(item =&gt; (item.split(&quot;,&quot;))).map(items =&gt; ((items(1), items(2)), items(6)))</span><br><span class="line">      .filter(it =&gt; StringUtils.isNoneEmpty(it._2) &amp;&amp; !&quot;NA&quot;.equalsIgnoreCase(it._2))</span><br><span class="line">      .map(it =&gt; (it._1, it._2.toInt))</span><br><span class="line">      .reduceByKey((curr, agg) =&gt; curr + agg)</span><br><span class="line">      .take(10)</span><br><span class="line">      .foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/spark/z.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/z.png" alt="img"></a></p>
</div><div class="post-copyright"><blockquote><p>Ursprünglicher Autor: hechao</p><p>Ursprünglicher Link: <a href="http://yoursite.com/2019/08/08/SparkRDD/">http://yoursite.com/2019/08/08/SparkRDD/</a></p><p>Copyright-Erklärung: Bitte geben Sie die Quelle des Nachdrucks an.</p></blockquote></div><div class="tags"></div><div class="post-share"><div class="social-share"><span>Aktie:</span></div></div><div class="post-nav"><a href="/2019/08/10/spark原理分析/" class="pre">spark原理分析</a><a href="/2019/02/08/Spark入门/" class="next">Spark入门</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">Inhalte</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一-深入RDDS"><span class="toc-text">一 深入RDDS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-需求"><span class="toc-text">1 需求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-明确数据结构"><span class="toc-text">2 明确数据结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-明确编码步骤"><span class="toc-text">3 明确编码步骤</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-解析"><span class="toc-text">4 解析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-rdd出现解决了什么问题"><span class="toc-text">5 rdd出现解决了什么问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-rdd的特点"><span class="toc-text">6 rdd的特点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rdd可以分区"><span class="toc-text">rdd可以分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rdd是只读的"><span class="toc-text">rdd是只读的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rdd是可以容错的"><span class="toc-text">rdd是可以容错的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#弹性分布式数据集"><span class="toc-text">弹性分布式数据集</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二-rdd算子"><span class="toc-text">二 rdd算子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-分类"><span class="toc-text">1 分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-特点"><span class="toc-text">2 特点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-TransFormation算子"><span class="toc-text">3 TransFormation算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-map"><span class="toc-text">1 map</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-flatMap-T-⇒-List-U"><span class="toc-text">2 flatMap(T ⇒ List[U])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3mapPartitions-List-T-⇒-List-U"><span class="toc-text">3mapPartitions(List[T] ⇒ List[U])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-mapPartitionsWithIndex"><span class="toc-text">4 mapPartitionsWithIndex</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-filter"><span class="toc-text">5 filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-sample-withReplacement-fraction-seed"><span class="toc-text">6 sample(withReplacement, fraction, seed)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-mapValues"><span class="toc-text">7 mapValues</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-差集-交集和并集"><span class="toc-text">8 差集,交集和并集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-distinct-numPartitions"><span class="toc-text">9 distinct(numPartitions)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10groupByKey与reduceByKey"><span class="toc-text">10groupByKey与reduceByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-foldByKey-zeroValue-V-V-⇒-V"><span class="toc-text">12 foldByKey(zeroValue)((V, V) ⇒ V)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-aggregateByKey"><span class="toc-text">13 aggregateByKey()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-JOIN"><span class="toc-text">14 JOIN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-sortBy和sortByKey"><span class="toc-text">15 sortBy和sortByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-repartitioin-和-coalesce"><span class="toc-text">16 repartitioin 和 coalesce</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Action-算子"><span class="toc-text">2 Action 算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-reduce-T-T-⇒-U-不是一个shuffle操作"><span class="toc-text">1 reduce( (T, T) ⇒ U ) 不是一个shuffle操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-collect"><span class="toc-text">2 collect()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-foreach-T-⇒-…"><span class="toc-text">3 foreach( T ⇒ … )</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-count-和countByKey"><span class="toc-text">4 count() 和countByKey()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-take-first-和takeSample"><span class="toc-text">5 take first 和takeSample</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-rdd对不同类型数据的支持"><span class="toc-text">3 rdd对不同类型数据的支持</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/08/24/sparkSql高级/">sparkSql高级</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/20/sparkSQL/">sparkSQL</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/18/spark原理分析2/">spark原理分析2</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/10/spark原理分析/">spark原理分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/SparkRDD/">SparkRDD</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/08/Spark入门/">Spark入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/shell/">shell</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/18/Scala高级/">Scala高级</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/04/Yarn-资源调度/">Yarn-资源调度</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/27/Scala进阶2/">Scala进阶2</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> Tags</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> Archiv</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Sitemap</a> |  <a href="/atom.xml">Abonnieren Sie diese Site</a> |  <a href="/about/">Kontaktieren Sie den Blogger</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">hechao.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.4"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.4" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>