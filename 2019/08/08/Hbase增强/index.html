<!DOCTYPE html><html lang="hc-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一只沙皮狗的悲伤"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.4"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.4"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>Hbase增强 | 一只沙皮狗的悲伤</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Hbase增强</h1><a id="logo" href="/.">一只沙皮狗的悲伤</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Inicio</i></a><a href="/archives/"><i class="fa fa-archive"> Archivo</i></a><a href="/about/"><i class="fa fa-user"> Acerca de</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Búsqueda"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Hbase增强</h1><div class="post-meta"><a href="/2019/08/08/Hbase增强/#comments" class="comment-count"></a><p><span class="date">Aug 08, 2019</span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>Golpes</i></i></span></p></div><div class="post-content"><h1 id="Hbase增强"><a href="#Hbase增强" class="headerlink" title="Hbase增强"></a>Hbase增强</h1><h2 id="一-Hbase与MapReduce的集成"><a href="#一-Hbase与MapReduce的集成" class="headerlink" title="一 Hbase与MapReduce的集成"></a>一 Hbase与MapReduce的集成</h2><p>HBase当中的数据最终都是存储在HDFS上面的，HBase天生的支持MR的操作，我们可以通过MR直接处理HBase当中的数据，并且MR可以将处理后的结果直接存储到HBase当中去</p>
<h3 id="需求一-读取myuser这张表当中的数据写入到HBase的另外一张表当中去"><a href="#需求一-读取myuser这张表当中的数据写入到HBase的另外一张表当中去" class="headerlink" title="需求一 读取myuser这张表当中的数据写入到HBase的另外一张表当中去"></a>需求一 读取myuser这张表当中的数据写入到HBase的另外一张表当中去</h3><h4 id="1-创建myuser2-表"><a href="#1-创建myuser2-表" class="headerlink" title="1 创建myuser2 表"></a>1 创建myuser2 表</h4><p>其中列簇名与myuser中列簇名一致</p>
<p>依赖:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">        &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hbase-client&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.0.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hbase-server&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.0.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;4.12&lt;/version&gt;</span><br><span class="line">            &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.testng&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;testng&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;6.14.3&lt;/version&gt;</span><br><span class="line">            &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-mapreduce --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hbase-mapreduce&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.0.0&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt; 2.7.5&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.7.5&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">    &lt;build&gt;</span><br><span class="line">        &lt;plugins&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;3.0&lt;/version&gt;</span><br><span class="line">                &lt;configuration&gt;</span><br><span class="line">                    &lt;source&gt;1.8&lt;/source&gt;</span><br><span class="line">                    &lt;target&gt;1.8&lt;/target&gt;</span><br><span class="line">                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;</span><br><span class="line">                    &lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;</span><br><span class="line">                &lt;/configuration&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">            &lt;!--将我们其他用到的一些jar包全部都打包进来  --&gt;</span><br><span class="line">            &lt;plugin&gt;</span><br><span class="line">                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">                &lt;version&gt;2.4.3&lt;/version&gt;</span><br><span class="line">                &lt;executions&gt;</span><br><span class="line">                    &lt;execution&gt;</span><br><span class="line">                        &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                        &lt;goals&gt;</span><br><span class="line">                            &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                        &lt;/goals&gt;</span><br><span class="line">                        &lt;configuration&gt;</span><br><span class="line">                            &lt;minimizeJar&gt;false&lt;/minimizeJar&gt;</span><br><span class="line">                        &lt;/configuration&gt;</span><br><span class="line">                    &lt;/execution&gt;</span><br><span class="line">                &lt;/executions&gt;</span><br><span class="line">            &lt;/plugin&gt;</span><br><span class="line">        &lt;/plugins&gt;</span><br><span class="line">    &lt;/build&gt;</span><br></pre></td></tr></table></figure>

<h4 id="定义mapper类"><a href="#定义mapper类" class="headerlink" title="定义mapper类"></a>定义mapper类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 负责读取myuser表当中的数据</span><br><span class="line"> * 如果mapper类需要读取hbase表数据，那么我们mapper类需要继承TableMapper这样的一个类</span><br><span class="line"> * 将key2   value2定义成 text  和put类型</span><br><span class="line"> * text里面装rowkey</span><br><span class="line"> * put装我们需要插入的数据</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">public class HBaseSourceMapper extends TableMapper&lt;Text,Put&gt; &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     *</span><br><span class="line">     * @param key  rowkey</span><br><span class="line">     * @param value  result对象，封装了我们一条条的数据</span><br><span class="line">     * @param context  上下文对象</span><br><span class="line">     * @throws IOException</span><br><span class="line">     * @throws InterruptedException</span><br><span class="line">     *</span><br><span class="line">     * 需求：读取myuser表当中f1列族下面的name和age列</span><br><span class="line">     *</span><br><span class="line">     ImmutableBytesWritable 封装了rowkey</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">       //获取到rowkey的字节数组</span><br><span class="line">        byte[] bytes = key.get();</span><br><span class="line">        String rowkey = Bytes.toString(bytes);</span><br><span class="line"></span><br><span class="line">        Put put = new Put(bytes);</span><br><span class="line"></span><br><span class="line">        //获取到所有的cell</span><br><span class="line">        List&lt;Cell&gt; cells = value.listCells();</span><br><span class="line">        for (Cell cell : cells) &#123;</span><br><span class="line">            //获取cell对应的列族</span><br><span class="line">            byte[] familyBytes = CellUtil.cloneFamily(cell);</span><br><span class="line">            //获取对应的列</span><br><span class="line">            byte[] qualifierBytes = CellUtil.cloneQualifier(cell);</span><br><span class="line">            //这里判断我们只需要f1列族，下面的name和age列</span><br><span class="line">            if(Bytes.toString(familyBytes).equals(&quot;f1&quot;) &amp;&amp; Bytes.toString(qualifierBytes).equals(&quot;name&quot;) ||  Bytes.toString(qualifierBytes).equals(&quot;age&quot;))&#123;</span><br><span class="line">                put.add(cell);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        //将数据写出去</span><br><span class="line">        if(!put.isEmpty())&#123;</span><br><span class="line">            context.write(new Text(rowkey),put);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="定义-reduce"><a href="#定义-reduce" class="headerlink" title="定义 reduce"></a>定义 reduce</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 负责将数据写入到myuser2</span><br><span class="line"> *</span><br><span class="line"> */</span><br><span class="line">public class HBaseSinkReducer extends TableReducer&lt;Text,Put,ImmutableBytesWritable&gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        for (Put put : values) &#123;</span><br><span class="line">            context.write(new ImmutableBytesWritable(key.toString().getBytes()),put);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="定义主类"><a href="#定义主类" class="headerlink" title="定义主类"></a>定义主类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.conf.Configured;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.client.Scan;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.util.Tool;</span><br><span class="line">import org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line">import javax.swing.plaf.nimbus.AbstractRegionPainter;</span><br><span class="line"></span><br><span class="line">public class HBaseMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;hbaseMR&quot;);</span><br><span class="line"></span><br><span class="line">        //打包运行，必须设置main方法所在的主类</span><br><span class="line">        job.setJarByClass(HBaseMain.class);</span><br><span class="line"></span><br><span class="line">        Scan scan = new Scan();</span><br><span class="line"></span><br><span class="line">        //定义我们的mapper类和reducer类</span><br><span class="line">        /**</span><br><span class="line">         * String table, Scan scan,</span><br><span class="line">         Class&lt;? extends TableMapper&gt; mapper,</span><br><span class="line">         Class&lt;?&gt; outputKeyClass,</span><br><span class="line">         Class&lt;?&gt; outputValueClass, Job job,</span><br><span class="line">         boolean addDependencyJars</span><br><span class="line">         */</span><br><span class="line">        TableMapReduceUtil.initTableMapperJob(&quot;myuser&quot;,scan,HBaseSourceMapper.class, Text.class, Put.class,job,false);</span><br><span class="line">        //使用工具类初始化reducer类</span><br><span class="line">        TableMapReduceUtil.initTableReducerJob(&quot;myuser2&quot;,HBaseSinkReducer.class,job);</span><br><span class="line">        boolean b = job.waitForCompletion(true);</span><br><span class="line">        return b?0:1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //程序入口类</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        //Configuration conf, Tool tool, String[] args</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(&quot;hbase.zookeeper.quorum&quot;,&quot;node01:2181,node02:2181,node03:2181&quot;);</span><br><span class="line">        int run = ToolRunner.run(configuration, new HBaseMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h4><p>1 本地运行</p>
<p>直接选中main方法所在的类，运行即可</p>
<p>2 打包集群运行</p>
<p>注意，我们需要使用打包插件，将HBase的依赖jar包都打入到工程jar包里面去</p>
<p>pom.xml当中添加打包插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.4.3&lt;/version&gt;</span><br><span class="line">            &lt;executions&gt;</span><br><span class="line">                &lt;execution&gt;</span><br><span class="line">                    &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">                    &lt;goals&gt;</span><br><span class="line">                        &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">                    &lt;/goals&gt;</span><br><span class="line">                    &lt;configuration&gt;</span><br><span class="line">                        &lt;minimizeJar&gt;true&lt;/minimizeJar&gt;</span><br><span class="line">                    &lt;/configuration&gt;</span><br><span class="line">                &lt;/execution&gt;</span><br><span class="line">            &lt;/executions&gt;</span><br><span class="line">        &lt;/plugin&gt;</span><br></pre></td></tr></table></figure>

<p>代码中添加:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setJarByClass(HBaseMain.class);</span><br></pre></td></tr></table></figure>

<p>使用maven打包</p>
<p>将jar包上传服务器:运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar hbaseStudy-1.0-SNAPSHOT.jar  cn.baidu.hbasemr.HBaseMR</span><br></pre></td></tr></table></figure>

<p>或者我们也可以自己设置我们的环境变量，然后运行original那个比较小的jar包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/export/servers/hadoop-2.7.5/</span><br><span class="line">export HBASE_HOME=/export/servers/hbase-2.0.0/</span><br><span class="line">export HADOOP_CLASSPATH=$&#123;HBASE_HOME&#125;/bin/hbase mapredcp</span><br><span class="line">yarn jar original-hbaseStudy-1.0-SNAPSHOT.jar  cn.baidu.hbasemr.HbaseMR</span><br></pre></td></tr></table></figure>

<h4 id="需求2-读取HDFS文件，写入到HBase表当中去"><a href="#需求2-读取HDFS文件，写入到HBase表当中去" class="headerlink" title="需求2 读取HDFS文件，写入到HBase表当中去"></a>需求2 读取HDFS文件，写入到HBase表当中去</h4><p>读取hdfs路径/hbase/input/user.txt，然后将数据写入到myuser2这张表当中去</p>
<p>准备数据:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /hbase/input</span><br><span class="line">cd /export/servers/</span><br><span class="line">vim user.txt</span><br><span class="line"> </span><br><span class="line"> 0007    zhangsan        18</span><br><span class="line">0008    lisi    25</span><br><span class="line">0009    wangwu  20</span><br></pre></td></tr></table></figure>

<p>上传hdfs:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put user.txt /hbase/input</span><br></pre></td></tr></table></figure>

<h4 id="定义mapper"><a href="#定义mapper" class="headerlink" title="定义mapper"></a>定义mapper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 通过这个mapper读取hdfs上面的文件，然后进行处理</span><br><span class="line"> */</span><br><span class="line">public class HDFSMapper extends Mapper&lt;LongWritable,Text,Text,NullWritable&gt;&#123;</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        //读取到数据之后不做任何处理，直接将数据写入到reduce里面去进行处理</span><br><span class="line">        context.write(value,NullWritable.get());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="定义reduce"><a href="#定义reduce" class="headerlink" title="定义reduce"></a>定义reduce</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class HBaseWriteReducer extends TableReducer&lt;Text,NullWritable,ImmutableBytesWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 0007    zhangsan        18</span><br><span class="line">     0008    lisi    25</span><br><span class="line">     0009    wangwu  20</span><br><span class="line"></span><br><span class="line">     * @param key</span><br><span class="line">     * @param values</span><br><span class="line">     * @param context</span><br><span class="line">     * @throws IOException</span><br><span class="line">     * @throws InterruptedException</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;NullWritable&gt; values, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        String[] split = key.toString().split(&quot;\t&quot;);</span><br><span class="line"></span><br><span class="line">        Put put = new Put(split[0].getBytes());</span><br><span class="line">        put.addColumn(&quot;f1&quot;.getBytes(),&quot;name&quot;.getBytes(),split[1].getBytes());</span><br><span class="line">        put.addColumn(&quot;f1&quot;.getBytes(),&quot;age&quot;.getBytes(),split[2].getBytes());</span><br><span class="line">        //将我们的数据写出去，key3是ImmutableBytesWritable，这个里面装的是rowkey</span><br><span class="line">        //然后将写出去的数据封装到put对象里面去了</span><br><span class="line">        context.write(new ImmutableBytesWritable(split[0].getBytes()),put);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="定义主类-1"><a href="#定义主类-1" class="headerlink" title="定义主类"></a>定义主类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.conf.Configured;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;</span><br><span class="line">import org.apache.hadoop.io.NullWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line">import org.apache.hadoop.util.Tool;</span><br><span class="line">import org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line">import javax.swing.plaf.nimbus.AbstractRegionPainter;</span><br><span class="line"></span><br><span class="line">public class HdfsHBaseMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        //获取job对象</span><br><span class="line">        Job job = Job.getInstance(super.getConf(), &quot;hdfs2Hbase&quot;);</span><br><span class="line"></span><br><span class="line">        //第一步：读取文件，解析成key，value对</span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,new Path(&quot;hdfs://node01:8020/hbase/input&quot;));</span><br><span class="line"></span><br><span class="line">        //第二步：自定义map逻辑，接受k1,v1，转换成为k2  v2进行输出</span><br><span class="line">        job.setMapperClass(HDFSMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        //分区，排序，规约，分组</span><br><span class="line"></span><br><span class="line">        //第七步：设置reduce类</span><br><span class="line">        TableMapReduceUtil.initTableReducerJob(&quot;myuser2&quot;,HBaseWriteReducer.class,job);</span><br><span class="line"></span><br><span class="line">        boolean b = job.waitForCompletion(true);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        return b?0:1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(&quot;hbase.zookeeper.quorum&quot;,&quot;node01:2181,node02:2181,node03:2181&quot;);</span><br><span class="line">        int run = ToolRunner.run(configuration, new HdfsHBaseMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="需求三-通过bulkload的方式批量加载数据到HBase当中去"><a href="#需求三-通过bulkload的方式批量加载数据到HBase当中去" class="headerlink" title="需求三 通过bulkload的方式批量加载数据到HBase当中去"></a>需求三 通过bulkload的方式批量加载数据到HBase当中去</h4><p>加载数据到HBase当中去的方式多种多样，我们可以使用HBase的javaAPI或者使用sqoop将我们的数据写入或者导入到HBase当中去，但是这些方式不是慢就是在导入的过程的占用Region资源导致效率低下，我们也可以通过MR的程序，将我们的数据直接转换成HBase的最终存储格式HFile，然后直接load数据到HBase当中去即可</p>
<p>HBase中每张Table在根目录（/HBase）下用一个文件夹存储，Table名为文件夹名，在Table文件夹下每个Region同样用一个文件夹存储，每个Region文件夹下的每个列族也用文件夹存储，而每个列族下存储的就是一些HFile文件，HFile就是HBase数据在HFDS下存储格式，所以HBase存储文件最终在hdfs上面的表现形式就是HFile，如果我们可以直接将数据转换为HFile的格式，那么我们的HBase就可以直接读取加载HFile格式的文件，就可以直接读取了</p>
<p>优点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.导入过程不占用Region资源 </span><br><span class="line">2.能快速导入海量的数据</span><br><span class="line">3.节省内存</span><br></pre></td></tr></table></figure>

<p>使用bulkload的方式将我们的数据直接生成HFile格式，然后直接加载到HBase的表当中去,不走hlog和hRegionServer.</p>
<p>例如:</p>
<p>将我们hdfs上面的这个路径/hbase/input/user.txt的数据文件，转换成HFile格式，然后load到myuser2这张表里面去</p>
<h4 id="定义mapper-1"><a href="#定义mapper-1" class="headerlink" title="定义mapper"></a>定义mapper</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">public class HDFSReadMapper  extends Mapper&lt;LongWritable,Text,ImmutableBytesWritable,Put&gt;&#123;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 0007    zhangsan        18</span><br><span class="line">     0008    lisi    25</span><br><span class="line">     0009    wangwu  20</span><br><span class="line"></span><br><span class="line">     * @param key</span><br><span class="line">     * @param value</span><br><span class="line">     * @param context</span><br><span class="line">     * @throws IOException</span><br><span class="line">     * @throws InterruptedException</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123;</span><br><span class="line">        String[] split = value.toString().split(&quot;\t&quot;);</span><br><span class="line"></span><br><span class="line">        Put put = new Put(split[0].getBytes());</span><br><span class="line">        put.addColumn(&quot;f1&quot;.getBytes(),&quot;name&quot;.getBytes(),split[1].getBytes());</span><br><span class="line">        put.addColumn(&quot;f1&quot;.getBytes(),&quot;age&quot;.getBytes(),split[2].getBytes());</span><br><span class="line"></span><br><span class="line">        context.write(new ImmutableBytesWritable(split[0].getBytes()),put);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="主类-程序入口"><a href="#主类-程序入口" class="headerlink" title="主类 程序入口"></a>主类 程序入口</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.conf.Configured;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.TableName;</span><br><span class="line">import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.apache.hadoop.hbase.client.Put;</span><br><span class="line">import org.apache.hadoop.hbase.client.Table;</span><br><span class="line">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2;</span><br><span class="line">import org.apache.hadoop.hdfs.DFSUtil;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line">import org.apache.hadoop.util.Tool;</span><br><span class="line">import org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line">public class BulkLoadMain extends Configured implements Tool &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public int run(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration conf = super.getConf();</span><br><span class="line">        //获取job对象</span><br><span class="line">        Job job = Job.getInstance(conf, &quot;bulkLoad&quot;);</span><br><span class="line">        Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(&quot;myuser2&quot;));</span><br><span class="line"></span><br><span class="line">        //读取文件</span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,new Path(&quot;hdfs://node01:8020/hbase/input&quot;));</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(HDFSReadMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">        job.setMapOutputValueClass(Put.class);</span><br><span class="line"></span><br><span class="line">        //将数据输出成为HFile格式</span><br><span class="line"></span><br><span class="line">        //Job job, Table table, RegionLocator regionLocator</span><br><span class="line">        //配置增量的添加数据</span><br><span class="line">        HFileOutputFormat2.configureIncrementalLoad(job,table,connection.getRegionLocator(TableName.valueOf(&quot;myuser2&quot;)));</span><br><span class="line">        //设置输出classs类，决定了我们输出数据格式</span><br><span class="line">        job.setOutputFormatClass(HFileOutputFormat2.class);</span><br><span class="line">        //设置输出路径</span><br><span class="line">        HFileOutputFormat2.setOutputPath(job,new Path(&quot;hdfs://node01:8020/hbase/hfile_out&quot;));</span><br><span class="line"></span><br><span class="line">        boolean b = job.waitForCompletion(true);</span><br><span class="line"></span><br><span class="line">        return b?0:1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(&quot;hbase.zookeeper.quorum&quot;,&quot;node01:2181,node02:2181,node03:2181&quot;);</span><br><span class="line">        int run = ToolRunner.run(configuration, new BulkLoadMain(), args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>打jar包上传运行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar original-hbaseStudy-1.0-SNAPSHOT.jar  cn.baidu.hbasemr.HBaseLoad</span><br></pre></td></tr></table></figure>

<h4 id="开发代码-加载数据"><a href="#开发代码-加载数据" class="headerlink" title="开发代码 加载数据"></a>开发代码 加载数据</h4><p>将我们的输出路径下面的HFile文件，加载到我们的hbase表当中去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line">import org.apache.hadoop.hbase.TableName;</span><br><span class="line">import org.apache.hadoop.hbase.client.Admin;</span><br><span class="line">import org.apache.hadoop.hbase.client.Connection;</span><br><span class="line">import org.apache.hadoop.hbase.client.ConnectionFactory;</span><br><span class="line">import org.apache.hadoop.hbase.client.Table;</span><br><span class="line">import org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles;</span><br><span class="line"></span><br><span class="line">public class LoadData &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);</span><br><span class="line">        configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;node01,node02,node03&quot;);</span><br><span class="line">        Connection connection =  ConnectionFactory.createConnection(configuration);</span><br><span class="line">        Admin admin = connection.getAdmin();</span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(&quot;myuser2&quot;));</span><br><span class="line">        LoadIncrementalHFiles load = new LoadIncrementalHFiles(configuration);</span><br><span class="line">        load.doBulkLoad(new Path(&quot;hdfs://node01:8020/hbase/hfile_out&quot;), admin,table,connection.getRegionLocator(TableName.valueOf(&quot;myuser2&quot;)));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>或者我们也可以通过命令行来进行加载数据</p>
<p>先将hbase的jar包添加到hadoop的classpath路径下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/export/servers/hbase-2.0.0/</span><br><span class="line">export HADOOP_HOME=/export/servers/hadoop-2.7.5/</span><br><span class="line">export HADOOP_CLASSPATH=$&#123;HBASE_HOME&#125;/bin/hbase mapredcp</span><br></pre></td></tr></table></figure>

<p>然后执行以下命令，将hbase的HFile直接导入到表myuser2当中来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn jar /export/servers/hbase-2.0.0/lib/hbase-server-1.2.0-cdh5.14.0.jar completebulkload /hbase/hfile_out myuser2</span><br></pre></td></tr></table></figure>

<p>##二 hive 与Hbase的对比</p>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><h3 id="数据仓库工具"><a href="#数据仓库工具" class="headerlink" title="数据仓库工具"></a>数据仓库工具</h3><p>Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。</p>
<h3 id="用于数据分析、清洗"><a href="#用于数据分析、清洗" class="headerlink" title="用于数据分析、清洗"></a>用于数据分析、清洗</h3><p>Hive适用于离线的数据分析和清洗，延迟较高</p>
<h3 id="基于HDFS、MapReduce"><a href="#基于HDFS、MapReduce" class="headerlink" title="基于HDFS、MapReduce"></a>基于HDFS、MapReduce</h3><p>Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。</p>
<h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><h3 id="nosql数据库"><a href="#nosql数据库" class="headerlink" title="nosql数据库"></a>nosql数据库</h3><p>是一种面向列存储的非关系型数据库。</p>
<h3 id="用于存储结构化和非结构话的数据"><a href="#用于存储结构化和非结构话的数据" class="headerlink" title="用于存储结构化和非结构话的数据"></a>用于存储结构化和非结构话的数据</h3><p>适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。</p>
<h3 id="基于HDFS"><a href="#基于HDFS" class="headerlink" title="基于HDFS"></a>基于HDFS</h3><p>数据持久化存储的体现形式是Hfile，存放于DataNode中，被ResionServer以region的形式进行管理。</p>
<h3 id="延迟较低，接入在线业务使用"><a href="#延迟较低，接入在线业务使用" class="headerlink" title="延迟较低，接入在线业务使用"></a>延迟较低，接入在线业务使用</h3><p>面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。</p>
<h3 id="总结：Hive与HBase"><a href="#总结：Hive与HBase" class="headerlink" title="总结：Hive与HBase"></a>总结：Hive与HBase</h3><p>Hive和Hbase是两种基于Hadoop的不同技术，Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，或者从HBase写回Hive。</p>
<p>##三 hive 与hbase的整合</p>
<p>hive与我们的HBase各有千秋，各自有着不同的功能，但是归根接地，hive与hbase的数据最终都是存储在hdfs上面的，一般的我们为了存储磁盘的空间，不会将一份数据存储到多个地方，导致磁盘空间的浪费，我们可以直接将数据存入hbase，然后通过hive整合hbase直接使用sql语句分析hbase里面的数据即可，非常方便</p>
<h3 id="需求一将hive分析结果的数据，保存到HBase当中去"><a href="#需求一将hive分析结果的数据，保存到HBase当中去" class="headerlink" title="需求一将hive分析结果的数据，保存到HBase当中去"></a>需求一将hive分析结果的数据，保存到HBase当中去</h3><h4 id="1-拷贝hbase的五个依赖jar包到hive的lib目录下"><a href="#1-拷贝hbase的五个依赖jar包到hive的lib目录下" class="headerlink" title="1 拷贝hbase的五个依赖jar包到hive的lib目录下"></a>1 拷贝hbase的五个依赖jar包到hive的lib目录下</h4><p>将我们HBase的五个jar包拷贝到hive的lib目录下</p>
<p>hbase的jar包都在/export/servers/hbase-2.0.0/lib</p>
<p>我们需要拷贝五个jar包名字如下</p>
<p>hbase-client-2.0.0.jar</p>
<p>hbase-hadoop2-compat-2.0.0.jar</p>
<p>hbase-hadoop-compat-2.0.0.jar</p>
<p>hbase-it-2.0.0.jar</p>
<p>hbase-server-2.0.0.jar</p>
<p>我们直接在node03执行以下命令，通过创建软连接的方式来进行jar包的依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-client-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-client-2.0.0.jar</span><br><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-hadoop2-compat-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-hadoop2-compat-2.0.0.jar</span><br><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-hadoop-compat-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-hadoop-compat-2.0.0.jar</span><br><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-it-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-it-2.0.0.jar</span><br><span class="line">ln -s /export/servers/hbase-2.0.0/lib/hbase-server-2.0.0.jar /export/servers/apache-hive-2.1.0-bin/lib/hbase-server-2.0.0.jar</span><br></pre></td></tr></table></figure>

<h4 id="2-修改hive的配置文件"><a href="#2-修改hive的配置文件" class="headerlink" title="2 修改hive的配置文件"></a>2 修改hive的配置文件</h4><p>编辑node03服务器上面的hive的配置文件hive-site.xml添加以下两行配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;node01,node02,node03&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;node01,node02,node03&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h4 id="3-修改hive-env-sh配置文件添加以下配置"><a href="#3-修改hive-env-sh配置文件添加以下配置" class="headerlink" title="3 修改hive-env.sh配置文件添加以下配置"></a>3 修改hive-env.sh配置文件添加以下配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/export/servers/hadoop-2.7.5</span><br><span class="line">export HBASE_HOME=/export/servers/hbase-2.0.0</span><br><span class="line">export HIVE_CONF_DIR=/export/servers/apache-hive-2.1.0-bin/conf</span><br></pre></td></tr></table></figure>

<h4 id="4-hive当中建表并加载以下数据"><a href="#4-hive当中建表并加载以下数据" class="headerlink" title="4 hive当中建表并加载以下数据"></a>4 hive当中建表并加载以下数据</h4><h4 id="hive当中建表"><a href="#hive当中建表" class="headerlink" title="hive当中建表"></a>hive当中建表</h4><p>进入hive客户端</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive</span><br></pre></td></tr></table></figure>

<p>创建hive数据库与hive对应的数据库表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">create database course;</span><br><span class="line">use course;</span><br><span class="line">create external table if not exists course.score(id int,cname string,score int) row format delimited fields terminated by &apos;\t&apos; stored as textfile;</span><br></pre></td></tr></table></figure>

<h4 id="准备数据内容如下"><a href="#准备数据内容如下" class="headerlink" title="准备数据内容如下"></a>准备数据内容如下</h4><p>node03执行以下命令，准备数据文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim hive-hbase.txt</span><br><span class="line"></span><br><span class="line">1       zhangsan        80</span><br><span class="line">2       lisi    60</span><br><span class="line">3       wangwu  30</span><br><span class="line">4       zhaoliu 70</span><br></pre></td></tr></table></figure>

<h4 id="进行加载数据"><a href="#进行加载数据" class="headerlink" title="进行加载数据"></a>进行加载数据</h4><p>进入hive客户端进行加载数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (course)&gt; load data local inpath &apos;/export/hive-hbase.txt&apos; into table score;</span><br><span class="line">hive (course)&gt; select * from score;</span><br></pre></td></tr></table></figure>

<h4 id="5-创建hive管理表与HBase进行映射"><a href="#5-创建hive管理表与HBase进行映射" class="headerlink" title="5 创建hive管理表与HBase进行映射"></a>5 创建hive管理表与HBase进行映射</h4><p>我们可以创建一个hive的管理表与hbase当中的表进行映射，hive管理表当中的数据，都会存储到hbase上面去</p>
<p>hive当中创建内部表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table course.hbase_score(id int,cname string,score int)  </span><br><span class="line">stored by &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;  </span><br><span class="line">with serdeproperties(&quot;hbase.columns.mapping&quot; = &quot;cf:name,cf:score&quot;) </span><br><span class="line">tblproperties(&quot;hbase.table.name&quot; = &quot;hbase_score&quot;);</span><br></pre></td></tr></table></figure>

<p>通过insert overwrite select 插入数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table course.hbase_score select id,cname,score from course.score;</span><br></pre></td></tr></table></figure>

<h4 id="6-hbase当中查看表hbase-score"><a href="#6-hbase当中查看表hbase-score" class="headerlink" title="6 hbase当中查看表hbase_score"></a>6 hbase当中查看表hbase_score</h4><p>进入hbase的客户端查看表hbase_score，并查看当中的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):023:0&gt; list</span><br><span class="line">TABLE                                                                                       </span><br><span class="line">hbase_score                                                                                 </span><br><span class="line">myuser                                                                                      </span><br><span class="line">myuser2                                                                                     </span><br><span class="line">student                                                                                     </span><br><span class="line">user                                                                                        </span><br><span class="line">5 row(s) in 0.0210 seconds</span><br><span class="line"></span><br><span class="line">=&gt; [&quot;hbase_score&quot;, &quot;myuser&quot;, &quot;myuser2&quot;, &quot;student&quot;, &quot;user&quot;]</span><br><span class="line">hbase(main):024:0&gt; scan &apos;hbase_score&apos;</span><br><span class="line">ROW                      COLUMN+CELL                                                        </span><br><span class="line"> 1                       column=cf:name, timestamp=1550628395266, value=zhangsan            </span><br><span class="line"> 1                       column=cf:score, timestamp=1550628395266, value=80                 </span><br><span class="line"> 2                       column=cf:name, timestamp=1550628395266, value=lisi                </span><br><span class="line"> 2                       column=cf:score, timestamp=1550628395266, value=60                 </span><br><span class="line"> 3                       column=cf:name, timestamp=1550628395266, value=wangwu              </span><br><span class="line"> 3                       column=cf:score, timestamp=1550628395266, value=30                 </span><br><span class="line"> 4                       column=cf:name, timestamp=1550628395266, value=zhaoliu             </span><br><span class="line"> 4                       column=cf:score, timestamp=1550628395266, value=70                 </span><br><span class="line">4 row(s) in 0.0360 seconds</span><br></pre></td></tr></table></figure>

<h3 id="需求二创建hive外部表，映射HBase当中已有的表模型，"><a href="#需求二创建hive外部表，映射HBase当中已有的表模型，" class="headerlink" title="需求二创建hive外部表，映射HBase当中已有的表模型，"></a>需求二创建hive外部表，映射HBase当中已有的表模型，</h3><h3 id="第一步：HBase当中创建表并手动插入加载一些数据"><a href="#第一步：HBase当中创建表并手动插入加载一些数据" class="headerlink" title="第一步：HBase当中创建表并手动插入加载一些数据"></a>第一步：HBase当中创建表并手动插入加载一些数据</h3><p>进入HBase的shell客户端，手动创建一张表，并插入加载一些数据进去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create &apos;hbase_hive_score&apos;,&#123; NAME =&gt;&apos;cf&apos;&#125;</span><br><span class="line">put &apos;hbase_hive_score&apos;,&apos;1&apos;,&apos;cf:name&apos;,&apos;zhangsan&apos;</span><br><span class="line">put &apos;hbase_hive_score&apos;,&apos;1&apos;,&apos;cf:score&apos;, &apos;95&apos;</span><br><span class="line">put &apos;hbase_hive_score&apos;,&apos;2&apos;,&apos;cf:name&apos;,&apos;lisi&apos;</span><br><span class="line">put &apos;hbase_hive_score&apos;,&apos;2&apos;,&apos;cf:score&apos;, &apos;96&apos;</span><br><span class="line">put &apos;hbase_hive_score&apos;,&apos;3&apos;,&apos;cf:name&apos;,&apos;wangwu&apos;</span><br><span class="line">put &apos;hbase_hive_score&apos;,&apos;3&apos;,&apos;cf:score&apos;, &apos;97&apos;</span><br></pre></td></tr></table></figure>

<p>操作成功结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):049:0&gt; create &apos;hbase_hive_score&apos;,&#123; NAME =&gt;&apos;cf&apos;&#125;</span><br><span class="line">0 row(s) in 1.2970 seconds</span><br><span class="line"></span><br><span class="line">=&gt; Hbase::Table - hbase_hive_score</span><br><span class="line">hbase(main):050:0&gt; put &apos;hbase_hive_score&apos;,&apos;1&apos;,&apos;cf:name&apos;,&apos;zhangsan&apos;</span><br><span class="line">0 row(s) in 0.0600 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):051:0&gt; put &apos;hbase_hive_score&apos;,&apos;1&apos;,&apos;cf:score&apos;, &apos;95&apos;</span><br><span class="line">0 row(s) in 0.0310 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):052:0&gt; put &apos;hbase_hive_score&apos;,&apos;2&apos;,&apos;cf:name&apos;,&apos;lisi&apos;</span><br><span class="line">0 row(s) in 0.0230 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):053:0&gt; put &apos;hbase_hive_score&apos;,&apos;2&apos;,&apos;cf:score&apos;, &apos;96&apos;</span><br><span class="line">0 row(s) in 0.0220 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):054:0&gt; put &apos;hbase_hive_score&apos;,&apos;3&apos;,&apos;cf:name&apos;,&apos;wangwu&apos;</span><br><span class="line">0 row(s) in 0.0200 seconds</span><br><span class="line"></span><br><span class="line">hbase(main):055:0&gt; put &apos;hbase_hive_score&apos;,&apos;3&apos;,&apos;cf:score&apos;, &apos;97&apos;</span><br><span class="line">0 row(s) in 0.0250 seconds</span><br></pre></td></tr></table></figure>

<h3 id="第二步：建立hive的外部表，映射HBase当中的表以及字段"><a href="#第二步：建立hive的外部表，映射HBase当中的表以及字段" class="headerlink" title="第二步：建立hive的外部表，映射HBase当中的表以及字段"></a>第二步：建立hive的外部表，映射HBase当中的表以及字段</h3><p>在hive当中建立外部表，</p>
<p>进入hive客户端，然后执行以下命令进行创建hive外部表，就可以实现映射HBase当中的表数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE external TABLE course.hbase2hive(id int, name string, score int) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:name,cf:score&quot;) TBLPROPERTIES(&quot;hbase.table.name&quot; =&quot;hbase_hive_score&quot;);</span><br></pre></td></tr></table></figure>

<h2 id="四-hbase预分区"><a href="#四-hbase预分区" class="headerlink" title="四 hbase预分区"></a>四 hbase预分区</h2><h2 id="1、为何要预分区？"><a href="#1、为何要预分区？" class="headerlink" title="1、为何要预分区？"></a>1、为何要预分区？</h2><p>* 增加数据读写效率</p>
<p>* 负载均衡，防止数据倾斜</p>
<p>* 方便集群容灾调度region</p>
<p>* 优化Map数量</p>
<h2 id="2、如何预分区？"><a href="#2、如何预分区？" class="headerlink" title="2、如何预分区？"></a>2、如何预分区？</h2><p>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。</p>
<h2 id="3、如何设定预分区？"><a href="#3、如何设定预分区？" class="headerlink" title="3、如何设定预分区？"></a>3、如何设定预分区？</h2><h3 id="1、手动指定预分区"><a href="#1、手动指定预分区" class="headerlink" title="1、手动指定预分区"></a>1、手动指定预分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &apos;staff&apos;,&apos;info&apos;,&apos;partition1&apos;,SPLITS =&gt; [&apos;1000&apos;,&apos;2000&apos;,&apos;3000&apos;,&apos;4000&apos;]</span><br></pre></td></tr></table></figure>

<h3 id="2、使用16进制算法生成预分区"><a href="#2、使用16进制算法生成预分区" class="headerlink" title="2、使用16进制算法生成预分区"></a>2、使用16进制算法生成预分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):003:0&gt; create &apos;staff2&apos;,&apos;info&apos;,&apos;partition2&apos;,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; &apos;HexStringSplit&apos;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3、使用JavaAPI创建预分区"><a href="#3、使用JavaAPI创建预分区" class="headerlink" title="3、使用JavaAPI创建预分区"></a>3、使用JavaAPI创建预分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">同 hbase上篇</span><br></pre></td></tr></table></figure>

<h2 id="五-HBase的rowKey设计技巧"><a href="#五-HBase的rowKey设计技巧" class="headerlink" title="五 HBase的rowKey设计技巧"></a>五 HBase的rowKey设计技巧</h2><p>HBase是三维有序存储的，通过rowkey（行键），column key（column family和qualifier）和TimeStamp（时间戳）这个三个维度可以对HBase中的数据进行快速定位。</p>
<p>HBase中rowkey可以唯一标识一行记录，在HBase查询的时候，有以下几种方式：</p>
<ol>
<li>通过get方式，指定rowkey获取唯一一条记录</li>
<li>通过scan方式，设置startRow和stopRow参数进行范围匹配</li>
<li>全表扫描，即直接扫描整张表中所有行记录</li>
</ol>
<h3 id="1-rowkey长度原则"><a href="#1-rowkey长度原则" class="headerlink" title="1 rowkey长度原则"></a>1 rowkey长度原则</h3><p>rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes，以byte[]形式保存，一般设计成定长。</p>
<p>建议越短越好，不要超过16个字节，原因如下：</p>
<p>v 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率；</p>
<p>v MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。</p>
<h3 id="2-rowkey散列原则"><a href="#2-rowkey散列原则" class="headerlink" title="2 rowkey散列原则"></a>2 rowkey散列原则</h3><p>如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。</p>
<h3 id="3-rowkey唯一原则"><a href="#3-rowkey唯一原则" class="headerlink" title="3 rowkey唯一原则"></a>3 rowkey唯一原则</h3><p>必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。</p>
<h3 id="4什么是热点"><a href="#4什么是热点" class="headerlink" title="4什么是热点"></a>4什么是热点</h3><p>HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。</p>
<p>热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。</p>
<p>设计良好的数据访问模式以使集群被充分，均衡的利用。为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。下面是一些常见的避免热点的方法以及它们的优缺点：</p>
<h4 id="1加盐"><a href="#1加盐" class="headerlink" title="1加盐"></a>1加盐</h4><p>这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。</p>
<h4 id="2哈希"><a href="#2哈希" class="headerlink" title="2哈希"></a>2哈希</h4><p>哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。</p>
<h4 id="3反转"><a href="#3反转" class="headerlink" title="3反转"></a>3反转</h4><p>第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。</p>
<p>反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题</p>
<h4 id="3时间戳反转"><a href="#3时间戳反转" class="headerlink" title="3时间戳反转"></a>3时间戳反转</h4><p>一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到key的末尾，例如 [key][reverse_timestamp] , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。</p>
<p>其他一些建议：</p>
<p>尽量减少行键和列族的大小在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，这个时候它们将会占用大量的存储空间。</p>
<p>列族尽可能越短越好，最好是一个字符。</p>
<p>冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好。</p>
<h2 id="六-Hbase的协处理器"><a href="#六-Hbase的协处理器" class="headerlink" title="六 Hbase的协处理器"></a>六 Hbase的协处理器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://hbase.apache.org/book.html#cp</span><br></pre></td></tr></table></figure>

<p>1、 起源 Hbase 作为列族数据库最经常被人诟病的特性包括：无法轻易建立“二级索引”，难以执 行求和、计数、排序等操作。比如，在旧版本的(&lt;0.92)Hbase 中，统计数据表的总行数，需 要使用 Counter 方法，执行一次 MapReduce Job 才能得到。虽然 HBase 在数据存储层中集成 了 MapReduce，能够有效用于数据表的分布式计算。然而在很多情况下，做一些简单的相 加或者聚合计算的时候， 如果直接将计算过程放置在 server 端，能够减少通讯开销，从而获 得很好的性能提升。于是， HBase 在 0.92 之后引入了协处理器(coprocessors)，实现一些激动 人心的新特性：能够轻易建立二次索引、复杂过滤器(谓词下推)以及访问控制等。</p>
<h2 id="2、协处理器有两种：-observer-和-endpoint"><a href="#2、协处理器有两种：-observer-和-endpoint" class="headerlink" title="2、协处理器有两种： observer 和 endpoint"></a>2、协处理器有两种： observer 和 endpoint</h2><p>(1) Observer 类似于传统数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。Observer Coprocessor 就是一些散布在 HBase Server 端代码中的 hook 钩子， 在固定的事件发生时被调用。比如： put 操作之前有钩子函数 prePut，该函数在 put 操作<br>执行前会被 Region Server 调用；在 put 操作之后则有 postPut 钩子函数</p>
<p>以 Hbase2.0.0 版本为例，它提供了三种观察者接口：<br>● RegionObserver：提供客户端的数据操纵事件钩子： Get、 Put、 Delete、 Scan 等。<br>● WALObserver：提供 WAL 相关操作钩子。<br>● MasterObserver：提供 DDL-类型的操作钩子。如创建、删除、修改数据表等。<br>到 0.96 版本又新增一个 RegionServerObserver</p>
<p>下图是以 RegionObserver 为例子讲解 Observer 这种协处理器的原理：</p>
<p><a href="https://manzhong.github.io/images/Hbase/xcl.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/Hbase/xcl.png" alt="img"></a></p>
<p>(2) Endpoint 协处理器类似传统数据库中的存储过程，客户端可以调用这些 Endpoint 协处 理器执行一段 Server 端代码，并将 Server 端代码的结果返回给客户端进一步处理，最常 见的用法就是进行聚集操作。如果没有协处理器，当用户需要找出一张表中的最大数据，即</p>
<p>max 聚合操作，就必须进行全表扫描，在客户端代码内遍历扫描结果，并执行求最大值的 操作。这样的方法无法利用底层集群的并发能力，而将所有计算都集中到 Client 端统一执 行，势必效率低下。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，<br>HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内 执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出，仅仅将该 max 值返回给客户端。在客户端进一步将多个 Region 的最大值进一步处理而找到其中的最大值。<br>这样整体的执行效率就会提高很多<br>下图是 EndPoint 的工作原理：</p>
<p><a href="https://manzhong.github.io/images/Hbase/xcl2.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/Hbase/xcl2.png" alt="img"></a></p>
<p>(3)总结</p>
<p>Observer 允许集群在正常的客户端操作过程中可以有不同的行为表现<br>Endpoint 允许扩展集群的能力，对客户端应用开放新的运算命令<br>observer 类似于 RDBMS 中的触发器，主要在服务端工作<br>endpoint 类似于 RDBMS 中的存储过程，主要在 client 端工作<br>observer 可以实现权限管理、优先级设置、监控、 ddl 控制、 二级索引等功能<br>endpoint 可以实现 min、 max、 avg、 sum、 distinct、 group by 等功能</p>
<h2 id="3、协处理器加载方式"><a href="#3、协处理器加载方式" class="headerlink" title="3、协处理器加载方式"></a>3、协处理器加载方式</h2><p> 协处理器的加载方式有两种，我们称之为静态加载方式（ Static Load） 和动态加载方式 （ Dynamic Load）。 静态加载的协处理器称之为 System Coprocessor，动态加载的协处理器称 之为 Table Coprocessor<br>​ 1、静态加载</p>
<p>通过修改 hbase-site.xml 这个文件来实现， 启动全局 aggregation，能过操纵所有的表上 的数据。只需要添加如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;hbase.coprocessor.user.region.classes&lt;/name&gt;</span><br><span class="line">&lt;value&gt;org.apache.hadoop.hbase.coprocessor.AggregateImplementation&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>为所有 table 加载了一个 cp class，可以用” ,”分割加载多个 class</p>
<p>2、动态加载</p>
<p>启用表 aggregation，只对特定的表生效。通过 HBase Shell 来实现。<br>disable 指定表。 hbase&gt; disable ‘mytable’<br>添加 aggregation<br>hbase&gt; alter ‘mytable’, METHOD =&gt; ‘table_att’,’coprocessor’=&gt;<br>‘|org.apache.Hadoop.hbase.coprocessor.AggregateImplementation||’<br>重启指定表 hbase&gt; enable ‘mytable’</p>
<p>协处理器卸载</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">三步</span><br><span class="line">disable &apos;test&apos;</span><br><span class="line">alter &apos;test&apos;,METHOD=&gt;&apos;table_att_unset&apos;,NAME=&gt;&apos;coprocessor$1&apos;</span><br><span class="line">enable &apos;test&apos;</span><br></pre></td></tr></table></figure>

<h2 id="4、协处理器Observer应用实战"><a href="#4、协处理器Observer应用实战" class="headerlink" title="4、协处理器Observer应用实战"></a>4、协处理器Observer应用实战</h2><p>通过协处理器Observer实现hbase当中一张表插入数据，然后通过协处理器，将数据复制一份保存到另外一张表当中去，但是只取当第一张表当中的部分列数据保存到第二张表当中去</p>
<h3 id="第一步：HBase当中创建第一张表proc1"><a href="#第一步：HBase当中创建第一张表proc1" class="headerlink" title="第一步：HBase当中创建第一张表proc1"></a>第一步：HBase当中创建第一张表proc1</h3><p>在HBase当中创建一张表，表名user2，并只有一个列族info</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers/hbase-2.0.0/</span><br><span class="line">bin/hbase shell</span><br><span class="line">hbase(main):053:0&gt; create &apos;proc1&apos;,&apos;info&apos;</span><br></pre></td></tr></table></figure>

<h3 id="第二步：Hbase当中创建第二张表proc2"><a href="#第二步：Hbase当中创建第二张表proc2" class="headerlink" title="第二步：Hbase当中创建第二张表proc2"></a>第二步：Hbase当中创建第二张表proc2</h3><p>创建第二张表’proc2，作为目标表，将第一张表当中插入数据的部分列，使用协处理器，复制到’proc2表当中来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):054:0&gt; create &apos;proc2&apos;,&apos;info&apos;</span><br></pre></td></tr></table></figure>

<h3 id="第三步：开发HBase的协处理器"><a href="#第三步：开发HBase的协处理器" class="headerlink" title="第三步：开发HBase的协处理器"></a>第三步：开发HBase的协处理器</h3><p>开发HBase的协处理器Copo</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.hbase.*;</span><br><span class="line">import org.apache.hadoop.hbase.client.*;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.ObserverContext;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;</span><br><span class="line">import org.apache.hadoop.hbase.coprocessor.RegionObserver;</span><br><span class="line">import org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line">import org.apache.hadoop.hbase.wal.WALEdit;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Optional;</span><br><span class="line"></span><br><span class="line">public class MyProcessor implements RegionObserver,RegionCoprocessor &#123;</span><br><span class="line"></span><br><span class="line">    static Connection connection = null;</span><br><span class="line">    static Table table = null;</span><br><span class="line">    //使用静态代码块来创建连接对象，避免频繁的创建连接对象</span><br><span class="line">    static&#123;</span><br><span class="line">        Configuration conf = HBaseConfiguration.create();</span><br><span class="line">        conf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;node01:2181&quot;);</span><br><span class="line">        try &#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">            table = connection.getTable(TableName.valueOf(&quot;proc2&quot;));</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    private RegionCoprocessorEnvironment env = null;</span><br><span class="line">    //定义列族名</span><br><span class="line">    private static final String FAMAILLY_NAME = &quot;info&quot;;</span><br><span class="line">    //定义列名</span><br><span class="line">    private static final String QUALIFIER_NAME = &quot;name&quot;;</span><br><span class="line">    //2.0加入该方法，否则无法生效</span><br><span class="line">    @Override</span><br><span class="line">    public Optional&lt;RegionObserver&gt; getRegionObserver() &#123;</span><br><span class="line">        // Extremely important to be sure that the coprocessor is invoked as a RegionObserver</span><br><span class="line">        return Optional.of(this);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * 初始化协处理器环境</span><br><span class="line">     * @param e</span><br><span class="line">     * @throws IOException</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public void start(CoprocessorEnvironment e) throws IOException &#123;</span><br><span class="line">        env = (RegionCoprocessorEnvironment) e;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void stop(CoprocessorEnvironment e) throws IOException &#123;</span><br><span class="line">        // nothing to do here</span><br><span class="line">    &#125;</span><br><span class="line">    /**</span><br><span class="line">     * 覆写prePut方法，在我们数据插入之前进行拦截，</span><br><span class="line">     * @param e</span><br><span class="line">     * @param put  put对象里面封装了我们需要插入到目标表的数据</span><br><span class="line">     * @param edit</span><br><span class="line">     * @param durability</span><br><span class="line">     * @throws IOException</span><br><span class="line">     */</span><br><span class="line">    @Override</span><br><span class="line">    public void prePut(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; e,</span><br><span class="line">                       final Put put, final WALEdit edit, final Durability durability)</span><br><span class="line">            throws IOException &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">            //通过put对象获取插入数据的rowkey</span><br><span class="line">            byte[] rowBytes = put.getRow();</span><br><span class="line">            String rowkey = Bytes.toString(rowBytes);</span><br><span class="line">            //获取我们插入数据的name字段的值</span><br><span class="line"></span><br><span class="line">            List&lt;Cell&gt; list = put.get(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(QUALIFIER_NAME));</span><br><span class="line">            //判断如果没有获取到info列族，和name列，直接返回即可</span><br><span class="line">            if (list == null || list.size() == 0) &#123;</span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line">            //获取到info列族，name列对应的cell</span><br><span class="line">            Cell cell2 = list.get(0);</span><br><span class="line"></span><br><span class="line">            //通过cell获取数据值</span><br><span class="line">            String nameValue = Bytes.toString(CellUtil.cloneValue(cell2));</span><br><span class="line">            //创建put对象，将数据插入到proc2表里面去</span><br><span class="line">            Put put2 = new Put(rowkey.getBytes());</span><br><span class="line">            put2.addColumn(Bytes.toBytes(FAMAILLY_NAME), Bytes.toBytes(QUALIFIER_NAME),  nameValue.getBytes());</span><br><span class="line">            table.put(put2);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; catch (Exception e1) &#123;</span><br><span class="line">            return ;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="第四步：将项目打成jar包，并上传到HDFS上面"><a href="#第四步：将项目打成jar包，并上传到HDFS上面" class="headerlink" title="第四步：将项目打成jar包，并上传到HDFS上面"></a>第四步：将项目打成jar包，并上传到HDFS上面</h3><p>将我们的协处理器打成一个jar包，此处不需要用任何的打包插件即可，然后上传到hdfs</p>
<p>将打好的jar包上传到linux的/export/servers路径下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/servers</span><br><span class="line">mv original-hbase-1.0-SNAPSHOT.jar  processor.jar</span><br><span class="line">hdfs dfs -mkdir -p /processor</span><br><span class="line">hdfs dfs -put processor.jar /processor</span><br></pre></td></tr></table></figure>

<h3 id="第五步：将打好的jar包挂载到proc1表当中去"><a href="#第五步：将打好的jar包挂载到proc1表当中去" class="headerlink" title="第五步：将打好的jar包挂载到proc1表当中去"></a>第五步：将打好的jar包挂载到proc1表当中去</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):056:0&gt; describe &apos;proc1&apos;</span><br><span class="line">hbase(main):055:0&gt; alter &apos;proc1&apos;,METHOD =&gt; &apos;table_att&apos;,&apos;Coprocessor&apos;=&gt;&apos;hdfs://node01:8020/processor/processor.jar|cn.itcast.hbasemr.demo4.MyProcessor|1001|&apos;</span><br></pre></td></tr></table></figure>

<p>再次查看’proc1’表，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):043:0&gt; describe &apos;proc1&apos;</span><br></pre></td></tr></table></figure>

<p>可以查看到我们的卸载器已经加载了</p>
<h3 id="第六步：proc1表当中添加数据"><a href="#第六步：proc1表当中添加数据" class="headerlink" title="第六步：proc1表当中添加数据"></a>第六步：proc1表当中添加数据</h3><p>进入hbase-shell客户端，然后直接执行以下命令向proc1表当中添加数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">put &apos;proc1&apos;,&apos;0001&apos;,&apos;info:name&apos;,&apos;zhangsan&apos;</span><br><span class="line">put &apos;proc1&apos;,&apos;0001&apos;,&apos;info:age&apos;,&apos;28&apos;</span><br><span class="line">put &apos;proc1&apos;,&apos;0002&apos;,&apos;info:name&apos;,&apos;lisi&apos;</span><br><span class="line">put &apos;proc1&apos;,&apos;0002&apos;,&apos;info:age&apos;,&apos;25&apos;</span><br></pre></td></tr></table></figure>

<p>向proc1表当中添加数据，然后通过</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan  &apos;proc2&apos;</span><br></pre></td></tr></table></figure>

<p>我们会发现，proc2表当中也插入了数据，并且只有info列族，name列</p>
<p> 注意：如果需要卸载我们的协处理器，那么进入hbase的shell命令行，执行以下命令即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">disable &apos;proc1&apos;</span><br><span class="line">alter &apos;proc1&apos;,METHOD=&gt;&apos;table_att_unset&apos;,NAME=&gt;&apos;coprocessor$1&apos;</span><br><span class="line">enable &apos;proc1&apos;</span><br></pre></td></tr></table></figure>

<h2 id="七-HBase当中的二级索引的基本介绍"><a href="#七-HBase当中的二级索引的基本介绍" class="headerlink" title="七 HBase当中的二级索引的基本介绍"></a>七 HBase当中的二级索引的基本介绍</h2><p>由于HBase的查询比较弱，如果需要实现类似于 select name,salary,count(1),max(salary) from user group by name,salary order by salary 等这样的复杂性的统计需求，基本上不可能，或者说比较困难，所以我们在使用HBase的时候，一般都会借助二级索引的方案来进行实现</p>
<p>HBase的一级索引就是rowkey，我们只能通过rowkey进行检索。如果我们相对hbase里面列族的列列进行一些组合查询，就需要采用HBase的二级索引方案来进行多条件的查询。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\1. MapReduce方案 </span><br><span class="line">\2. ITHBASE（Indexed-Transanctional HBase）方案 </span><br><span class="line">\3. IHBASE（Index HBase）方案 </span><br><span class="line">\4. Hbase Coprocessor(协处理器)方案 </span><br><span class="line">\5. Solr+hbase方案</span><br><span class="line">\6. CCIndex（complementalclustering index）方案</span><br><span class="line">还有 MySQL 等数据库</span><br><span class="line">常见的二级索引我们一般可以借助各种其他的方式来实现，例如Phoenix或者solr或者ES等</span><br></pre></td></tr></table></figure>

<h2 id="八-HBase调优"><a href="#八-HBase调优" class="headerlink" title="八 HBase调优"></a>八 HBase调优</h2><h2 id="1、通用优化"><a href="#1、通用优化" class="headerlink" title="1、通用优化"></a>1、通用优化</h2><h3 id="1、NameNode的元数据备份使用SSD"><a href="#1、NameNode的元数据备份使用SSD" class="headerlink" title="1、NameNode的元数据备份使用SSD"></a>1、NameNode的元数据备份使用SSD</h3><h3 id="2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5-10分钟备份一次。备份可以通过定时任务复制元数据目录即可。"><a href="#2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5-10分钟备份一次。备份可以通过定时任务复制元数据目录即可。" class="headerlink" title="2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5~10分钟备份一次。备份可以通过定时任务复制元数据目录即可。"></a>2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5~10分钟备份一次。备份可以通过定时任务复制元数据目录即可。</h3><h3 id="3、为NameNode指定多个元数据目录，使用dfs-name-dir或者dfs-namenode-name-dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。"><a href="#3、为NameNode指定多个元数据目录，使用dfs-name-dir或者dfs-namenode-name-dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。" class="headerlink" title="3、为NameNode指定多个元数据目录，使用dfs.name.dir或者dfs.namenode.name.dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。"></a>3、为NameNode指定多个元数据目录，使用dfs.name.dir或者dfs.namenode.name.dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。</h3><h3 id="4、设置dfs-namenode-name-dir-restore为true，允许尝试恢复之前失败的dfs-namenode-name-dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。"><a href="#4、设置dfs-namenode-name-dir-restore为true，允许尝试恢复之前失败的dfs-namenode-name-dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。" class="headerlink" title="4、设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。"></a>4、设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。</h3><h3 id="5、NameNode节点必须配置为RAID1（镜像盘）结构。"><a href="#5、NameNode节点必须配置为RAID1（镜像盘）结构。" class="headerlink" title="5、NameNode节点必须配置为RAID1（镜像盘）结构。"></a>5、NameNode节点必须配置为RAID1（镜像盘）结构。</h3><h3 id="6、补充：什么是Raid0、Raid0-1、Raid1、Raid5"><a href="#6、补充：什么是Raid0、Raid0-1、Raid1、Raid5" class="headerlink" title="6、补充：什么是Raid0、Raid0+1、Raid1、Raid5"></a>6、补充：什么是Raid0、Raid0+1、Raid1、Raid5</h3><p><a href="https://manzhong.github.io/images/Hbase/r.jpg" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/Hbase/r.jpg" alt="img"></a></p>
<p><strong>Standalone</strong></p>
<p>最普遍的单磁盘储存方式。</p>
<p><strong>Cluster</strong></p>
<p>集群储存是通过将数据分布到集群中各节点的存储方式,提供单一的使用接口与界面,使用户可以方便地对所有数据进行统一使用与管理。</p>
<p><strong>Hot swap</strong></p>
<p>用户可以再不关闭系统,不切断电源的情况下取出和更换硬盘,提高系统的恢复能力、拓展性和灵活性。</p>
<p><strong>Raid0</strong></p>
<p>Raid0是所有raid中存储性能最强的阵列形式。其工作原理就是在多个磁盘上分散存取连续的数据,这样,当需要存取数据是多个磁盘可以并排执行,每个磁盘执行属于它自己的那部分数据请求,显著提高磁盘整体存取性能。但是不具备容错能力,适用于低成本、低可靠性的台式系统。</p>
<p><strong>Raid1</strong></p>
<p>又称镜像盘,把一个磁盘的数据镜像到另一个磁盘上,采用镜像容错来提高可靠性,具有raid中最高的数据冗余能力。存数据时会将数据同时写入镜像盘内,读取数据则只从工作盘读出。发生故障时,系统将从镜像盘读取数据,然后再恢复工作盘正确数据。这种阵列方式可靠性极高,但是其容量会减去一半。广泛用于数据要求极严的应用场合,如商业金融、档案管理等领域。只允许一颗硬盘出故障。</p>
<p><strong>Raid0+1</strong></p>
<p>将Raid0和Raid1技术结合在一起,兼顾两者的优势。在数据得到保障的同时,还能提供较强的存储性能。不过至少要求4个或以上的硬盘，但也只允许一个磁盘出错。是一种三高技术。</p>
<p><strong>Raid5</strong></p>
<p>Raid5可以看成是Raid0+1的低成本方案。采用循环偶校验独立存取的阵列方式。将数据和相对应的奇偶校验信息分布存储到组成RAID5的各个磁盘上。当其中一个磁盘数据发生损坏后,利用剩下的磁盘和相应的奇偶校验信息 重新恢复/生成丢失的数据而不影响数据的可用性。至少需要3个或以上的硬盘。适用于大数据量的操作。成本稍高、储存性强、可靠性强的阵列方式。</p>
<p>RAID还有其他方式，请自行查阅。</p>
<h3 id="7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。"><a href="#7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。" class="headerlink" title="7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。"></a>7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。</h3><h3 id="8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。"><a href="#8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。" class="headerlink" title="8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。"></a>8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。</h3><h2 id="2-、Linux优化"><a href="#2-、Linux优化" class="headerlink" title="2 、Linux优化"></a>2 、Linux优化</h2><h3 id="1、开启文件系统的预读缓存可以提高读取速度"><a href="#1、开启文件系统的预读缓存可以提高读取速度" class="headerlink" title="1、开启文件系统的预读缓存可以提高读取速度"></a>1、开启文件系统的预读缓存可以提高读取速度</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo blockdev --setra 32768 /dev/sda</span><br><span class="line"></span><br><span class="line">（注意：ra是readahead的缩写）</span><br></pre></td></tr></table></figure>

<h3 id="2、关闭进程睡眠池"><a href="#2、关闭进程睡眠池" class="headerlink" title="2、关闭进程睡眠池"></a>2、关闭进程睡眠池</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w vm.swappiness=0</span><br></pre></td></tr></table></figure>

<h3 id="3、调整ulimit上限，默认值为比较小的数字"><a href="#3、调整ulimit上限，默认值为比较小的数字" class="headerlink" title="3、调整ulimit上限，默认值为比较小的数字"></a>3、调整ulimit上限，默认值为比较小的数字</h3><p>$ ulimit -n 查看允许最大进程数</p>
<p>$ ulimit -u 查看允许打开最大文件数</p>
<p>修改:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vi /etc/security/limits.conf 修改打开文件数限制</span><br><span class="line">末尾添加：</span><br><span class="line">*                soft    nofile          1024000</span><br><span class="line">*                hard    nofile          1024000</span><br><span class="line">Hive             -       nofile          1024000</span><br><span class="line">hive             -       nproc           1024000 </span><br><span class="line">$ sudo vi /etc/security/limits.d/20-nproc.conf 修改用户打开进程数限制</span><br><span class="line">修改为：</span><br><span class="line">#*          soft    nproc     4096</span><br><span class="line">#root       soft    nproc     unlimited</span><br><span class="line">*          soft    nproc     40960</span><br><span class="line">root       soft    nproc     unlimited</span><br></pre></td></tr></table></figure>

<h3 id="4、开启集群的时间同步NTP，请参看之前文档"><a href="#4、开启集群的时间同步NTP，请参看之前文档" class="headerlink" title="4、开启集群的时间同步NTP，请参看之前文档"></a>4、开启集群的时间同步NTP，请参看之前文档</h3><h3 id="5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）"><a href="#5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）" class="headerlink" title="5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）"></a>5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）</h3><h2 id="3、HDFS优化（hdfs-site-xml）"><a href="#3、HDFS优化（hdfs-site-xml）" class="headerlink" title="3、HDFS优化（hdfs-site.xml）"></a>3、HDFS优化（hdfs-site.xml）</h2><h3 id="1、保证RPC调用会有较多的线程数"><a href="#1、保证RPC调用会有较多的线程数" class="headerlink" title="1、保证RPC调用会有较多的线程数"></a>1、保证RPC调用会有较多的线程数</h3><p>属性：dfs.namenode.handler.count</p>
<p>解释：该属性是NameNode服务默认线程数，的默认值是10，根据机器的可用内存可以调整为50~100</p>
<p>属性：dfs.datanode.handler.count</p>
<p>解释：该属性默认值为10，是DataNode的处理线程数，如果HDFS客户端程序读写请求比较多，可以调高到15<del>20，设置的值越大，内存消耗越多，不要调整的过高，一般业务中，5</del>10即可。</p>
<h3 id="2、副本数的调整"><a href="#2、副本数的调整" class="headerlink" title="2、副本数的调整"></a>2、副本数的调整</h3><p>属性：dfs.replication</p>
<p>解释：如果数据量巨大，且不是非常之重要，可以调整为2<del>3，如果数据非常之重要，可以调整为3</del>5。</p>
<h3 id="3-、文件块大小的调整"><a href="#3-、文件块大小的调整" class="headerlink" title="3.、文件块大小的调整"></a>3.、文件块大小的调整</h3><p>属性：dfs.blocksize</p>
<p>解释：块大小定义，该属性应该根据存储的大量的单个文件大小来设置，如果大量的单个文件都小于100M，建议设置成64M块大小，对于大于100M或者达到GB的这种情况，建议设置成256M，一般设置范围波动在64M~256M之间。</p>
<h2 id="4、MapReduce优化（mapred-site-xml）"><a href="#4、MapReduce优化（mapred-site-xml）" class="headerlink" title="4、MapReduce优化（mapred-site.xml）"></a>4、MapReduce优化（mapred-site.xml）</h2><h3 id="1、Job任务服务线程数调整"><a href="#1、Job任务服务线程数调整" class="headerlink" title="1、Job任务服务线程数调整"></a>1、Job任务服务线程数调整</h3><p>mapreduce.jobtracker.handler.count</p>
<p>该属性是Job任务线程数，默认值是10，根据机器的可用内存可以调整为50~100</p>
<h3 id="2、Http服务器工作线程数"><a href="#2、Http服务器工作线程数" class="headerlink" title="2、Http服务器工作线程数"></a>2、Http服务器工作线程数</h3><p>属性：mapreduce.tasktracker.http.threads</p>
<p>解释：定义HTTP服务器工作线程数，默认值为40，对于大集群可以调整到80~100</p>
<h3 id="3、文件排序合并优化"><a href="#3、文件排序合并优化" class="headerlink" title="3、文件排序合并优化"></a>3、文件排序合并优化</h3><p>属性：mapreduce.task.io.sort.factor</p>
<p>解释：文件排序时同时合并的数据流的数量，这也定义了同时打开文件的个数，默认值为10，如果调高该参数，可以明显减少磁盘IO，即减少文件读取的次数。</p>
<h3 id="4、设置任务并发"><a href="#4、设置任务并发" class="headerlink" title="4、设置任务并发"></a>4、设置任务并发</h3><p>属性：mapreduce.map.speculative</p>
<p>解释：该属性可以设置任务是否可以并发执行，如果任务多而小，该属性设置为true可以明显加快任务执行效率，但是对于延迟非常高的任务，建议改为false，这就类似于迅雷下载。</p>
<h3 id="5、MR输出数据的压缩"><a href="#5、MR输出数据的压缩" class="headerlink" title="5、MR输出数据的压缩"></a>5、MR输出数据的压缩</h3><p>属性：mapreduce.map.output.compress、mapreduce.output.fileoutputformat.compress</p>
<p>解释：对于大集群而言，建议设置Map-Reduce的输出为压缩的数据，而对于小集群，则不需要。</p>
<h3 id="6、优化Mapper和Reducer的个数"><a href="#6、优化Mapper和Reducer的个数" class="headerlink" title="6、优化Mapper和Reducer的个数"></a>6、优化Mapper和Reducer的个数</h3><p>属性：</p>
<p>mapreduce.tasktracker.map.tasks.maximum</p>
<p>mapreduce.tasktracker.reduce.tasks.maximum</p>
<p>解释：以上两个属性分别为一个单独的Job任务可以同时运行的Map和Reduce的数量。</p>
<p>设置上面两个参数时，需要考虑CPU核数、磁盘和内存容量。假设一个8核的CPU，业务内容非常消耗CPU，那么可以设置map数量为4，如果该业务不是特别消耗CPU类型的，那么可以设置map数量为40，reduce数量为20。这些参数的值修改完成之后，一定要观察是否有较长等待的任务，如果有的话，可以减少数量以加快任务执行，如果设置一个很大的值，会引起大量的上下文切换，以及内存与磁盘之间的数据交换，这里没有标准的配置数值，需要根据业务和硬件配置以及经验来做出选择。</p>
<p>在同一时刻，不要同时运行太多的MapReduce，这样会消耗过多的内存，任务会执行的非常缓慢，我们需要根据CPU核数，内存容量设置一个MR任务并发的最大值，使固定数据量的任务完全加载到内存中，避免频繁的内存和磁盘数据交换，从而降低磁盘IO，提高性能。</p>
<p>大概配比：</p>
<table>
<thead>
<tr>
<th align="left">CPU CORE</th>
<th align="left">MEM（GB）</th>
<th align="left">Map</th>
<th align="left">Reduce</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">5</td>
<td align="left">1</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">5</td>
<td align="left">1~4</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left">16</td>
<td align="left">32</td>
<td align="left">16</td>
<td align="left">8</td>
</tr>
<tr>
<td align="left">16</td>
<td align="left">64</td>
<td align="left">16</td>
<td align="left">8</td>
</tr>
<tr>
<td align="left">24</td>
<td align="left">64</td>
<td align="left">24</td>
<td align="left">12</td>
</tr>
<tr>
<td align="left">24</td>
<td align="left">128</td>
<td align="left">24</td>
<td align="left">12</td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">大概估算公式：</span><br><span class="line">map = 2 + ⅔cpu_core</span><br><span class="line">reduce = 2 + ⅓cpu_core</span><br></pre></td></tr></table></figure>

<h2 id="5、HBase优化"><a href="#5、HBase优化" class="headerlink" title="5、HBase优化"></a>5、HBase优化</h2><h3 id="1、在HDFS的文件中追加内容"><a href="#1、在HDFS的文件中追加内容" class="headerlink" title="1、在HDFS的文件中追加内容"></a>1、在HDFS的文件中追加内容</h3><p>不是不允许追加内容么？没错，请看背景故事：</p>
<p>属性：dfs.support.append</p>
<p>文件：hdfs-site.xml、hbase-site.xml</p>
<p>解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。</p>
<h3 id="2、优化DataNode允许的最大文件打开数"><a href="#2、优化DataNode允许的最大文件打开数" class="headerlink" title="2、优化DataNode允许的最大文件打开数"></a>2、优化DataNode允许的最大文件打开数</h3><p>属性：dfs.datanode.max.transfer.threads</p>
<p>文件：hdfs-site.xml</p>
<p>解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096</p>
<h3 id="3、优化延迟高的数据操作的等待时间"><a href="#3、优化延迟高的数据操作的等待时间" class="headerlink" title="3、优化延迟高的数据操作的等待时间"></a>3、优化延迟高的数据操作的等待时间</h3><p>属性：dfs.image.transfer.timeout</p>
<p>文件：hdfs-site.xml</p>
<p>解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。</p>
<h3 id="4、优化数据的写入效率"><a href="#4、优化数据的写入效率" class="headerlink" title="4、优化数据的写入效率"></a>4、优化数据的写入效率</h3><p>属性：</p>
<p>mapreduce.map.output.compress</p>
<p>mapreduce.map.output.compress.codec</p>
<p>文件：mapred-site.xml</p>
<p>解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec</p>
<h3 id="5、优化DataNode存储"><a href="#5、优化DataNode存储" class="headerlink" title="5、优化DataNode存储"></a>5、优化DataNode存储</h3><p>属性：dfs.datanode.failed.volumes.tolerated</p>
<p>文件：hdfs-site.xml</p>
<p>解释：默认为0，意思是当DataNode中有一个磁盘出现故障，则会认为该DataNode shutdown了。如果修改为1，则一个磁盘出现故障时，数据会被复制到其他正常的DataNode上，当前的DataNode继续工作。</p>
<h3 id="6、设置RPC监听数量"><a href="#6、设置RPC监听数量" class="headerlink" title="6、设置RPC监听数量"></a>6、设置RPC监听数量</h3><p>属性：hbase.regionserver.handler.count</p>
<p>文件：hbase-site.xml</p>
<p>解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。</p>
<h3 id="7、优化HStore文件大小"><a href="#7、优化HStore文件大小" class="headerlink" title="7、优化HStore文件大小"></a>7、优化HStore文件大小</h3><p>属性：hbase.hregion.max.filesize</p>
<p>文件：hbase-site.xml</p>
<p>解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。</p>
<h3 id="8、优化hbase客户端缓存"><a href="#8、优化hbase客户端缓存" class="headerlink" title="8、优化hbase客户端缓存"></a>8、优化hbase客户端缓存</h3><p>属性：hbase.client.write.buffer</p>
<p>文件：hbase-site.xml</p>
<p>解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。</p>
<h3 id="9、指定scan-next扫描HBase所获取的行数"><a href="#9、指定scan-next扫描HBase所获取的行数" class="headerlink" title="9、指定scan.next扫描HBase所获取的行数"></a>9、指定scan.next扫描HBase所获取的行数</h3><p>属性：hbase.client.scanner.caching</p>
<p>文件：hbase-site.xml</p>
<p>解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。</p>
<h2 id="6、内存优化"><a href="#6、内存优化" class="headerlink" title="6、内存优化"></a>6、内存优化</h2><p>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</p>
<h2 id="7、JVM优化"><a href="#7、JVM优化" class="headerlink" title="7、JVM优化"></a>7、JVM优化</h2><p>涉及文件：hbase-env.sh</p>
<h3 id="1、并行GC"><a href="#1、并行GC" class="headerlink" title="1、并行GC"></a>1、并行GC</h3><p>参数：-XX:+UseParallelGC</p>
<p>解释：开启并行GC</p>
<h3 id="2、同时处理垃圾回收的线程数"><a href="#2、同时处理垃圾回收的线程数" class="headerlink" title="2、同时处理垃圾回收的线程数"></a>2、同时处理垃圾回收的线程数</h3><p>参数：-XX:ParallelGCThreads=cpu_core – 1</p>
<p>解释：该属性设置了同时处理垃圾回收的线程数。</p>
<h3 id="3、禁用手动GC"><a href="#3、禁用手动GC" class="headerlink" title="3、禁用手动GC"></a>3、禁用手动GC</h3><p>参数：-XX:DisableExplicitGC</p>
<p>解释：防止开发人员手动调用GC</p>
<h2 id="8、Zookeeper优化"><a href="#8、Zookeeper优化" class="headerlink" title="8、Zookeeper优化"></a>8、Zookeeper优化</h2><h3 id="1、优化Zookeeper会话超时时间"><a href="#1、优化Zookeeper会话超时时间" class="headerlink" title="1、优化Zookeeper会话超时时间"></a>1、优化Zookeeper会话超时时间</h3><p>参数：zookeeper.session.timeout</p>
<p>文件：hbase-site.xml</p>
<p>解释：In hbase-site.xml, set zookeeper.session.timeout to 30 seconds or less to bound failure detection (20-30 seconds is a good start).该值会直接关系到master发现服务器宕机的最大周期，默认值为30秒，如果该值过小，会在HBase在写入大量数据发生而GC时，导致RegionServer短暂的不可用，从而没有向ZK发送心跳包，最终导致认为从节点shutdown。一般20台左右的集群需要配置5台zookeeper。</p>
</div><div class="post-copyright"><blockquote><p>Autor original: hechao</p><p>Enlace original: <a href="http://yoursite.com/2019/08/08/Hbase增强/">http://yoursite.com/2019/08/08/Hbase增强/</a></p><p>Declaración de copyright: Indique la fuente de la reimpresión (debe conservar la firma del autor y el enlace)</p></blockquote></div><div class="tags"></div><div class="post-share"><div class="social-share"><span>Cuota:</span></div></div><div class="post-nav"><a href="/2019/08/08/Hbase/" class="pre">Hbase</a><a href="/2019/08/08/Storm/" class="next">Storm</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">Contenidos</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hbase增强"><span class="toc-text">Hbase增强</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#一-Hbase与MapReduce的集成"><span class="toc-text">一 Hbase与MapReduce的集成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#需求一-读取myuser这张表当中的数据写入到HBase的另外一张表当中去"><span class="toc-text">需求一 读取myuser这张表当中的数据写入到HBase的另外一张表当中去</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-创建myuser2-表"><span class="toc-text">1 创建myuser2 表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义mapper类"><span class="toc-text">定义mapper类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义-reduce"><span class="toc-text">定义 reduce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义主类"><span class="toc-text">定义主类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#运行"><span class="toc-text">运行</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#需求2-读取HDFS文件，写入到HBase表当中去"><span class="toc-text">需求2 读取HDFS文件，写入到HBase表当中去</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义mapper"><span class="toc-text">定义mapper</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义reduce"><span class="toc-text">定义reduce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义主类-1"><span class="toc-text">定义主类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#需求三-通过bulkload的方式批量加载数据到HBase当中去"><span class="toc-text">需求三 通过bulkload的方式批量加载数据到HBase当中去</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义mapper-1"><span class="toc-text">定义mapper</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#主类-程序入口"><span class="toc-text">主类 程序入口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#开发代码-加载数据"><span class="toc-text">开发代码 加载数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hive"><span class="toc-text">Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据仓库工具"><span class="toc-text">数据仓库工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#用于数据分析、清洗"><span class="toc-text">用于数据分析、清洗</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于HDFS、MapReduce"><span class="toc-text">基于HDFS、MapReduce</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HBase"><span class="toc-text">HBase</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nosql数据库"><span class="toc-text">nosql数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#用于存储结构化和非结构话的数据"><span class="toc-text">用于存储结构化和非结构话的数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于HDFS"><span class="toc-text">基于HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#延迟较低，接入在线业务使用"><span class="toc-text">延迟较低，接入在线业务使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结：Hive与HBase"><span class="toc-text">总结：Hive与HBase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#需求一将hive分析结果的数据，保存到HBase当中去"><span class="toc-text">需求一将hive分析结果的数据，保存到HBase当中去</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-拷贝hbase的五个依赖jar包到hive的lib目录下"><span class="toc-text">1 拷贝hbase的五个依赖jar包到hive的lib目录下</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-修改hive的配置文件"><span class="toc-text">2 修改hive的配置文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-修改hive-env-sh配置文件添加以下配置"><span class="toc-text">3 修改hive-env.sh配置文件添加以下配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-hive当中建表并加载以下数据"><span class="toc-text">4 hive当中建表并加载以下数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#hive当中建表"><span class="toc-text">hive当中建表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#准备数据内容如下"><span class="toc-text">准备数据内容如下</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#进行加载数据"><span class="toc-text">进行加载数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-创建hive管理表与HBase进行映射"><span class="toc-text">5 创建hive管理表与HBase进行映射</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-hbase当中查看表hbase-score"><span class="toc-text">6 hbase当中查看表hbase_score</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#需求二创建hive外部表，映射HBase当中已有的表模型，"><span class="toc-text">需求二创建hive外部表，映射HBase当中已有的表模型，</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第一步：HBase当中创建表并手动插入加载一些数据"><span class="toc-text">第一步：HBase当中创建表并手动插入加载一些数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第二步：建立hive的外部表，映射HBase当中的表以及字段"><span class="toc-text">第二步：建立hive的外部表，映射HBase当中的表以及字段</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#四-hbase预分区"><span class="toc-text">四 hbase预分区</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1、为何要预分区？"><span class="toc-text">1、为何要预分区？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、如何预分区？"><span class="toc-text">2、如何预分区？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、如何设定预分区？"><span class="toc-text">3、如何设定预分区？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、手动指定预分区"><span class="toc-text">1、手动指定预分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、使用16进制算法生成预分区"><span class="toc-text">2、使用16进制算法生成预分区</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、使用JavaAPI创建预分区"><span class="toc-text">3、使用JavaAPI创建预分区</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#五-HBase的rowKey设计技巧"><span class="toc-text">五 HBase的rowKey设计技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-rowkey长度原则"><span class="toc-text">1 rowkey长度原则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-rowkey散列原则"><span class="toc-text">2 rowkey散列原则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-rowkey唯一原则"><span class="toc-text">3 rowkey唯一原则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4什么是热点"><span class="toc-text">4什么是热点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1加盐"><span class="toc-text">1加盐</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2哈希"><span class="toc-text">2哈希</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3反转"><span class="toc-text">3反转</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3时间戳反转"><span class="toc-text">3时间戳反转</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#六-Hbase的协处理器"><span class="toc-text">六 Hbase的协处理器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、协处理器有两种：-observer-和-endpoint"><span class="toc-text">2、协处理器有两种： observer 和 endpoint</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、协处理器加载方式"><span class="toc-text">3、协处理器加载方式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、协处理器Observer应用实战"><span class="toc-text">4、协处理器Observer应用实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#第一步：HBase当中创建第一张表proc1"><span class="toc-text">第一步：HBase当中创建第一张表proc1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第二步：Hbase当中创建第二张表proc2"><span class="toc-text">第二步：Hbase当中创建第二张表proc2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第三步：开发HBase的协处理器"><span class="toc-text">第三步：开发HBase的协处理器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第四步：将项目打成jar包，并上传到HDFS上面"><span class="toc-text">第四步：将项目打成jar包，并上传到HDFS上面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第五步：将打好的jar包挂载到proc1表当中去"><span class="toc-text">第五步：将打好的jar包挂载到proc1表当中去</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#第六步：proc1表当中添加数据"><span class="toc-text">第六步：proc1表当中添加数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#七-HBase当中的二级索引的基本介绍"><span class="toc-text">七 HBase当中的二级索引的基本介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#八-HBase调优"><span class="toc-text">八 HBase调优</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1、通用优化"><span class="toc-text">1、通用优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、NameNode的元数据备份使用SSD"><span class="toc-text">1、NameNode的元数据备份使用SSD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5-10分钟备份一次。备份可以通过定时任务复制元数据目录即可。"><span class="toc-text">2、定时备份NameNode上的元数据，每小时或者每天备份，如果数据极其重要，可以5~10分钟备份一次。备份可以通过定时任务复制元数据目录即可。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、为NameNode指定多个元数据目录，使用dfs-name-dir或者dfs-namenode-name-dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。"><span class="toc-text">3、为NameNode指定多个元数据目录，使用dfs.name.dir或者dfs.namenode.name.dir指定。一个指定本地磁盘，一个指定网络磁盘。这样可以提供元数据的冗余和健壮性，以免发生故障。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、设置dfs-namenode-name-dir-restore为true，允许尝试恢复之前失败的dfs-namenode-name-dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。"><span class="toc-text">4、设置dfs.namenode.name.dir.restore为true，允许尝试恢复之前失败的dfs.namenode.name.dir目录，在创建checkpoint时做此尝试，如果设置了多个磁盘，建议允许。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、NameNode节点必须配置为RAID1（镜像盘）结构。"><span class="toc-text">5、NameNode节点必须配置为RAID1（镜像盘）结构。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6、补充：什么是Raid0、Raid0-1、Raid1、Raid5"><span class="toc-text">6、补充：什么是Raid0、Raid0+1、Raid1、Raid5</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。"><span class="toc-text">7、保持NameNode日志目录有足够的空间，这些日志有助于帮助你发现问题。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。"><span class="toc-text">8、因为Hadoop是IO密集型框架，所以尽量提升存储的速度和吞吐量（类似位宽）。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-、Linux优化"><span class="toc-text">2 、Linux优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、开启文件系统的预读缓存可以提高读取速度"><span class="toc-text">1、开启文件系统的预读缓存可以提高读取速度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、关闭进程睡眠池"><span class="toc-text">2、关闭进程睡眠池</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、调整ulimit上限，默认值为比较小的数字"><span class="toc-text">3、调整ulimit上限，默认值为比较小的数字</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、开启集群的时间同步NTP，请参看之前文档"><span class="toc-text">4、开启集群的时间同步NTP，请参看之前文档</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）"><span class="toc-text">5、更新系统补丁（注意：更新补丁前，请先测试新版本补丁对集群节点的兼容性）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、HDFS优化（hdfs-site-xml）"><span class="toc-text">3、HDFS优化（hdfs-site.xml）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、保证RPC调用会有较多的线程数"><span class="toc-text">1、保证RPC调用会有较多的线程数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、副本数的调整"><span class="toc-text">2、副本数的调整</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-、文件块大小的调整"><span class="toc-text">3.、文件块大小的调整</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、MapReduce优化（mapred-site-xml）"><span class="toc-text">4、MapReduce优化（mapred-site.xml）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、Job任务服务线程数调整"><span class="toc-text">1、Job任务服务线程数调整</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、Http服务器工作线程数"><span class="toc-text">2、Http服务器工作线程数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、文件排序合并优化"><span class="toc-text">3、文件排序合并优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、设置任务并发"><span class="toc-text">4、设置任务并发</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、MR输出数据的压缩"><span class="toc-text">5、MR输出数据的压缩</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6、优化Mapper和Reducer的个数"><span class="toc-text">6、优化Mapper和Reducer的个数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5、HBase优化"><span class="toc-text">5、HBase优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、在HDFS的文件中追加内容"><span class="toc-text">1、在HDFS的文件中追加内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、优化DataNode允许的最大文件打开数"><span class="toc-text">2、优化DataNode允许的最大文件打开数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、优化延迟高的数据操作的等待时间"><span class="toc-text">3、优化延迟高的数据操作的等待时间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4、优化数据的写入效率"><span class="toc-text">4、优化数据的写入效率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5、优化DataNode存储"><span class="toc-text">5、优化DataNode存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6、设置RPC监听数量"><span class="toc-text">6、设置RPC监听数量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7、优化HStore文件大小"><span class="toc-text">7、优化HStore文件大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8、优化hbase客户端缓存"><span class="toc-text">8、优化hbase客户端缓存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9、指定scan-next扫描HBase所获取的行数"><span class="toc-text">9、指定scan.next扫描HBase所获取的行数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6、内存优化"><span class="toc-text">6、内存优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7、JVM优化"><span class="toc-text">7、JVM优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、并行GC"><span class="toc-text">1、并行GC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、同时处理垃圾回收的线程数"><span class="toc-text">2、同时处理垃圾回收的线程数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3、禁用手动GC"><span class="toc-text">3、禁用手动GC</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8、Zookeeper优化"><span class="toc-text">8、Zookeeper优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、优化Zookeeper会话超时时间"><span class="toc-text">1、优化Zookeeper会话超时时间</span></a></li></ol></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> Recientes</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Hbase/">Hbase</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Hbase增强/">Hbase增强</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Storm/">Storm</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Scala入门/">Scala入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Scala进阶1/">Scala进阶1</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Scala进阶2/">Scala进阶2</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Scala高级/">Scala高级</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/ElSearh/">ElSearh</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/Spark入门/">Spark入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/SparkRDD/">SparkRDD</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> Etiquetas</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> Archivo</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Mapa del sitio</a> |  <a href="/atom.xml">suscribirse a este sitio</a> |  <a href="/about/">Contacta al blogger</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">hechao.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.4"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.4" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>