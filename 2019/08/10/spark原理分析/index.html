<!DOCTYPE html><html lang="hc-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一只沙皮狗的悲伤"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.4"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.4"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>spark原理分析 | 一只沙皮狗的悲伤</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">spark原理分析</h1><a id="logo" href="/.">一只沙皮狗的悲伤</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Start</i></a><a href="/archives/"><i class="fa fa-archive"> Archiv</i></a><a href="/about/"><i class="fa fa-user"> Über</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Suche"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">spark原理分析</h1><div class="post-meta"><a href="/2019/08/10/spark原理分析/#comments" class="comment-count"></a><p><span class="date">Aug 10, 2019</span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>Schlägt</i></i></span></p></div><div class="post-content"><h1 id="一spark特性"><a href="#一spark特性" class="headerlink" title="一spark特性"></a>一spark特性</h1><h2 id="1-rdd的分区和shuffle"><a href="#1-rdd的分区和shuffle" class="headerlink" title="1 rdd的分区和shuffle"></a>1 rdd的分区和shuffle</h2><p>分区的作用</p>
<p>RDD 使用分区来分布式并行处理数据, 并且要做到尽量少的在不同的 Executor 之间使用网络交换数据, 所以当使用 RDD 读取数据的时候, 会尽量的在物理上靠近数据源, 比如说在读取 Cassandra 或者 HDFS 中数据的时候, 会尽量的保持 RDD 的分区和数据源的分区数, 分区模式等一一对应</p>
<p>分区和 Shuffle 的关系</p>
<p>分区的主要作用是用来实现并行计算, 本质上和 Shuffle 没什么关系, 但是往往在进行数据处理的时候, 例如 <code>reduceByKey</code>, <code>groupByKey</code> 等聚合操作, 需要把 Key 相同的 Value 拉取到一起进行计算, 这个时候因为这些 Key 相同的 Value 可能会坐落于不同的分区, 于是理解分区才能理解 Shuffle 的根本原理</p>
<p>Spark 中的 Shuffle 操作的特点</p>
<ul>
<li>只有 <code>Key-Value</code> 型的 RDD 才会有 Shuffle 操作, 例如 <code>RDD[(K, V)]</code>, 但是有一个特例, 就是 <code>repartition</code> 算子可以对任何数据类型 Shuffle</li>
<li>早期版本 Spark 的 Shuffle 算法是 <code>Hash base shuffle</code>, 后来改为 <code>Sort base shuffle</code>, 更适合大吞吐量的场景</li>
</ul>
<h2 id="2分区操作"><a href="#2分区操作" class="headerlink" title="2分区操作"></a>2分区操作</h2><p>很多算子(支持shuffle的 )都可以指定分区数</p>
<ul>
<li>这些算子,可以在最后一个参数的位置传入新的分区数</li>
<li>如没有重新指定分区数,默认从父rdd中继承分区数</li>
</ul>
<h3 id="1-查看分区数"><a href="#1-查看分区数" class="headerlink" title="1 查看分区数"></a>1 查看分区数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.parallelize(1 to 100).count</span><br><span class="line">res0: Long = 100</span><br></pre></td></tr></table></figure>

<p><code>spark-shell --master local[8]</code>, 这样会生成 1 个 Executors, 这个 Executors 有 8 个 Cores, 所以默认会有 8 个 Tasks, 每个 Cores 对应一个分区, 每个分区对应一个 Tasks, 可以通过 <code>rdd.partitions.size</code> 来查看分区数量</p>
<p>默认的分区数量是和 Cores 的数量有关的, 也可以通过如下三种方式修改或者重新指定分区数量</p>
<ul>
<li><h3 id="创建-RDD-时指定分区数"><a href="#创建-RDD-时指定分区数" class="headerlink" title="创建 RDD 时指定分区数"></a><strong>创建 RDD 时指定分区数</strong></h3></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(1 to 100, 6)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.partitions.size</span><br><span class="line">res1: Int = 6</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.textFile(&quot;hdfs:///dataset/wordcount.txt&quot;, 6)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] = hdfs:///dataset/wordcount.txt MapPartitionsRDD[3] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.partitions.size</span><br><span class="line">res2: Int = 7</span><br></pre></td></tr></table></figure>

<p>rdd1 是通过本地集合创建的, 创建的时候通过第二个参数指定了分区数量. rdd2 是通过读取 HDFS 中文件创建的, 同样通过第二个参数指定了分区数, 因为是从 HDFS 中读取文件, 所以最终的分区数是由 Hadoop 的 InputFormat 来指定的, 所以比指定的分区数大了一个.</p>
<ul>
<li><strong>通过</strong> <code>coalesce</code> <strong>算子指定</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null): RDD[T]</span><br></pre></td></tr></table></figure>

<p>numPartitions 新生成的 RDD 的分区数</p>
<p>shuffle 是否 Shuffle</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val source = sc.parallelize(1 to 100, 6)</span><br><span class="line">source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; source.partitions.size</span><br><span class="line">res0: Int = 6</span><br><span class="line"></span><br><span class="line">scala&gt; val noShuffleRdd = source.coalesce(numPartitions=8, shuffle=false)</span><br><span class="line">noShuffleRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; noShuffleRdd.toDebugString </span><br><span class="line">res1: String =</span><br><span class="line">(6) CoalescedRDD[1] at coalesce at &lt;console&gt;:26 []</span><br><span class="line"> |  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 []</span><br><span class="line"></span><br><span class="line"> scala&gt; val noShuffleRdd = source.coalesce(numPartitions=8, shuffle=false)</span><br><span class="line"> noShuffleRdd: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[1] at coalesce at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; shuffleRdd.toDebugString </span><br><span class="line">res3: String =</span><br><span class="line">(8) MapPartitionsRDD[5] at coalesce at &lt;console&gt;:26 []</span><br><span class="line"> |  CoalescedRDD[4] at coalesce at &lt;console&gt;:26 []</span><br><span class="line"> |  ShuffledRDD[3] at coalesce at &lt;console&gt;:26 []</span><br><span class="line"> +-(6) MapPartitionsRDD[2] at coalesce at &lt;console&gt;:26 []</span><br><span class="line">    |  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 []</span><br><span class="line"></span><br><span class="line">scala&gt; noShuffleRdd.partitions.size     </span><br><span class="line">res4: Int = 6</span><br><span class="line"></span><br><span class="line">scala&gt; shuffleRdd.partitions.size</span><br><span class="line">res5: Int = 8</span><br><span class="line"></span><br><span class="line">如果 shuffle 参数指定为 false, 运行计划中确实没有 ShuffledRDD, 没有 shuffled 这个过程</span><br><span class="line">如果 shuffle 参数指定为 true, 运行计划中有一个 ShuffledRDD, 有一个明确的显式的 shuffled 过程</span><br><span class="line">如果 shuffle 参数指定为 false 却增加了分区数, 分区数并不会发生改变, 这是因为增加分区是一个宽依赖, 没有 shuffled 过程无法做到, 后续会详细解释宽依赖的概念</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>通过</strong> <code>repartition</code> <strong>算子指定</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</span><br><span class="line">repartition` 算子本质上就是 `coalesce(numPartitions, shuffle = true)</span><br><span class="line">scala&gt; val source = sc.parallelize(1 to 100, 6)</span><br><span class="line">source: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; source.partitions.size</span><br><span class="line">res7: Int = 6</span><br><span class="line"></span><br><span class="line">scala&gt; source.repartition(100).partitions.size </span><br><span class="line">res8: Int = 100</span><br><span class="line"></span><br><span class="line">scala&gt; source.repartition(1).partitions.size </span><br><span class="line">res9: Int = 1</span><br><span class="line">增加分区有效</span><br><span class="line">减少分区有效</span><br></pre></td></tr></table></figure>

<p><code>repartition</code> 算子无论是增加还是减少分区都是有效的, 因为本质上 <code>repartition</code> 会通过 <code>shuffle</code> 操作把数据分发给新的 RDD 的不同的分区, 只有 <code>shuffle</code> 操作才可能做到增大分区数, 默认情况下, 分区函数是 <code>RoundRobin</code>, 如果希望改变分区函数, 也就是数据分布的方式, 可以通过自定义分区函数来实现</p>
<p><a href="https://manzhong.github.io/images/spark/fqd.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/fqd.png" alt="img"></a></p>
<h2 id="2-RDD-的-Shuffle-是什么"><a href="#2-RDD-的-Shuffle-是什么" class="headerlink" title="2 RDD 的 Shuffle 是什么"></a>2 RDD 的 Shuffle 是什么</h2><p><strong>partitioner用于计算一个数据应该发往哪个机器上 (就是哪个分区)</strong></p>
<ul>
<li>HashPartitioner 函数 对key求hashcode和对应的reduce的数量取模 摸出来是几就发往哪个分区</li>
<li>key .hashCode % reduce count 的值 取值范围在0 到 reduce count之间</li>
</ul>
<p><strong>hash base 和sort base 用于描述中间过程如何存放文件*</strong></p>
<p>rdd2 =rdd1.reduceByKey(_ + _) 这个reduceByKey() 是属于rdd2的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val sourceRdd = sc.textFile(&quot;hdfs://node01:9020/dataset/wordcount.txt&quot;)</span><br><span class="line">val flattenCountRdd = sourceRdd.flatMap(_.split(&quot; &quot;)).map((_, 1))</span><br><span class="line">val aggCountRdd = flattenCountRdd.reduceByKey(_ + _)  //这个reduceByKey() 是属于aggCountRdd</span><br><span class="line">val result = aggCountRdd.collect</span><br></pre></td></tr></table></figure>

<p><a href="https://manzhong.github.io/images/spark/rbk.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/rbk.png" alt="img"></a></p>
<p><code>reduceByKey</code> 这个算子本质上就是先按照 Key 分组, 后对每一组数据进行 <code>reduce</code>, 所面临的挑战就是 Key 相同的所有数据可能分布在不同的 Partition 分区中, 甚至可能在不同的节点中, 但是它们必须被共同计算.</p>
<p>为了让来自相同 Key 的所有数据都在 <code>reduceByKey</code> 的同一个 <code>reduce</code> 中处理, 需要执行一个 <code>all-to-all</code> 的操作, 需要在不同的节点(不同的分区)之间拷贝数据, 必须跨分区聚集相同 Key 的所有数据, 这个过程叫做 <code>Shuffle</code>.</p>
<h2 id="3-RDD-的-Shuffle-原理"><a href="#3-RDD-的-Shuffle-原理" class="headerlink" title="3 RDD 的 Shuffle 原理"></a>3 RDD 的 Shuffle 原理</h2><p>Spark 的 Shuffle 发展大致有两个阶段: <code>Hash base shuffle</code> 和 <code>Sort base shuffle</code></p>
<h3 id="3-1-Hash-base-shuffle"><a href="#3-1-Hash-base-shuffle" class="headerlink" title="3.1 Hash base shuffle"></a>3.1 <strong>Hash base shuffle</strong></h3><p><a href="https://manzhong.github.io/images/spark/hbs.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/hbs.png" alt="img"></a></p>
<p>大致的原理是分桶, 假设 Reducer 的个数为 R, 那么每个 Mapper 有 R 个桶, 按照 Key 的 Hash 将数据映射到不同的桶中, Reduce 找到每一个 Mapper 中对应自己的桶拉取数据.</p>
<p>假设 Mapper 的个数为 M, 整个集群的文件数量是 <code>M * R</code>, 如果有 1,000 个 Mapper 和 Reducer, 则会生成 1,000,000 个文件, 这个量非常大了.</p>
<p>过多的文件会导致文件系统打开过多的文件描述符, 占用系统资源. 所以这种方式并不适合大规模数据的处理, 只适合中等规模和小规模的数据处理, 在 Spark 1.2 版本中废弃了这种方式.</p>
<h3 id="3-2-Sort-base-shuffle"><a href="#3-2-Sort-base-shuffle" class="headerlink" title="3.2 Sort base shuffle"></a>3.2 <strong>Sort base shuffle</strong></h3><p><a href="https://manzhong.github.io/images/spark/sbs.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/sbs.png" alt="img"></a></p>
<p>对于 Sort base shuffle 来说, 每个 Map 侧的分区只有一个输出文件, Reduce 侧的 Task 来拉取, 大致流程如下</p>
<ol>
<li>Map 侧将数据全部放入一个叫做 AppendOnlyMap 的组件中, 同时可以在这个特殊的数据结构中做聚合操作</li>
<li>然后通过一个类似于 MergeSort 的排序算法 TimSort 对 AppendOnlyMap 底层的 Array 排序<ul>
<li>先按照 Partition ID 排序, 后按照 Key 的 HashCode 排序</li>
</ul>
</li>
<li>最终每个 Map Task 生成一个 输出文件, Reduce Task 来拉取自己对应的数据</li>
</ol>
<p>从上面可以得到结论, Sort base shuffle 确实可以大幅度减少所产生的中间文件, 从而能够更好的应对大吞吐量的场景, 在 Spark 1.2 以后, 已经默认采用这种方式.</p>
<p>但是需要大家知道的是, Spark 的 Shuffle 算法并不只是这一种, 即使是在最新版本, 也有三种 Shuffle 算法, 这三种算法对每个 Map 都只产生一个临时文件, 但是产生文件的方式不同, 一种是类似 Hash 的方式, 一种是刚才所说的 Sort, 一种是对 Sort 的一种优化(使用 Unsafe API 直接申请堆外内存)</p>
<h1 id="二-缓存"><a href="#二-缓存" class="headerlink" title="二 缓存"></a>二 缓存</h1><p>缓存是将数据缓存到blockMananger 中 导入隐式转换的意义 是可以获得ds和df的底层 row 和internelRow</p>
<h2 id="1-缓存的意义"><a href="#1-缓存的意义" class="headerlink" title="1 缓存的意义"></a>1 缓存的意义</h2><p><strong>使用缓存的原因 - 多次使用 RDD</strong> 减少shuffle,减少其他算子执行,缓存算子生成结果</p>
<p>需求:</p>
<p>在日志文件中找到访问次数最少的 IP 和访问次数最多的 IP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">package com.nicai.demo</span><br><span class="line"></span><br><span class="line">import org.apache.commons.lang3.StringUtils</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.junit.Test</span><br><span class="line"></span><br><span class="line">class CacheDemo &#123;</span><br><span class="line">  /**</span><br><span class="line">    * 1. 创建sc</span><br><span class="line">    * 2. 读取文件</span><br><span class="line">    * 3. 取出IP, 赋予初始频率</span><br><span class="line">    * 4. 清洗</span><br><span class="line">    * 5. 统计IP出现的次数</span><br><span class="line">    * 6. 统计出现次数最少的IP</span><br><span class="line">    * 7. 统计出现次数最多的IP</span><br><span class="line">    */</span><br><span class="line">  @Test</span><br><span class="line">  def prepare(): Unit = &#123;</span><br><span class="line">    // 1. 创建 SC</span><br><span class="line">    val conf = new SparkConf().setAppName(&quot;cache_prepare&quot;).setMaster(&quot;local[6]&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    // 2. 读取文件</span><br><span class="line">    val source = sc.textFile(&quot;G:\\develop\\data\\access_log_sample.txt&quot;)</span><br><span class="line"></span><br><span class="line">    // 3. 取出IP, 赋予初始频率</span><br><span class="line">    val countRDD = source.map( item =&gt; (item.split(&quot; &quot;)(0), 1) )</span><br><span class="line"></span><br><span class="line">    // 4. 数据清洗</span><br><span class="line">    val cleanRDD = countRDD.filter( item =&gt; StringUtils.isNotEmpty(item._1) )</span><br><span class="line"></span><br><span class="line">    // 5. 统计IP出现的次数(聚合)</span><br><span class="line">    val aggRDD = cleanRDD.reduceByKey( (curr, agg) =&gt; curr + agg )</span><br><span class="line"></span><br><span class="line">    // 6. 统计出现次数最少的IP(得出结论)  </span><br><span class="line">    val lessIp = aggRDD.sortBy(item =&gt; item._2, ascending = true).first()</span><br><span class="line"></span><br><span class="line">    // 7. 统计出现次数最多的IP(得出结论) 降序</span><br><span class="line">    val moreIp = aggRDD.sortBy(item =&gt; item._2, ascending = false).first()</span><br><span class="line"></span><br><span class="line">    println((lessIp, moreIp))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">5是一个 Shuffle 操作, Shuffle 操作会在集群内进行数据拷贝</span><br><span class="line">在上述代码中, 多次使用到了 aggRDD并执行了action操作, 导致文件读取两次,6之前的代码(执行)计算两次, 性能有损耗</span><br></pre></td></tr></table></figure>

<p><strong>使用缓存的原因 - 容错</strong></p>
<p><a href="https://manzhong.github.io/images/spark/rc2.png" target="_blank" rel="noopener"><img src="https://manzhong.github.io/images/spark/rc2.png" alt="img"></a></p>
<p>当在计算 RDD3 的时候如果出错了, 会怎么进行容错?</p>
<p>会再次计算 RDD1 和 RDD2 的整个链条, 假设 RDD1 和 RDD2 是通过比较昂贵的操作得来的, 有没有什么办法减少这种开销?</p>
<p>上述两个问题的解决方案其实都是 <code>缓存</code>, 除此之外, 使用缓存的理由还有很多, 但是总结一句, 就是缓存能够帮助开发者在进行一些昂贵操作后, 将其结果保存下来, 以便下次使用无需再次执行, 缓存能够显著的提升性能.</p>
<p>所以, 缓存适合在一个 RDD 需要重复多次利用, 并且还不是特别大的情况下使用, 例如迭代计算等场景.</p>
<h2 id="2-缓存相关的-API"><a href="#2-缓存相关的-API" class="headerlink" title="2 缓存相关的 API"></a>2 缓存相关的 API</h2><h3 id="2-1-可以使用-cache-方法进行缓存"><a href="#2-1-可以使用-cache-方法进行缓存" class="headerlink" title="2.1 可以使用 cache 方法进行缓存"></a>2.1 <strong>可以使用</strong> <code>cache</code> <strong>方法进行缓存</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//使用cache缓存</span><br><span class="line">@Test</span><br><span class="line">def cacheDemo(): Unit = &#123;</span><br><span class="line">  val conf = new SparkConf().setMaster(&quot;local[3]&quot;).setAppName(&quot;cache&quot;)</span><br><span class="line">  val context =new  SparkContext(conf)</span><br><span class="line">  //读取文件</span><br><span class="line">  val file = context.textFile(&quot;G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt&quot;)</span><br><span class="line">  val rdd1 = file.map(it =&gt; ((it.split(&quot; &quot;)) (0), 1))</span><br><span class="line">    .reduceByKey((curr, agg) =&gt; curr + agg)</span><br><span class="line">    .cache()                //缓存结果</span><br><span class="line">  val rddMax = rdd1</span><br><span class="line">    .sortBy(item =&gt; item._2, true)</span><br><span class="line">    .first()</span><br><span class="line">  val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first()</span><br><span class="line">  println(rddMax,rddMin)</span><br><span class="line">&#125;</span><br><span class="line">//其实 cache 方法底层就是persist()方法  这个persist()  底层有调用了persist(StorageLevel.MEMORY_ONLY)方法   StorageLevel.MEMORY_ONLY为其默认</span><br></pre></td></tr></table></figure>

<h3 id="2-2-也可以使用-persist-方法进行缓存"><a href="#2-2-也可以使用-persist-方法进行缓存" class="headerlink" title="2.2 也可以使用 persist 方法进行缓存"></a>2.2 <strong>也可以使用 persist 方法进行缓存</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//使用persist缓存</span><br><span class="line"> @Test</span><br><span class="line"> def persistDemo(): Unit = &#123;</span><br><span class="line">   val conf = new SparkConf().setMaster(&quot;local[3]&quot;).setAppName(&quot;cache&quot;)</span><br><span class="line">   val context =new  SparkContext(conf)</span><br><span class="line">   //读取文件</span><br><span class="line">   val file = context.textFile(&quot;G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt&quot;)</span><br><span class="line">   val rdd1 = file.map(it =&gt; ((it.split(&quot; &quot;)) (0), 1))</span><br><span class="line">     .reduceByKey((curr, agg) =&gt; curr + agg)</span><br><span class="line">     .persist(StorageLevel.MEMORY_ONLY)              //缓存结果  与cache, persist()一样</span><br><span class="line">   //.persist(参数)    //缓存级别</span><br><span class="line">   val rddMax = rdd1</span><br><span class="line">     .sortBy(item =&gt; item._2, true)</span><br><span class="line">     .first()</span><br><span class="line">   val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first()</span><br><span class="line">   println(rddMax,rddMin)</span><br><span class="line"> &#125;</span><br><span class="line"> //其实 cache 方法底层就是persist()方法  这个persist()  底层有调用了persist(StorageLevel.MEMORY_ONLY)方法   StorageLevel.MEMORY_ONLY为其默认</span><br></pre></td></tr></table></figure>

<h3 id="2-3-清理缓存"><a href="#2-3-清理缓存" class="headerlink" title="2.3 清理缓存"></a>2.3 清理缓存</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line"> def cacheDemo(): Unit = &#123;</span><br><span class="line">   val conf = new SparkConf().setMaster(&quot;local[3]&quot;).setAppName(&quot;cache&quot;)</span><br><span class="line">   val context =new  SparkContext(conf)</span><br><span class="line">   //读取文件</span><br><span class="line">   val file = context.textFile(&quot;G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt&quot;)</span><br><span class="line">   val rdd1 = file.map(it =&gt; ((it.split(&quot; &quot;)) (0), 1))</span><br><span class="line">     .reduceByKey((curr, agg) =&gt; curr + agg)</span><br><span class="line">     .cache()                //缓存结果</span><br><span class="line">     </span><br><span class="line">     //清除缓存</span><br><span class="line">     rdd.unpersist()  </span><br><span class="line">   val rddMax = rdd1</span><br><span class="line">     .sortBy(item =&gt; item._2, true)</span><br><span class="line">     .first()</span><br><span class="line">   val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first()</span><br><span class="line">   println(rddMax,rddMin)</span><br><span class="line">   context.close()</span><br><span class="line"> &#125;</span><br><span class="line"> 根据缓存级别的不同, 缓存存储的位置也不同, 但是使用 unpersist 可以指定删除 RDD 对应的缓存信息, 并指定缓存级别为 NONE</span><br></pre></td></tr></table></figure>

<h2 id="3-缓存级别"><a href="#3-缓存级别" class="headerlink" title="3 缓存级别"></a>3 缓存级别</h2><p>缓存是一个技术活, 要考虑很多</p>
<ul>
<li>是否使用磁盘缓存?</li>
<li>是否使用内存缓存?</li>
<li>是否使用堆外内存?</li>
<li>缓存前是否先序列化?</li>
<li>是否需要有副本?</li>
</ul>
<h3 id="3-1-查看缓存级别对象"><a href="#3-1-查看缓存级别对象" class="headerlink" title="3.1 查看缓存级别对象"></a>3.1 查看缓存级别对象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line"> def persistDemo(): Unit = &#123;</span><br><span class="line">   val conf = new SparkConf().setMaster(&quot;local[3]&quot;).setAppName(&quot;cache&quot;)</span><br><span class="line">   val context =new  SparkContext(conf)</span><br><span class="line">   //读取文件</span><br><span class="line">   val file = context.textFile(&quot;G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt&quot;)</span><br><span class="line">   val rdd1 = file.map(it =&gt; ((it.split(&quot; &quot;)) (0), 1))</span><br><span class="line">     .reduceByKey((curr, agg) =&gt; curr + agg)</span><br><span class="line">     .persist(StorageLevel.MEMORY_ONLY)              //缓存结果  与cache, persist()一样</span><br><span class="line">   //.persist(参数)    //缓存级别</span><br><span class="line">   //查看缓存级别</span><br><span class="line">   println(rdd1.getStorageLevel)</span><br><span class="line">   val rddMax = rdd1</span><br><span class="line">     .sortBy(item =&gt; item._2, true)</span><br><span class="line">     .first()</span><br><span class="line">   val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first()</span><br><span class="line">   println(rddMax,rddMin)</span><br><span class="line">   context.close()</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>deserialized`是否以反序列化形式存储 若是true 则不序列化 ,若是false 则存储序列化后的值</p>
<p>其值为true 存储的是对象 若是false 则存储的是二进制文件</p>
<p>带2的为副本数</p>
<table>
<thead>
<tr>
<th align="left">缓存级别</th>
<th align="left"><code>userDisk</code> 是否使用磁盘</th>
<th align="left"><code>useMemory</code>是否使用内存</th>
<th align="left"><code>useOffHeap</code> 是否使用堆外内存</th>
<th align="left"><code>deserialized</code>是否以反序列化形式存储</th>
<th align="left"><code>replication</code> 副本数</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>NONE</code> 就是不存</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left"><code>DISK_ONLY</code> 存到磁盘里</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left"><code>DISK_ONLY_2</code></td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left"><code>MEMORY_ONLY</code> 存到内存</td>
<td align="left">false</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">true</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left"><code>MEMORY_ONLY_2</code></td>
<td align="left">false</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">true</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left"><code>MEMORY_ONLY_SER</code>存到内存以二进制文件的形式</td>
<td align="left">false</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left"><code>MEMORY_ONLY_SER_2</code></td>
<td align="left">false</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left"><code>MEMORY_AND_DISK</code>内存和磁盘都有</td>
<td align="left">true</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">true</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left"><code>MEMORY_AND_DISK</code></td>
<td align="left">true</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">true</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left"><code>MEMORY_AND_DISK_SER</code>内存和磁盘都有以二进制形式存储</td>
<td align="left">true</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">1</td>
</tr>
<tr>
<td align="left"><code>MEMORY_AND_DISK_SER_2</code></td>
<td align="left">true</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">false</td>
<td align="left">2</td>
</tr>
<tr>
<td align="left"><code>OFF_HEAP</code></td>
<td align="left">true</td>
<td align="left">true</td>
<td align="left">true</td>
<td align="left">false</td>
<td align="left">1</td>
</tr>
</tbody></table>
<h3 id="3-2如何选择分区级别"><a href="#3-2如何选择分区级别" class="headerlink" title="3.2如何选择分区级别"></a>3.2如何选择分区级别</h3><p>Spark 的存储级别的选择，核心问题是在 memory 内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择:</p>
<p>如果您的 RDD 适合于默认存储级别（MEMORY_ONLY），leave them that way。这是 CPU 效率最高的选项，允许 RDD 上的操作尽可能快地运行.</p>
<p>如果不是，试着使用 MEMORY_ONLY_SER 和 selecting a fast serialization library 以使对象更加节省空间，但仍然能够快速访问。(Java和Scala)</p>
<p>不要溢出到磁盘，除非计算您的数据集的函数是昂贵的，或者它们过滤大量的数据。否则，重新计算分区可能与从磁盘读取分区一样快.</p>
<p>如果需要快速故障恢复，请使用复制的存储级别（例如，如果使用 Spark 来服务 来自网络应用程序的请求）。All 存储级别通过重新计算丢失的数据来提供完整的容错能力，但复制的数据可让您继续在 RDD 上运行任务，而无需等待重新计算一个丢失的分区.</p>
<h1 id="三-Checkpoint"><a href="#三-Checkpoint" class="headerlink" title="三 Checkpoint"></a>三 Checkpoint</h1><h2 id="1-Checkpoint-的作用"><a href="#1-Checkpoint-的作用" class="headerlink" title="1 Checkpoint 的作用"></a>1 Checkpoint 的作用</h2><p> Checkpoint 的主要作用是斩断 RDD 的依赖链, 并且将数据存储在可靠的存储引擎中, 例如支持分布式存储和副本机制的 HDFS.</p>
<p><strong>Checkpoint 的方式</strong></p>
<ul>
<li><strong>可靠的</strong> 将数据存储在可靠的存储引擎中, 例如 HDFS</li>
<li><strong>本地的</strong> 将数据存储在本地</li>
</ul>
<p><strong>什么是斩断依赖链</strong></p>
<p>斩断依赖链是一个非常重要的操作, 接下来以 HDFS 的 NameNode 的原理来举例说明</p>
<p>HDFS 的 NameNode 中主要职责就是维护两个文件, 一个叫做 <code>edits</code>, 另外一个叫做 <code>fsimage</code>. <code>edits</code> 中主要存放 <code>EditLog</code>, <code>FsImage</code> 保存了当前系统中所有目录和文件的信息. 这个 <code>FsImage</code> 其实就是一个 <code>Checkpoint</code>.</p>
<p>HDFS 的 NameNode 维护这两个文件的主要过程是, 首先, 会由 <code>fsimage</code> 文件记录当前系统某个时间点的完整数据, 自此之后的数据并不是时刻写入 <code>fsimage</code>, 而是将操作记录存储在 <code>edits</code> 文件中. 其次, 在一定的触发条件下, <code>edits</code>会将自身合并进入 <code>fsimage</code>. 最后生成新的 <code>fsimage</code> 文件, <code>edits</code> 重置, 从新记录这次 <code>fsimage</code> 以后的操作日志.</p>
<p>如果不合并 <code>edits</code> 进入 <code>fsimage</code> 会怎样? 会导致 <code>edits</code> 中记录的日志过长, 容易出错.</p>
<p>所以当 Spark 的一个 Job 执行流程过长的时候, 也需要这样的一个斩断依赖链的过程, 使得接下来的计算轻装上阵.</p>
<p><strong>Checkpoint 和 Cache 的区别</strong></p>
<p>Cache 可以把 RDD 计算出来然后放在内存中, 但是 RDD 的依赖链(相当于 NameNode 中的 Edits 日志)是不能丢掉的, 因为这种缓存是不可靠的, 如果出现了一些错误(例如 Executor 宕机), 这个 RDD 的容错就只能通过回溯依赖链, 重放计算出来.</p>
<p>但是 Checkpoint 把结果保存在 HDFS 这类存储中, 就是可靠的了, 所以可以斩断依赖, 如果出错了, 则通过复制 HDFS 中的文件来实现容错.</p>
<p>所以他们的区别主要在以下两点</p>
<ul>
<li>Checkpoint 可以保存数据到 HDFS 这类可靠的存储上, Persist 和 Cache 只能保存在本地的磁盘和内存中</li>
<li>Checkpoint 可以斩断 RDD 的依赖链, 而 Persist 和 Cache 不行</li>
<li>因为 CheckpointRDD 没有向上的依赖链, 所以程序结束后依然存在, 不会被删除. 而 Cache 和 Persist 会在程序结束后立刻被清除.</li>
</ul>
<h2 id="2-使用-Checkpoint"><a href="#2-使用-Checkpoint" class="headerlink" title="2 使用 Checkpoint"></a>2 使用 Checkpoint</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">//checkpoint</span><br><span class="line">  @Test</span><br><span class="line">  def checkPointDemo(): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf().setMaster(&quot;local[3]&quot;).setAppName(&quot;cache&quot;)</span><br><span class="line">    val context =new  SparkContext(conf)</span><br><span class="line">    //设置保存checkpoint的目录 也可以在hdfs上</span><br><span class="line">    context.setCheckpointDir(&quot;checkpoint&quot;)</span><br><span class="line">    //读取文件</span><br><span class="line">    val file = context.textFile(&quot;G:\\develop\\bigdatas\\BigData\\day26sparkActions\\data\\access_log_sample.txt&quot;)</span><br><span class="line">    val rdd1 = file.map(it =&gt; ((it.split(&quot; &quot;)) (0), 1))</span><br><span class="line">      .reduceByKey((curr, agg) =&gt; curr + agg)</span><br><span class="line">      /*</span><br><span class="line">      checkpoint的返回值为unit空</span><br><span class="line">      * 1 不准确的说checkpoint是一个action操作,</span><br><span class="line">      * 2 如果调用 checkpoint则会重新计算rdd,然后把结果存在hdfs或本地目录</span><br><span class="line">      * 所以在checkpoint之前应该先缓存一下 </span><br><span class="line">      * */</span><br><span class="line">      .persist(StorageLevel.MEMORY_ONLY)</span><br><span class="line">      rdd1.checkpoint()</span><br><span class="line">    val rddMax = rdd1</span><br><span class="line">      .sortBy(item =&gt; item._2, true)</span><br><span class="line">      .first()</span><br><span class="line">    val rddMin = rdd1.sortBy(item2 =&gt; item2._2,false).first()</span><br><span class="line">    println(rddMax,rddMin)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></div><div class="post-copyright"><blockquote><p>Ursprünglicher Autor: hechao</p><p>Ursprünglicher Link: <a href="http://yoursite.com/2019/08/10/spark原理分析/">http://yoursite.com/2019/08/10/spark原理分析/</a></p><p>Copyright-Erklärung: Bitte geben Sie die Quelle des Nachdrucks an.</p></blockquote></div><div class="tags"></div><div class="post-share"><div class="social-share"><span>Aktie:</span></div></div><div class="post-nav"><a href="/2019/08/18/spark原理分析2/" class="pre">spark原理分析2</a><a href="/2019/08/08/SparkRDD/" class="next">SparkRDD</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">Inhalte</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一spark特性"><span class="toc-text">一spark特性</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-rdd的分区和shuffle"><span class="toc-text">1 rdd的分区和shuffle</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2分区操作"><span class="toc-text">2分区操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-查看分区数"><span class="toc-text">1 查看分区数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建-RDD-时指定分区数"><span class="toc-text">创建 RDD 时指定分区数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-RDD-的-Shuffle-是什么"><span class="toc-text">2 RDD 的 Shuffle 是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-RDD-的-Shuffle-原理"><span class="toc-text">3 RDD 的 Shuffle 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Hash-base-shuffle"><span class="toc-text">3.1 Hash base shuffle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Sort-base-shuffle"><span class="toc-text">3.2 Sort base shuffle</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二-缓存"><span class="toc-text">二 缓存</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-缓存的意义"><span class="toc-text">1 缓存的意义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-缓存相关的-API"><span class="toc-text">2 缓存相关的 API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-可以使用-cache-方法进行缓存"><span class="toc-text">2.1 可以使用 cache 方法进行缓存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-也可以使用-persist-方法进行缓存"><span class="toc-text">2.2 也可以使用 persist 方法进行缓存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-清理缓存"><span class="toc-text">2.3 清理缓存</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-缓存级别"><span class="toc-text">3 缓存级别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-查看缓存级别对象"><span class="toc-text">3.1 查看缓存级别对象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2如何选择分区级别"><span class="toc-text">3.2如何选择分区级别</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三-Checkpoint"><span class="toc-text">三 Checkpoint</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Checkpoint-的作用"><span class="toc-text">1 Checkpoint 的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-使用-Checkpoint"><span class="toc-text">2 使用 Checkpoint</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> Letzte</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/08/24/sparkSql高级/">sparkSql高级</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/20/sparkSQL/">sparkSQL</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/18/spark原理分析2/">spark原理分析2</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/10/spark原理分析/">spark原理分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/08/08/SparkRDD/">SparkRDD</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/08/Spark入门/">Spark入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/shell/">shell</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/18/Scala高级/">Scala高级</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/04/Yarn-资源调度/">Yarn-资源调度</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/27/Scala进阶2/">Scala进阶2</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> Tags</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> Archiv</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Sitemap</a> |  <a href="/atom.xml">Abonnieren Sie diese Site</a> |  <a href="/about/">Kontaktieren Sie den Blogger</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">hechao.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.4"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.4" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>